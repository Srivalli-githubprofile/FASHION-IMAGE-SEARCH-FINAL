{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Fashion Image Search with Picture Descriptions\n",
        "                                             -SRIVALLI VANGAVETI"
      ],
      "metadata": {
        "id": "0a10jy2KdQl4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hC6sBmgul-03",
        "outputId": "7c4c4a8b-59d4-416e-e8f0-e317e7e5172b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'fashion-iq'...\n",
            "remote: Enumerating objects: 140, done.\u001b[K\n",
            "remote: Counting objects: 100% (31/31), done.\u001b[K\n",
            "remote: Compressing objects: 100% (21/21), done.\u001b[K\n",
            "remote: Total 140 (delta 18), reused 10 (delta 10), pack-reused 109 (from 1)\u001b[K\n",
            "Receiving objects: 100% (140/140), 1.48 MiB | 4.72 MiB/s, done.\n",
            "Resolving deltas: 100% (46/46), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/XiaoxiaoGuo/fashion-iq.git\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clone the FashionIQ repositories\n",
        "\n",
        "\n",
        "The FashionIQ dataset contains images and captions for dresses, shirts, and tops.\n",
        "\n",
        "\n",
        "The metadata repository contains additional information such as image URLs.\n"
      ],
      "metadata": {
        "id": "-syLtmFpxB03"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5RNOcj-mB00",
        "outputId": "d0ea5875-6a10-4cfb-b9e8-4a7dc9b011a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'fashion-iq-metadata'...\n",
            "remote: Enumerating objects: 17, done.\u001b[K\n",
            "remote: Total 17 (delta 0), reused 0 (delta 0), pack-reused 17 (from 1)\u001b[K\n",
            "Receiving objects: 100% (17/17), 1.49 MiB | 4.10 MiB/s, done.\n",
            "Resolving deltas: 100% (5/5), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/hongwang600/fashion-iq-metadata.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3C6emk5JmDl-",
        "outputId": "8ba28dba-b67d-4983-abc8-bad70fbacca0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision transformers matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-U8m_eFmFhs",
        "outputId": "e0e4eaf5-bf0a-47e5-f65f-d09f67d968fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"target\": \"B008BHCT58\",\n",
            "  \"candidate\": \"B003FGW7MK\",\n",
            "  \"captions\": [\n",
            "    \"is solid black with no sleeves\",\n",
            "    \"is black with straps\"\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "#Load a sample captions file\n",
        "\n",
        "# This JSON file contains training captions for dress images\n",
        "import json\n",
        "captions_file = \"fashion-iq/captions/cap.dress.train.json\"\n",
        "\n",
        "# Open the JSON file and load its content into a Python list\n",
        "with open(captions_file, \"r\") as f:\n",
        "    captions_data = json.load(f)\n",
        "\n",
        "# Print the first entry\n",
        "print(json.dumps(captions_data[0], indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "HXC82CafmRSD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9172ab3b-05b1-452c-ea1f-b14d8950fb39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing category: dress\n",
            "CSV file saved for category 'dress' at: fashion-iq/annotations/dress_annotations.csv\n",
            "Processing category: shirt\n",
            "CSV file saved for category 'shirt' at: fashion-iq/annotations/shirt_annotations.csv\n",
            "Processing category: toptee\n",
            "CSV file saved for category 'toptee' at: fashion-iq/annotations/toptee_annotations.csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "#Set up directories for processing\n",
        "\n",
        "# BASE_DIR: main FashionIQ dataset directory\n",
        "# CAPTIONS_DIR: directory containing all captions\n",
        "# IMAGE_SPLITS_DIR: directory containing train/val/test splits\n",
        "# OUTPUT_DIR: directory where processed CSVs will be saved\n",
        "\n",
        "BASE_DIR = \"fashion-iq\"\n",
        "CAPTIONS_DIR = os.path.join(BASE_DIR, \"captions\")\n",
        "IMAGE_SPLITS_DIR = os.path.join(BASE_DIR, \"image_splits\")\n",
        "OUTPUT_DIR = os.path.join(BASE_DIR, \"annotations\")\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "#Function to load image splits\n",
        "def load_image_splits(category, split):\n",
        "    \"\"\"\n",
        "    Load image splits for a given category and split.\n",
        "    \"\"\"\n",
        "    split_file = os.path.join(IMAGE_SPLITS_DIR, f\"split.{category}.{split}.json\")\n",
        "    with open(split_file, \"r\") as f:\n",
        "        return set(json.load(f))\n",
        "\n",
        "\n",
        "#Function to process a category\n",
        "def process_category(category):\n",
        "    \"\"\"\n",
        "    Process all splits (train, val, test) for a given category and return combined data.\n",
        "    \"\"\"\n",
        "    splits = [\"train\", \"val\", \"test\"]\n",
        "    rows = [] # This will store all structured data\n",
        "\n",
        "    for split in splits:\n",
        "        # Load valid image IDs for this split\n",
        "        valid_images = load_image_splits(category, split)\n",
        "\n",
        "        # Load captions JSON for this category and split\n",
        "        captions_file = os.path.join(CAPTIONS_DIR, f\"cap.{category}.{split}.json\")\n",
        "        if not os.path.exists(captions_file):\n",
        "            print(f\"File not found: {captions_file}\")\n",
        "            continue\n",
        "\n",
        "        with open(captions_file, \"r\") as f:\n",
        "            captions_data = json.load(f)\n",
        "\n",
        "        for item in captions_data:\n",
        "            ref_image = item[\"candidate\"] # Reference image ID\n",
        "            captions = item.get(\"captions\", []) # List of textual descriptions\n",
        "\n",
        "            target_image = item.get(\"target\", \"unknown\" if split == \"test\" else None)\n",
        "\n",
        "            # Skip images that are not in the valid image split\n",
        "            if ref_image not in valid_images:\n",
        "                continue\n",
        "\n",
        "            if split != \"test\" and (not target_image or target_image not in valid_images):\n",
        "                continue\n",
        "\n",
        "            # For each caption, create a structured row\n",
        "            for caption in captions:\n",
        "                rows.append({\n",
        "                    \"split\": split,                 # train, val, or test\n",
        "                    \"category\": category,           # dress/shirt/toptee\n",
        "                    \"reference_image\": ref_image,   # candidate image\n",
        "                    \"target_image\": target_image,   # target image\n",
        "                    \"text_description\": caption     # caption text\n",
        "                })\n",
        "\n",
        "    return rows\n",
        "\n",
        "\n",
        "#Process all categories and save as CSV\n",
        "categories = [\"dress\", \"shirt\", \"toptee\"]\n",
        "for category in categories:\n",
        "    print(f\"Processing category: {category}\")\n",
        "    rows = process_category(category)\n",
        "\n",
        "    # Convert list of dictionaries into a pandas DataFrame for easier handling\n",
        "    df_category = pd.DataFrame(rows)\n",
        "\n",
        "    # Save DataFrame to CSV\n",
        "    category_csv_path = os.path.join(OUTPUT_DIR, f\"{category}_annotations.csv\")\n",
        "    df_category.to_csv(category_csv_path, index=False)\n",
        "    print(f\"CSV file saved for category '{category}' at: {category_csv_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "Q4UIxir8mTn_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bd2a23b-db2f-4764-b888-4f6916552153"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'target': 'B008BHCT58', 'candidate': 'B003FGW7MK', 'captions': ['is solid black with no sleeves', 'is black with straps']}\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "#Inspect first annotation entry for preprocessing\n",
        "annotation_file = \"/content/fashion-iq/captions/cap.dress.train.json\"\n",
        "\n",
        "with open(annotation_file, 'r') as f:\n",
        "    annotations = json.load(f)\n",
        "\n",
        "# Print the first entry\n",
        "print(annotations[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "t5GjLmK4mXQX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4783a78a-d21f-41a3-fce8-2ade3ce259ee"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_items([('target', 'B008BHCT58'), ('candidate', 'B003FGW7MK'), ('captions', ['is solid black with no sleeves', 'is black with straps'])])"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "# Inspect all items (keys and values) in the first annotation entry\n",
        "annotations[0].items()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "ne1W6o8YmZNH"
      },
      "outputs": [],
      "source": [
        "target = annotations[0]['target']     # Target image ID\n",
        "candidate = annotations[0]['candidate']  # Reference image ID\n",
        "captions = annotations[0]['captions']    # List of captions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kADrtd6Ume9X"
      },
      "source": [
        "# **DATA PREPROCESSING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "2T_T1FzBmfrM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d699dc7e-ff13-4b4b-9d9e-f2b38c06f3d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1:\n",
            "  Reference (Candidate): B008VILBSI\n",
            "  Target: B00AIBYOW6\n",
            "  Text Description (Raw/Tokenized): {'input_ids': tensor([[[49406,   533,   731,  ..., 49407, 49407, 49407]],\n",
            "\n",
            "        [[49406,   791,  1538,  ..., 49407, 49407, 49407]],\n",
            "\n",
            "        [[49406,   533,  1746,  ..., 49407, 49407, 49407]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[49406,   533, 18737,  ..., 49407, 49407, 49407]],\n",
            "\n",
            "        [[49406,  6148,  1449,  ..., 49407, 49407, 49407]],\n",
            "\n",
            "        [[49406,   533, 20350,  ..., 49407, 49407, 49407]]]), 'attention_mask': tensor([[[1, 1, 1,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[1, 1, 1,  ..., 0, 0, 0]]])}\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import CLIPTokenizer\n",
        "\n",
        "# Define a custom PyTorch Dataset for FashionIQ\n",
        "\n",
        "class FashionIQDataset(Dataset):\n",
        "    def __init__(self, annotation_file, transform=None, tokenizer=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            annotation_file (str): Path to the JSON annotation file.\n",
        "            transform (callable, optional): Transformation to apply to images (not used here).\n",
        "            tokenizer (callable, optional): Tokenizer for captions.\n",
        "        \"\"\"\n",
        "\n",
        "        # Load JSON annotations into memory\n",
        "        with open(annotation_file, 'r') as f:\n",
        "            self.annotations = json.load(f)\n",
        "\n",
        "        # Store optional transform and tokenizer\n",
        "        self.transform = transform\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations)   # Return the total number of entries in the dataset\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "        Get a single item from the dataset at index `idx`.\n",
        "        Returns a dictionary containing:\n",
        "        - 'reference': candidate image ID\n",
        "        - 'target': target image ID\n",
        "        - 'text': tokenized text description (or raw text if tokenizer not provided)\n",
        "        \"\"\"\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.annotations[idx]  # Get the annotation at the given index\n",
        "        reference = sample['candidate']  # Candidate image ID\n",
        "        target = sample['target']        # Target image ID\n",
        "        text_description = sample['captions'][0]  # Take the first caption from the list\n",
        "\n",
        "\n",
        "        # If a tokenizer is provided, convert text description to token tensors\n",
        "        # Pad the text to a fixed length\n",
        "        # Maximum sequence length for CLIP\n",
        "        # Return PyTorch tensor\n",
        "        if self.tokenizer:\n",
        "            text_tokens = self.tokenizer(text_description, padding='max_length', max_length=77, return_tensors=\"pt\")\n",
        "        else:\n",
        "            text_tokens = text_description             # Otherwise, keep text as raw string\n",
        "        return {\n",
        "            \"reference\": reference,\n",
        "            \"target\": target,\n",
        "            \"text\": text_tokens\n",
        "        }\n",
        "\n",
        "# Initialize the CLIP tokenizer from the pretrained model\n",
        "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "# Path to the annotations JSON file for dresses\n",
        "ANNOTATION_FILE = \"fashion-iq/captions/cap.dress.train.json\"\n",
        "\n",
        "# Create the dataset object\n",
        "dataset = FashionIQDataset(\n",
        "    annotation_file=ANNOTATION_FILE,\n",
        "    transform=None,      # No image transform applied yet\n",
        "    tokenizer=tokenizer  # Use CLIP tokenizer for text\n",
        ")\n",
        "\n",
        "# Create a DataLoader to iterate through dataset in batches\n",
        "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "\n",
        "# Iterate over the first batch to inspect contents\n",
        "for i, batch in enumerate(dataloader):\n",
        "    reference = batch['reference'][0]      # Get first reference image ID in the batch\n",
        "    target = batch['target'][0]            # Get first target image ID in the batch\n",
        "    text_description = batch['text']       # Get tokenized text for the batch\n",
        "\n",
        "    # Print batch info for verification\n",
        "    print(f\"Batch {i + 1}:\")\n",
        "    print(f\"  Reference (Candidate): {reference}\")\n",
        "    print(f\"  Target: {target}\")\n",
        "    print(f\"  Text Description (Raw/Tokenized): {text_description}\")\n",
        "\n",
        "    # Stop after the first batch\n",
        "    if i == 0:\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "5KdwXzfRmosA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d086bb4b-6844-430a-c332-79fc4627b282"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading B009S5Y2FM from http://ecx.images-amazon.com/images/I/41IHkTaLBoL._SX342_.jpg\n",
            "Downloading B004ZWCHBE from http://ecx.images-amazon.com/images/I/41eAzD3L-QL._SX342_.jpg\n",
            "Downloading B00AYXA7TM from http://ecx.images-amazon.com/images/I/41e4tz6LZBL._SX342_.jpg\n",
            "Downloading B00E9OR1YG from http://ecx.images-amazon.com/images/I/31d9JSREA7L._SX342_.jpg\n",
            "Failed to download B00E9OR1YG: HTTP 404\n",
            "Downloading B00DEPBLDI from http://ecx.images-amazon.com/images/I/41Hc%2BBvTP6L._SX342_.jpg\n",
            "Downloading B00CLPGF38 from http://ecx.images-amazon.com/images/I/31RGSEowXTL._SY445_.jpg\n",
            "Downloading B00CNFG1MQ from http://ecx.images-amazon.com/images/I/51WDCLQIHbL._SY445_.jpg\n",
            "Downloading B008P3Z1KS from http://ecx.images-amazon.com/images/I/31L3ePhAFVL._SX342_.jpg\n",
            "Downloading B0016CHSPQ from http://ecx.images-amazon.com/images/I/31AXbmWQtsL._SY445_.jpg\n",
            "Downloading B009LIFA9I from http://ecx.images-amazon.com/images/I/41xDNNM0fmL._SX342_.jpg\n",
            "Downloading B007IXDESW from http://ecx.images-amazon.com/images/I/31BrgjaaIZL._SY445_.jpg\n",
            "Downloading B00DO9PN6U from http://ecx.images-amazon.com/images/I/41KxgyXczRL._SY445_.jpg\n",
            "Downloading B00C2MFMTI from http://ecx.images-amazon.com/images/I/31XrzEelC2L._SX342_.jpg\n",
            "Downloading B00FIZYWDS from http://ecx.images-amazon.com/images/I/4174ibEdgNL._SX342_.jpg\n",
            "Downloading B008BA7THS from http://ecx.images-amazon.com/images/I/41oavsPlNjL._SY445_.jpg\n",
            "Downloading B006X53KX0 from http://ecx.images-amazon.com/images/I/31kDycB12vL._SX342_.jpg\n",
            "Downloading B007NGI998 from http://ecx.images-amazon.com/images/I/31bsgfc5GgL._SX342_.jpg\n",
            "Downloading B00B62A9F2 from http://ecx.images-amazon.com/images/I/41ZOwV%2B9M8L._SX342_.jpg\n",
            "Downloading B0094D087E from http://ecx.images-amazon.com/images/I/31n4dHc7VRL._SY445_.jpg\n",
            "Downloading B00BSKJ6BA from http://ecx.images-amazon.com/images/I/41I9r7Jg9GL._SX342_.jpg\n",
            "Downloading B00AJNGW4G from http://ecx.images-amazon.com/images/I/31UgTKJpymL._SY445_.jpg\n",
            "Downloading B00BM1QDWU from http://ecx.images-amazon.com/images/I/41lkrrCQ1nL._SY200_.jpg\n",
            "Downloading B007CDY5R2 from http://ecx.images-amazon.com/images/I/41K0X84Qd-L._SX342_.jpg\n",
            "Downloading B005RYO48S from http://ecx.images-amazon.com/images/I/31aIUm6TLnL._SX342_.jpg\n",
            "Downloading B00EDZUYEU from http://ecx.images-amazon.com/images/I/31gYyqDnX8L._SY445_.jpg\n",
            "Downloading B008EQ06I8 from http://ecx.images-amazon.com/images/I/41Rkonq1YYL._SX342_.jpg\n",
            "Downloading B007G3NEN4 from http://ecx.images-amazon.com/images/I/316xYBN%2BKeL._SX342_.jpg\n",
            "Downloading B00APK6WQ6 from http://ecx.images-amazon.com/images/I/41w4mUlXCVL._SX342_.jpg\n",
            "Downloading B004GBERG2 from http://ecx.images-amazon.com/images/I/31TZCiUelvL._SY445_.jpg\n",
            "Downloading B00FKKPEYC from http://ecx.images-amazon.com/images/I/31qaKKpH8UL._SY445_.jpg\n",
            "Downloading B004S08UY6 from http://ecx.images-amazon.com/images/I/41zTnL9JrfL._SX342_.jpg\n",
            "Downloading B008RX9C7O from http://ecx.images-amazon.com/images/I/41mRiyIxwgL._SX342_.jpg\n",
            "Downloading B00CIQE186 from http://ecx.images-amazon.com/images/I/31j8dtCBXYL._SY200_.jpg\n",
            "Downloading B002PJ5EIC from http://ecx.images-amazon.com/images/I/51WmnfP2u7L._SY445_.jpg\n",
            "Downloading B009S3E9KW from http://ecx.images-amazon.com/images/I/41LIAjAymFL._SY445_.jpg\n",
            "Downloading B00E98XFF6 from http://ecx.images-amazon.com/images/I/31751ooL7sL._SY200_.jpg\n",
            "Downloading B005WN58S4 from http://ecx.images-amazon.com/images/I/41ezoCR9ZpL._SX342_.jpg\n",
            "Downloading B00CWZACPE from http://ecx.images-amazon.com/images/I/31wzRwM9jLL._SY445_.jpg\n",
            "Downloading B004RE7COC from http://ecx.images-amazon.com/images/I/41VNj3NKgNL._SX342_.jpg\n",
            "Downloading B0071I2NPY from http://ecx.images-amazon.com/images/I/414PM6X9MkL._SY445_.jpg\n",
            "Downloading B0085JEB4O from http://ecx.images-amazon.com/images/I/414pxk9CXOL._SX342_.jpg\n",
            "Downloading B00CX8EZTY from http://ecx.images-amazon.com/images/I/41KrN6u%2BiML._SX342_.jpg\n",
            "Downloading B008SMUFO8 from http://ecx.images-amazon.com/images/I/31nFMks4a7L._SY445_.jpg\n",
            "Downloading B00BJZ9WNQ from http://ecx.images-amazon.com/images/I/41Z6NNKRQ7L._SY445_.jpg\n",
            "Downloading B00GZKN3V2 from http://ecx.images-amazon.com/images/I/51FHCQ8bQrL._SX342_.jpg\n",
            "Downloading B005P780VY from http://ecx.images-amazon.com/images/I/31zBTNeaadL._SX342_.jpg\n",
            "Downloading B006GK20BK from http://ecx.images-amazon.com/images/I/41BqG7bC9rL._SX342_.jpg\n",
            "Downloading B005R5216O from http://ecx.images-amazon.com/images/I/41lE55fJ10L._SX342_.jpg\n",
            "Downloading B00ECF1I28 from http://ecx.images-amazon.com/images/I/31THB1rFmTL._SY445_.jpg\n",
            "Downloading B007H3JKW2 from http://ecx.images-amazon.com/images/I/41QUDVXIBFL._SX342_.jpg\n",
            "Downloading B007MF6EBU from http://ecx.images-amazon.com/images/I/51n0RvqgExL._SY445_.jpg\n",
            "Downloading B007CPTQ1A from http://ecx.images-amazon.com/images/I/41uiRylsiyL._SX342_.jpg\n",
            "Downloading B0087RFRHY from http://ecx.images-amazon.com/images/I/31ZqIYS3PKL._SY445_.jpg\n",
            "Downloading B00B1OSP8S from http://ecx.images-amazon.com/images/I/41USnDuS6QL._SY445_.jpg\n",
            "Downloading B0061K4F7M from http://ecx.images-amazon.com/images/I/41EycyFvQTL._SX342_.jpg\n",
            "Downloading B006BBKVPG from http://ecx.images-amazon.com/images/I/31XdurMhz2L._SX342_.jpg\n",
            "Downloading B004Q9T84A from http://ecx.images-amazon.com/images/I/41qmLt6q4PL._SX342_.jpg\n",
            "Downloading B00CDAL83I from http://ecx.images-amazon.com/images/I/41HSlv8ZTXL._SX342_.jpg\n",
            "Downloading B004ZWGGDY from http://ecx.images-amazon.com/images/I/417z1EMN23L._SX342_.jpg\n",
            "Downloading B0030KF444 from http://ecx.images-amazon.com/images/I/411O%2Bm0TM7L._SX342_.jpg\n",
            "Downloading B00BM7318S from http://ecx.images-amazon.com/images/I/51KfkoHh17L._SY445_.jpg\n",
            "Downloading B00GN6FLG8 from http://ecx.images-amazon.com/images/I/411Tk8BbZhL._SX342_.jpg\n",
            "Downloading B004TOUSX2 from http://ecx.images-amazon.com/images/I/416LoXPtG4L._SX342_.jpg\n",
            "Downloading B009LUJQ7S from http://ecx.images-amazon.com/images/I/41oRJ0J0O1L._SX342_.jpg\n",
            "Downloading B006MPVUO2 from http://ecx.images-amazon.com/images/I/319EvMDtDSL._SX342_.jpg\n",
            "Downloading B00A0BI83K from http://ecx.images-amazon.com/images/I/41dk8rBAMaL._SY200_.jpg\n",
            "Failed to download B00A0BI83K: HTTP 404\n",
            "Downloading B00FGNAA74 from http://ecx.images-amazon.com/images/I/41fvsa-zVDL._SX342_.jpg\n",
            "Downloading B00DDS2J32 from http://ecx.images-amazon.com/images/I/31WV555zYzL._SX342_.jpg\n",
            "Downloading B008O4YWWQ from http://ecx.images-amazon.com/images/I/318GAezk-nL._SY445_.jpg\n",
            "Downloading B004YUE61Q from http://ecx.images-amazon.com/images/I/31MlViiJlhL._SY445_.jpg\n",
            "Downloading B00A36O6ZQ from http://ecx.images-amazon.com/images/I/31RKLU29T2L._SY445_.jpg\n",
            "Downloading B008PHPQ5O from http://ecx.images-amazon.com/images/I/41QPcyXgKYL._SY445_.jpg\n",
            "Downloading B0081ZKNNA from http://ecx.images-amazon.com/images/I/31Xds5%2BgC%2BL._SX342_.jpg\n",
            "Downloading B003JQK7M8 from http://ecx.images-amazon.com/images/I/31RKAB2cNIL._SX342_.jpg\n",
            "Downloading B00AYCL55S from http://ecx.images-amazon.com/images/I/31ibsce1T0L._SY200_.jpg\n",
            "Downloading B007WAFLKU from http://ecx.images-amazon.com/images/I/417OpdZroaL._SX342_.jpg\n",
            "Downloading B005N7GPO0 from http://ecx.images-amazon.com/images/I/41WsgcmuzzL._SX342_.jpg\n",
            "Downloading B003RI360I from http://ecx.images-amazon.com/images/I/51y5czRAYOL._SY445_.jpg\n",
            "Downloading B007WACXG0 from http://ecx.images-amazon.com/images/I/51DMiaSbVfL._SY445_.jpg\n",
            "Downloading B00FS7K0HI from http://ecx.images-amazon.com/images/I/31fpfn2hNEL._SX342_.jpg\n",
            "Downloading B00BHSAXQU from http://ecx.images-amazon.com/images/I/41H2k%2B3V2SL._SX342_.jpg\n",
            "Downloading B0098MOTAI from http://ecx.images-amazon.com/images/I/41nXKP%2BMKhL._SY445_.jpg\n",
            "Downloading B00BJ4X8LO from http://ecx.images-amazon.com/images/I/51o4RRv6qRL._SY445_.jpg\n",
            "Downloading B002UD653Q from http://ecx.images-amazon.com/images/I/41w%2BZXA2mhL._SX342_.jpg\n",
            "Downloading B005KQWIYK from http://ecx.images-amazon.com/images/I/41s9ksvamoL._SX342_.jpg\n",
            "Downloading B00FM5I8GQ from http://ecx.images-amazon.com/images/I/415x7dsA3GL._SY445_.jpg\n",
            "Downloading B00BKU8KUQ from http://ecx.images-amazon.com/images/I/31igo3AiKXL._SY200_.jpg\n",
            "Downloading B008FEOHC0 from http://ecx.images-amazon.com/images/I/31Abb-gU4xL._SX342_.jpg\n",
            "Downloading B006NU77ES from http://ecx.images-amazon.com/images/I/31nP7K2Z04L._SX342_.jpg\n",
            "Downloading B004IK9BLC from http://ecx.images-amazon.com/images/I/31esohLF4-L._SX342_.jpg\n",
            "Downloading B008UWXSH2 from http://ecx.images-amazon.com/images/I/41Zap%2BbK2PL._SY445_.jpg\n",
            "Downloading B00BG5PASU from http://ecx.images-amazon.com/images/I/31gErpQy8hL._SY445_.jpg\n",
            "Downloading B001E764X0 from http://ecx.images-amazon.com/images/I/31Go3ovEj%2BL._SY445_.jpg\n",
            "Downloading B00AOC71JC from http://ecx.images-amazon.com/images/I/41DNRU4LgbL._SX342_.jpg\n",
            "Downloading B004TLF1MI from http://ecx.images-amazon.com/images/I/41pcreOO7rL._SX342_.jpg\n",
            "Downloading B007GSGDPU from http://ecx.images-amazon.com/images/I/31adZKwuMdL._SY445_.jpg\n",
            "Downloading B00C3CUPQW from http://g-ecx.images-amazon.com/images/G/01/x-locale/brands/small-logo/3032414011._CB359598581_SX150_.jpg\n",
            "Downloading B00ASRNQYW from http://ecx.images-amazon.com/images/I/313HNPKGcsL._SX342_.jpg\n",
            "Downloading B008VPNVDO from http://ecx.images-amazon.com/images/I/41SzOVlLORL._SX342_.jpg\n",
            "Downloading B00B81XW5A from http://ecx.images-amazon.com/images/I/41VRQvJPgQL._SY200_.jpg\n",
            "Failed to download B00B81XW5A: HTTP 404\n",
            "Downloading B00C4NV5ZA from http://ecx.images-amazon.com/images/I/41jm%2BAvbfEL._SY445_.jpg\n",
            "Downloading B002GYOV1W from http://ecx.images-amazon.com/images/I/41a09jl-5yL._SY445_.jpg\n",
            "Downloading B005R13MW0 from http://ecx.images-amazon.com/images/I/31o1rt8I51L._SX342_.jpg\n",
            "Downloading B00BR3C8GS from http://ecx.images-amazon.com/images/I/31XJhjCM2TL._SY445_.jpg\n",
            "Downloading B00AU1U4X2 from http://ecx.images-amazon.com/images/I/31BxWsT6FjL._SX342_.jpg\n",
            "Downloading B00FQZQ486 from http://ecx.images-amazon.com/images/I/319OLP9YDQL._SX342_.jpg\n",
            "Downloading B005VIA4S4 from http://ecx.images-amazon.com/images/I/416JYwhUUDL._SY445_.jpg\n",
            "Downloading B00BG45SEM from http://ecx.images-amazon.com/images/I/41X6Ke45V4L._SX342_.jpg\n",
            "Downloading B00C97FCYG from http://ecx.images-amazon.com/images/I/41OJicj4CfL._SY445_.jpg\n",
            "Downloading B009017BRA from http://ecx.images-amazon.com/images/I/41l2QO4tjSL._SX342_.jpg\n",
            "Downloading B00B3PUFX8 from http://ecx.images-amazon.com/images/I/31WD0n4H%2BuL._SY200_.jpg\n",
            "Downloading B00C4OMRUQ from http://ecx.images-amazon.com/images/I/41fOE34uU3L._SX342_.jpg\n",
            "Downloading B007XHPH7O from http://ecx.images-amazon.com/images/I/31WCg03z1FL._SY445_.jpg\n",
            "Downloading B008KKZ5VQ from http://ecx.images-amazon.com/images/I/31z%2Bmk4IS1L._SX342_.jpg\n",
            "Downloading B003VWOX6Q from http://ecx.images-amazon.com/images/I/31WSbLXxBUL._SY445_.jpg\n",
            "Downloading B004H3OZNY from http://ecx.images-amazon.com/images/I/31wAuSLPzhL._SY445_.jpg\n",
            "Downloading B00AMQ9TTA from http://ecx.images-amazon.com/images/I/41wWzlMqecL._SX342_.jpg\n",
            "Downloading B005NIUYLE from http://ecx.images-amazon.com/images/I/416iOi41EqL._SY445_.jpg\n",
            "Downloading B00E4NM9LW from http://ecx.images-amazon.com/images/I/31Inhm4GKQL._SY445_.jpg\n",
            "Downloading B007RKTO6C from http://ecx.images-amazon.com/images/I/41hyg1IKNgL._SX342_.jpg\n",
            "Downloading B00FNJDPNW from http://ecx.images-amazon.com/images/I/31fx5Yf69XL._SX342_.jpg\n",
            "Failed to download B00FNJDPNW: HTTP 404\n",
            "Downloading B003ILHHD6 from http://ecx.images-amazon.com/images/I/41Io-gfraNL._SY445_.jpg\n",
            "Downloading B008LRMY3A from http://ecx.images-amazon.com/images/I/41No79vxwxL._SX342_.jpg\n",
            "Downloading B0082ABFEA from http://ecx.images-amazon.com/images/I/31DTm3%2ByAkL._SX342_.jpg\n",
            "Downloading B00CC6ZJPQ from http://ecx.images-amazon.com/images/I/31ZPckNrxWL._SY445_.jpg\n",
            "Downloading B005X4PHMY from http://ecx.images-amazon.com/images/I/41nK2hAd6jL._SX342_.jpg\n",
            "Downloading B00CX8ZDMC from http://ecx.images-amazon.com/images/I/31iGVkzxrHL._SY445_.jpg\n",
            "Downloading B006O3EH7Y from http://ecx.images-amazon.com/images/I/41tKNpQ66IL._SY445_.jpg\n",
            "Downloading B00BEMDD0C from http://ecx.images-amazon.com/images/I/414ug6ZEHgL._SX342_.jpg\n",
            "Downloading B003IF4HLC from http://ecx.images-amazon.com/images/I/417VNgv4LEL._SY445_.jpg\n",
            "Downloading B008P39VUY from http://ecx.images-amazon.com/images/I/41r4rhINMSL._SY445_.jpg\n",
            "Downloading B00DO9OMQW from http://ecx.images-amazon.com/images/I/41EWiBappdL._SY445_.jpg\n",
            "Downloading B00361FM12 from http://ecx.images-amazon.com/images/I/31XncyHt1FL._SX342_.jpg\n",
            "Failed to download B00361FM12: HTTP 404\n",
            "Downloading B007I8SNFQ from http://ecx.images-amazon.com/images/I/51DZaDzO0CL._SY445_.jpg\n",
            "Downloading B00A7HPMO0 from http://ecx.images-amazon.com/images/I/414Rb1HcMhL._SX342_.jpg\n",
            "Downloading B00CMSRQZU from http://ecx.images-amazon.com/images/I/4183azTvkvL._SY445_.jpg\n",
            "Downloading B008I2VXU8 from http://ecx.images-amazon.com/images/I/417q9e%2BWb9L._SX342_.jpg\n",
            "Downloading B005OEWSLG from http://ecx.images-amazon.com/images/I/41iBWkc4XEL._SX342_.jpg\n",
            "Downloading B00B7QPWUO from http://ecx.images-amazon.com/images/I/412VguuPHgL._SX342_.jpg\n",
            "Downloading B00CIAINUO from http://ecx.images-amazon.com/images/I/41J90Y22YaL._SX342_.jpg\n",
            "Downloading B00ASZ9HVA from http://ecx.images-amazon.com/images/I/41Uyi49C8LL._SY445_.jpg\n",
            "Downloading B00BI588U0 from http://ecx.images-amazon.com/images/I/31tNIZ9U03L._SY445_.jpg\n",
            "Downloading B00BRBTSJK from http://ecx.images-amazon.com/images/I/31fqxI3HI1L._SX342_.jpg\n",
            "Downloading B003VWY0VO from http://ecx.images-amazon.com/images/I/41Zm5J5tXrL._SY445_.jpg\n",
            "Downloading B004QTQ8FW from http://ecx.images-amazon.com/images/I/41ZrPiIKkLL._SX342_.jpg\n",
            "Downloading B00E9LDV5M from http://ecx.images-amazon.com/images/I/41reTRT1GZL._SX342_.jpg\n",
            "Downloading B009E2E0RE from http://ecx.images-amazon.com/images/I/315Sr4iVF3L._SX342_.jpg\n",
            "Downloading B002SSUKU2 from http://ecx.images-amazon.com/images/I/41EZOJbvoeL._SX342_.jpg\n",
            "Downloading B00ANK7H3A from http://ecx.images-amazon.com/images/I/41mCWkaEDLL._SY200_.jpg\n",
            "Downloading B009FBB0G8 from http://ecx.images-amazon.com/images/I/31%2B2s3ddIeL._SX342_.jpg\n",
            "Downloading B00A3USRAW from http://ecx.images-amazon.com/images/I/31oG5SbibsL._SY445_.jpg\n",
            "Downloading B00CXMLGT2 from http://ecx.images-amazon.com/images/I/41xOaYHwRnL._SX342_.jpg\n",
            "Failed to download B00CXMLGT2: HTTP 404\n",
            "Downloading B008NFAE1O from http://ecx.images-amazon.com/images/I/312T%2BuHKejL._SY445_.jpg\n",
            "Downloading B0053O76AA from http://ecx.images-amazon.com/images/I/51n5zjP0PXL._SX342_.jpg\n",
            "Downloading B007SRJ3CE from http://ecx.images-amazon.com/images/I/31mX4%2B-0rIL._SY445_.jpg\n",
            "Downloading B0099SRL2Y from http://ecx.images-amazon.com/images/I/41Yje5iOXEL._SY445_.jpg\n",
            "Downloading B00ELV528Y from http://ecx.images-amazon.com/images/I/41YkxnsnSzL._SX342_.jpg\n",
            "Downloading B00BXWJIA2 from http://ecx.images-amazon.com/images/I/418tEW6qijL._SY445_.jpg\n",
            "Downloading B00E1ZS3E0 from http://ecx.images-amazon.com/images/I/31s8-jGWw6L._SX342_.jpg\n",
            "Downloading B006WQWBEY from http://ecx.images-amazon.com/images/I/41mHH8feDML._SX342_.jpg\n",
            "Downloading B008AU6S0I from http://ecx.images-amazon.com/images/I/41XHdKdjOjL._SX342_.jpg\n",
            "Downloading B00COYHOGS from http://ecx.images-amazon.com/images/I/41czijAUHyL._SY445_.jpg\n",
            "Downloading B00BFFAA1S from http://ecx.images-amazon.com/images/I/4130SxdzbXL._SX342_.jpg\n",
            "Downloading B00BBLD1LC from http://ecx.images-amazon.com/images/I/511DS8rCHQL._SX342_.jpg\n",
            "Downloading B00850T28S from http://ecx.images-amazon.com/images/I/414rnkB%2BI0L._SX342_.jpg\n",
            "Downloading B005T98SP6 from http://ecx.images-amazon.com/images/I/31yzoJnt87L._SY445_.jpg\n",
            "Downloading B0085IY406 from http://ecx.images-amazon.com/images/I/41sdRjOiQ5L._SX342_.jpg\n",
            "Downloading B004I5A7SS from http://ecx.images-amazon.com/images/I/31V8FucNOyL._SX342_.jpg\n",
            "Downloading B00A4FQO92 from http://ecx.images-amazon.com/images/I/41ifw9D1R2L._SY445_.jpg\n",
            "Downloading B008AK0JAI from http://ecx.images-amazon.com/images/I/41tKyBSp4CL._SX342_.jpg\n",
            "Downloading B00AHTQ814 from http://ecx.images-amazon.com/images/I/31E1L9GpeVL._SY445_.jpg\n",
            "Downloading B007D7DXWA from http://ecx.images-amazon.com/images/I/41cyLjKnAbL._SX342_.jpg\n",
            "Downloading B008YQXJFU from http://ecx.images-amazon.com/images/I/31r7qKdwelL._SY445_.jpg\n",
            "Downloading B00A8E8Z1E from http://g-ecx.images-amazon.com/images/G/01/x-locale/brands/small-logo/3034061011._CB359598500_SX150_.jpg\n",
            "Downloading B008ZAYOQI from http://ecx.images-amazon.com/images/I/419Wsnv4PDL._SX342_.jpg\n",
            "Downloading B00DPENKUA from http://ecx.images-amazon.com/images/I/41FfP7qt8RL._SX342_.jpg\n",
            "Downloading B003MQPIUQ from http://ecx.images-amazon.com/images/I/31DFfWQrkZL.jpg\n",
            "Downloading B0099DCR8W from http://ecx.images-amazon.com/images/I/31H37w25u5L._SY445_.jpg\n",
            "Downloading B0032XUKTI from http://ecx.images-amazon.com/images/I/41egAZKuZHL._SY445_.jpg\n",
            "Downloading B00BGDHZSA from http://ecx.images-amazon.com/images/I/41jz2w%2BopoL._SY445_.jpg\n",
            "Downloading B00E7PLDPU from http://ecx.images-amazon.com/images/I/31hypXY2BZL._SY200_.jpg\n",
            "Failed to download B00E7PLDPU: HTTP 404\n",
            "Downloading B007ECBNEY from http://ecx.images-amazon.com/images/I/31oSqmGCalL._SX342_.jpg\n",
            "Downloading B0067VFMZO from http://ecx.images-amazon.com/images/I/31t8kJHULuL.jpg\n",
            "Downloading B00A76OX8W from http://ecx.images-amazon.com/images/I/41pjMZqa1OL._SY200_.jpg\n",
            "Downloading B004FPYVJC from http://ecx.images-amazon.com/images/I/51EWScOjdlL._SX342_.jpg\n",
            "Downloading B005W2G92E from http://ecx.images-amazon.com/images/I/31eZYrS2ccL._SX342_.jpg\n",
            "Downloading B0078LJMUS from http://ecx.images-amazon.com/images/I/31PL96j2csL._SX342_.jpg\n",
            "Downloading B004Z5COA0 from http://ecx.images-amazon.com/images/I/31oqMex%2Bd9L._SX342_.jpg\n",
            "Downloading B00BRBU0NI from http://ecx.images-amazon.com/images/I/41fxIoelwhL._SX342_.jpg\n",
            "Downloading B00FTGZDXY from http://ecx.images-amazon.com/images/I/41eBaSA4YgL._SY445_.jpg\n",
            "Downloading B0098ND9TO from http://ecx.images-amazon.com/images/I/51kf9ZHEInL._SY445_.jpg\n",
            "Downloading B00BIPLHSA from http://ecx.images-amazon.com/images/I/41fCvNFcDIL._SX342_.jpg\n",
            "Downloading B005SN1M74 from http://ecx.images-amazon.com/images/I/417TCi%2BIf0L._SX342_.jpg\n",
            "Downloading B00BJGNAIS from http://ecx.images-amazon.com/images/I/3128IejrE3L._SY445_.jpg\n",
            "Downloading B00AKTD2BU from http://ecx.images-amazon.com/images/I/31TazwHU5-L._SY200_.jpg\n",
            "Failed to download B00AKTD2BU: HTTP 404\n",
            "Downloading B00E36QG82 from http://ecx.images-amazon.com/images/I/41aRkICraEL._SY445_.jpg\n",
            "Downloading B00BU01DQO from http://ecx.images-amazon.com/images/I/31vnbi5idDL._SY200_.jpg\n",
            "Downloading B008FYW62I from http://ecx.images-amazon.com/images/I/41JmvbiXLwL._SX342_.jpg\n",
            "Downloading B00CD630R4 from http://ecx.images-amazon.com/images/I/31mD5xs%2BoBL._SX342_.jpg\n",
            "Downloading B004YD1ZF8 from http://ecx.images-amazon.com/images/I/316mxYggDQL._SY445_.jpg\n",
            "Downloading B00EC135HS from http://ecx.images-amazon.com/images/I/41ru1yz1TUL._SX342_.jpg\n",
            "Downloading B00D535NN8 from http://ecx.images-amazon.com/images/I/41hLvqJKmNL._SY445_.jpg\n",
            "Downloading B00BP407TO from http://g-ecx.images-amazon.com/images/G/01/x-locale/brands/small-logo/2594278011._CB359598537_SX150_.jpg\n",
            "Downloading B00C6E04LI from http://ecx.images-amazon.com/images/I/31aggBgTUwL._SX342_.jpg\n",
            "Downloading B00BF3FAMY from http://ecx.images-amazon.com/images/I/31z-Gfcg-yL._SY445_.jpg\n",
            "Downloading B00763WB3I from http://ecx.images-amazon.com/images/I/41K43Xfv5SL._SX342_.jpg\n",
            "Downloading B0042NOJEY from http://ecx.images-amazon.com/images/I/41AO4x3H3cL._SY445_.jpg\n",
            "Downloading B00AHGAAUW from http://ecx.images-amazon.com/images/I/31vuGGt44nL._SX342_.jpg\n",
            "Failed to download B00AHGAAUW: HTTP 404\n",
            "Downloading B00CMEZ44Y from http://ecx.images-amazon.com/images/I/41wjmhbfzCL._SY445_.jpg\n",
            "Downloading B00AQMU1XI from http://ecx.images-amazon.com/images/I/41k5myzVzqL._SX342_.jpg\n",
            "Downloading B006PA8A9W from http://ecx.images-amazon.com/images/I/41MoQui%2BCyL._SX342_.jpg\n",
            "Downloading B00F2IY0BK from http://ecx.images-amazon.com/images/I/41%2Bkt1HoZhL._SY445_.jpg\n",
            "Downloading B00D50351S from http://ecx.images-amazon.com/images/I/31sfqyPAojL._SY445_.jpg\n",
            "Downloading B0058ZAYAS from http://ecx.images-amazon.com/images/I/31DFUAoxipL._SX342_.jpg\n",
            "Downloading B00BN41QTQ from http://ecx.images-amazon.com/images/I/31m7eM1JrpL._SY445_.jpg\n",
            "Downloading B00D3RBI2Q from http://ecx.images-amazon.com/images/I/31%2BbODdKSUL._SY445_.jpg\n",
            "Downloading B005D9II2K from http://ecx.images-amazon.com/images/I/51OpnCgtBsL._SY445_.jpg\n",
            "Downloading B00C5W165O from http://ecx.images-amazon.com/images/I/31LLIZnHhgL._SY445_.jpg\n",
            "Downloading B00C9AIO8Y from http://ecx.images-amazon.com/images/I/519gJ3h3ClL._SY445_.jpg\n",
            "Downloading B006TAMC2O from http://ecx.images-amazon.com/images/I/51POBIfBqAL._SX342_.jpg\n",
            "Downloading B00741MYMA from http://ecx.images-amazon.com/images/I/31eheljU6nL._SX342_.jpg\n",
            "Downloading B00B4IBVQY from http://ecx.images-amazon.com/images/I/41Ciki1FyQL._SX342_.jpg\n",
            "Downloading B004T7XCOG from http://ecx.images-amazon.com/images/I/41ItBsuu5TL._SX342_.jpg\n",
            "Downloading B0054QYA1K from http://ecx.images-amazon.com/images/I/41LiFHpy8RL._SY445_.jpg\n",
            "Downloading B003FGW928 from http://ecx.images-amazon.com/images/I/41YtBx%2BPlKL._SX342_.jpg\n",
            "Downloading B00C7PAYRA from http://ecx.images-amazon.com/images/I/41rxN2t0zjL._SY445_.jpg\n",
            "Downloading B0099O258S from http://ecx.images-amazon.com/images/I/51OyYJyWl2L._SY445_.jpg\n",
            "Downloading B004L2L9SK from http://ecx.images-amazon.com/images/I/41qn8nPAlTL._SX342_.jpg\n",
            "Downloading B008PFWYZ6 from http://ecx.images-amazon.com/images/I/31g8lnce3BL._SX342_.jpg\n",
            "Downloading B00BTLE41K from http://ecx.images-amazon.com/images/I/41KB8YVbgiL._SY445_.jpg\n",
            "Downloading B004I6PIPY from http://ecx.images-amazon.com/images/I/41vGhLH-gLL._SY445_.jpg\n",
            "Downloading B00CDCD4GK from http://ecx.images-amazon.com/images/I/41QkZODTEYL._SX342_.jpg\n",
            "Downloading B006MW3CTG from http://ecx.images-amazon.com/images/I/41LD7vrlXkL._SY445_.jpg\n",
            "Downloading B00C6KR7A8 from http://ecx.images-amazon.com/images/I/41gN5Z6TcOL._SY445_.jpg\n",
            "Downloading B00BIPL352 from http://ecx.images-amazon.com/images/I/41Hp1RPPmkL._SY445_.jpg\n",
            "Downloading B00B9Z4CTA from http://ecx.images-amazon.com/images/I/31s2W7GEDuL._SY445_.jpg\n",
            "Downloading B00DI7BZI8 from http://ecx.images-amazon.com/images/I/41tOUtr0lLL._SY200_.jpg\n",
            "Failed to download B00DI7BZI8: HTTP 404\n",
            "Downloading B000ENMR3Q from http://ecx.images-amazon.com/images/I/31C-Pi8QRKL._SX342_.jpg\n",
            "Downloading B008UAEXZK from http://ecx.images-amazon.com/images/I/41KN39n18jL._SX342_.jpg\n",
            "Downloading B00C2TOOYA from http://ecx.images-amazon.com/images/I/31TddoIMMGL._SX342_.jpg\n",
            "Downloading B007XXL5RO from http://ecx.images-amazon.com/images/I/41dAPg-yg4L._SY445_.jpg\n",
            "Downloading B004UJHLQI from http://ecx.images-amazon.com/images/I/31qmr4wWh%2BL._SY445_.jpg\n",
            "Downloading B00A1C4INC from http://ecx.images-amazon.com/images/I/41lHLDdVv6L._SY445_.jpg\n",
            "Downloading B005O7EVRC from http://ecx.images-amazon.com/images/I/419YPIyVM3L._SX342_.jpg\n",
            "Downloading B0077IM8EE from http://ecx.images-amazon.com/images/I/31uxsib-UdL._SX342_.jpg\n",
            "Downloading B009KXNOKQ from http://ecx.images-amazon.com/images/I/41BWksGD73L._SX342_.jpg\n",
            "Downloading B004RDQB6I from http://ecx.images-amazon.com/images/I/31XBP8tVrpL._SX342_.jpg\n",
            "Downloading B00B1OSUM4 from http://ecx.images-amazon.com/images/I/31AN9iKFiaL._SY445_.jpg\n",
            "Downloading B0093UNBXQ from http://ecx.images-amazon.com/images/I/318lvT3sflL._SX342_.jpg\n",
            "Downloading B0089XS16K from http://ecx.images-amazon.com/images/I/315r06IwJhL._SX342_.jpg\n",
            "Downloading B003TW4OTO from http://ecx.images-amazon.com/images/I/41t%2Bar%2BSE2L._SX342_.jpg\n",
            "Downloading B004SKFM8S from http://ecx.images-amazon.com/images/I/51aAaklab5L._SX342_.jpg\n",
            "Downloading B00BSW48ZM from http://ecx.images-amazon.com/images/I/41QHzmj28XL._SX342_.jpg\n",
            "Downloading B00604Z1UY from http://ecx.images-amazon.com/images/I/41oIiFPQZCL._SY445_.jpg\n",
            "Downloading B00DEKZPR6 from http://ecx.images-amazon.com/images/I/31SZDOXTgvL._SY445_.jpg\n",
            "Downloading B008542KMO from http://ecx.images-amazon.com/images/I/416WXlhtKmL._SY445_.jpg\n",
            "Downloading B004A7XTKM from http://ecx.images-amazon.com/images/I/41A7GMZWXpL._SX342_.jpg\n",
            "Downloading B007ZZ3Q0O from http://ecx.images-amazon.com/images/I/31xjaLIn6iL._SX342_.jpg\n",
            "Downloading B00DRCJ270 from http://ecx.images-amazon.com/images/I/41Qdkw5lELL._SY200_.jpg\n",
            "Failed to download B00DRCJ270: HTTP 404\n",
            "Downloading B00B2H8VGA from http://ecx.images-amazon.com/images/I/31RLdg%2Be32L._SY200_.jpg\n",
            "Downloading B00740VYFE from http://ecx.images-amazon.com/images/I/31QkOJau0IL._SX342_.jpg\n",
            "Downloading B00DUR5CF8 from http://ecx.images-amazon.com/images/I/41-M3fNPfCL._SX342_.jpg\n",
            "Failed to download B00DUR5CF8: HTTP 404\n",
            "Downloading B004QGY0Q4 from http://ecx.images-amazon.com/images/I/51i0BKR3syL._SX342_.jpg\n",
            "Downloading B008MZHGMA from http://ecx.images-amazon.com/images/I/51KgdaraL0L._SX342_.jpg\n",
            "Downloading B00CEX5WV8 from http://ecx.images-amazon.com/images/I/4106IvemToL._SY445_.jpg\n",
            "Downloading B008YQV4KC from http://ecx.images-amazon.com/images/I/31wSQ04oJxL._SY445_.jpg\n",
            "Downloading B006Q97NY0 from http://ecx.images-amazon.com/images/I/412toqe4AML._SX342_.jpg\n",
            "Downloading B00CBDQWM4 from http://ecx.images-amazon.com/images/I/41bFwjU9xKL._SY200_.jpg\n",
            "Failed to download B00CBDQWM4: HTTP 404\n",
            "Downloading B0084OF18A from http://ecx.images-amazon.com/images/I/41Afc40704L._SX342_.jpg\n",
            "Downloading B007K4YUFK from http://ecx.images-amazon.com/images/I/41rqUYl-R4L._SX342_.jpg\n",
            "Downloading B00FNZN4BY from http://ecx.images-amazon.com/images/I/412kwz0bMTL._SY445_.jpg\n",
            "Downloading B00BBGMM1M from http://ecx.images-amazon.com/images/I/31ZUMUScJ8L._SY445_.jpg\n",
            "Downloading B00938V022 from http://ecx.images-amazon.com/images/I/41lBk1VfVfL._SX342_.jpg\n",
            "Downloading B0075NK230 from http://ecx.images-amazon.com/images/I/41wlRNgCBNL._SX342_.jpg\n",
            "Downloading B007CTM8S4 from http://ecx.images-amazon.com/images/I/31-KRXBftqL._SX342_.jpg\n",
            "Downloading B00C6743D0 from http://ecx.images-amazon.com/images/I/51fm3gRZ31L._SY200_.jpg\n",
            "Downloading B00915L5R2 from http://ecx.images-amazon.com/images/I/31RIy9vR-uL._SY445_.jpg\n",
            "Downloading B0037UZVOA from http://ecx.images-amazon.com/images/I/31zAlc0-35L._SX342_.jpg\n",
            "Downloading B0080N7Y1C from http://ecx.images-amazon.com/images/I/41uAf%2BcKxGL._SY445_.jpg\n",
            "Downloading B007S3V8QC from http://ecx.images-amazon.com/images/I/31OjvwunUdL._SX342_.jpg\n",
            "Downloading B0055QA66M from http://ecx.images-amazon.com/images/I/41WsIU0CfUL._SX342_.jpg\n",
            "Downloading B0091GQSAK from http://ecx.images-amazon.com/images/I/41bUcDAlHgL._SX342_.jpg\n",
            "Downloading B00ADRIN72 from http://ecx.images-amazon.com/images/I/41KdU2mVCML._SX342_.jpg\n",
            "Downloading B009VRTOG4 from http://ecx.images-amazon.com/images/I/41rftzwKPvL._SY445_.jpg\n",
            "Downloading B00FB79ZDU from http://ecx.images-amazon.com/images/I/31HTond5AGL._SX342_.jpg\n",
            "Failed to download B00FB79ZDU: HTTP 404\n",
            "Downloading B00CIOJR5K from http://ecx.images-amazon.com/images/I/31722fWTnZL._SY445_.jpg\n",
            "Downloading B00CDZNKIY from http://ecx.images-amazon.com/images/I/41OdJB2lJsL._SY445_.jpg\n",
            "Downloading B005OOGULA from http://ecx.images-amazon.com/images/I/31CVlMVvNlL._SY445_.jpg\n",
            "Downloading B00D5ODKOQ from http://ecx.images-amazon.com/images/I/314MmMlCRLL._SY200_.jpg\n",
            "Downloading B00AES91JE from http://ecx.images-amazon.com/images/I/41jq0gbi85L._SY445_.jpg\n",
            "Downloading B008PCI0Z2 from http://ecx.images-amazon.com/images/I/313OCQJ9XpL._SX342_.jpg\n",
            "Downloading B007U92552 from http://ecx.images-amazon.com/images/I/31zE58TX0PL._SY445_.jpg\n",
            "Downloading B004QTQ8EI from http://ecx.images-amazon.com/images/I/517FhM5wf0L._SX342_.jpg\n",
            "Downloading B00BMH6GXA from http://ecx.images-amazon.com/images/I/31YYaIva7sL._SY445_.jpg\n",
            "Downloading B0060OU0P0 from http://ecx.images-amazon.com/images/I/41dxaIcQH2L._SX342_.jpg\n",
            "Downloading B007WA1RV2 from http://ecx.images-amazon.com/images/I/31rUyaF3XXL._SX342_.jpg\n",
            "Downloading B0065589JI from http://ecx.images-amazon.com/images/I/41EW5dcqMVL._SY445_.jpg\n",
            "Downloading B004Y45XQO from http://ecx.images-amazon.com/images/I/41ZYNWWjyJL._SX342_.jpg\n",
            "Downloading B009M1V0AM from http://ecx.images-amazon.com/images/I/41RInNzypEL._SX342_.jpg\n",
            "Downloading B003UV9XTA from http://ecx.images-amazon.com/images/I/41YBuGKOg0L._SX342_.jpg\n",
            "Downloading B00CC6ZKPA from http://ecx.images-amazon.com/images/I/31F7vdAsO4L._SY445_.jpg\n",
            "Downloading B00BWISZDS from http://ecx.images-amazon.com/images/I/41XhXDCuY5L._SY200_.jpg\n",
            "Failed to download B00BWISZDS: HTTP 404\n",
            "Downloading B004J254Z6 from http://ecx.images-amazon.com/images/I/41OdiJEIncL._SY445_.jpg\n",
            "Downloading B004G6YFHS from http://ecx.images-amazon.com/images/I/41bIZllLOrL._SY445_.jpg\n",
            "Downloading B007V1QDNO from http://ecx.images-amazon.com/images/I/51CtN9qOH0L._SX342_.jpg\n",
            "Downloading B008D7EARG from http://ecx.images-amazon.com/images/I/31Y5PqjDMRL._SX342_.jpg\n",
            "Downloading B0098BUXPE from http://ecx.images-amazon.com/images/I/31xsYLAGoML._SY445_.jpg\n",
            "Downloading B008ZXTX84 from http://ecx.images-amazon.com/images/I/41HI76ukpML._SX342_.jpg\n",
            "Downloading B00DW1MW4G from http://ecx.images-amazon.com/images/I/41PbwNaXwlL._SY445_.jpg\n",
            "Downloading B00GMRNQ1U from http://ecx.images-amazon.com/images/I/41kPp135PZL._SX342_.jpg\n",
            "Downloading B006OZCKNU from http://ecx.images-amazon.com/images/I/41Khxp%2BByzL._SY445_.jpg\n",
            "Downloading B009JY5ABM from http://ecx.images-amazon.com/images/I/31H1mBnmd8L._SY445_.jpg\n",
            "Downloading B00BRBEBE2 from http://ecx.images-amazon.com/images/I/41Eok2pgbfL._SX342_.jpg\n",
            "Downloading B006VPSX0W from http://ecx.images-amazon.com/images/I/510G3P-7XjL._SY445_.jpg\n",
            "Downloading B00CXWW43E from http://ecx.images-amazon.com/images/I/4138PZgxYdL._SX342_.jpg\n",
            "Failed to download B00CXWW43E: HTTP 404\n",
            "Downloading B008IGRCNG from http://ecx.images-amazon.com/images/I/41r6T%2BzhZ5L._SX342_.jpg\n",
            "Downloading B006OP4GUK from http://ecx.images-amazon.com/images/I/41N2CHwAFoL._SX342_.jpg\n",
            "Downloading B00B7DZNG0 from http://ecx.images-amazon.com/images/I/41up010GVJL._SX342_.jpg\n",
            "Downloading B00EPQR8IC from http://ecx.images-amazon.com/images/I/417Iu5FZ%2BcL._SY445_.jpg\n",
            "Downloading B004KHVL5C from http://ecx.images-amazon.com/images/I/418EPpEIxML._SY445_.jpg\n",
            "Downloading B00E5ASNDW from http://ecx.images-amazon.com/images/I/31E7RW%2BCf5L._SY200_.jpg\n",
            "Failed to download B00E5ASNDW: HTTP 404\n",
            "Downloading B00AKW5QKW from http://ecx.images-amazon.com/images/I/31ddE7mz8zL._SY200_.jpg\n",
            "Failed to download B00AKW5QKW: HTTP 404\n",
            "Downloading B00C6SV5CG from http://ecx.images-amazon.com/images/I/41l6a83dheL._SX342_.jpg\n",
            "Downloading B00CJTB16C from http://ecx.images-amazon.com/images/I/411P-Jf%2BL3L._SY445_.jpg\n",
            "Downloading B007EMWDDO from http://ecx.images-amazon.com/images/I/51cislY276L._SX342_.jpg\n",
            "Downloading B0073114DG from http://ecx.images-amazon.com/images/I/31Gt3qdr6fL._SY445_.jpg\n",
            "Downloading B00DM2FZMQ from http://ecx.images-amazon.com/images/I/31g6nRWKv0L._SY200_.jpg\n",
            "Failed to download B00DM2FZMQ: HTTP 404\n",
            "Downloading B004PE707I from http://ecx.images-amazon.com/images/I/317zoeIYJiL._SY445_.jpg\n",
            "Downloading B0030MHYHW from http://ecx.images-amazon.com/images/I/41xMkGFmCPL._SX342_.jpg\n",
            "Downloading B000OVR5E4 from http://ecx.images-amazon.com/images/I/31SLoT%2BFlYL._SX342_.jpg\n",
            "Downloading B008589FU0 from http://ecx.images-amazon.com/images/I/41CRh82DyVL._SX342_.jpg\n",
            "Downloading B003TLODL4 from http://ecx.images-amazon.com/images/I/312xmfEwhXL._SX342_.jpg\n",
            "Downloading B006VA9CSE from http://ecx.images-amazon.com/images/I/41Jum4VqVGL._SX342_.jpg\n",
            "Downloading B00A7B7BH2 from http://ecx.images-amazon.com/images/I/41yR6zj%2BDSL._SY445_.jpg\n",
            "Downloading B007XCGVR4 from http://ecx.images-amazon.com/images/I/41H9sDil7IL._SX342_.jpg\n",
            "Downloading B004A7XTRK from http://ecx.images-amazon.com/images/I/41mEmPOq0fL._SX342_.jpg\n",
            "Downloading B00BQX40X8 from http://ecx.images-amazon.com/images/I/41oiNYZryzL._SY200_.jpg\n",
            "Downloading B009RVDEMO from http://ecx.images-amazon.com/images/I/31sFuTZdQAL._SY200_.jpg\n",
            "Downloading B008QW0CPW from http://ecx.images-amazon.com/images/I/41f2EdBu63L._SX342_.jpg\n",
            "Downloading B00C5TO0MI from http://ecx.images-amazon.com/images/I/31yMsywfUcL._SY445_.jpg\n",
            "Downloading B005541UQO from http://ecx.images-amazon.com/images/I/31D8AOrsbpL._SY445_.jpg\n",
            "Downloading B00BI4KLU6 from http://ecx.images-amazon.com/images/I/41yAdxGdbTL._SX342_.jpg\n",
            "Downloading B006GG0G6A from http://ecx.images-amazon.com/images/I/41cEuV8E8OL._SX342_.jpg\n",
            "Downloading B00B71DFJO from http://ecx.images-amazon.com/images/I/31GiFayRFfL._SY445_.jpg\n",
            "Downloading B009CSI5F8 from http://ecx.images-amazon.com/images/I/31NFSn1KLvL._SX342_.jpg\n",
            "Downloading B00DMSENKA from http://ecx.images-amazon.com/images/I/41CO-NW-dnL._SX342_.jpg\n",
            "Downloading B00E9CECW2 from http://ecx.images-amazon.com/images/I/413%2BJjvSY2L._SY445_.jpg\n",
            "Downloading B00G0SF448 from http://ecx.images-amazon.com/images/I/51b5JHmbnGL._SX342_.jpg\n",
            "Downloading B004U88WKI from http://ecx.images-amazon.com/images/I/51idyE2GvIL._SX342_.jpg\n",
            "Downloading B00F3PXWAC from http://ecx.images-amazon.com/images/I/41U%2B9HWyYXL._SY445_.jpg\n",
            "Downloading B008N0L2DI from http://ecx.images-amazon.com/images/I/41eRQadAT5L._SX342_.jpg\n",
            "Downloading B006UDKAH4 from http://ecx.images-amazon.com/images/I/31GNuRyUijL._SY445_.jpg\n",
            "Downloading B00E0CYF9G from http://ecx.images-amazon.com/images/I/41IhrkRm%2BWL._SY445_.jpg\n",
            "Downloading B00591JN8A from http://ecx.images-amazon.com/images/I/41czFTMRKRL._SY445_.jpg\n",
            "Downloading B00FGSWUU4 from http://ecx.images-amazon.com/images/I/31zeGi0kgcL._SY445_.jpg\n",
            "Downloading B008P46ACU from http://ecx.images-amazon.com/images/I/41md1h0AMlL._SY445_.jpg\n",
            "Downloading B00CHSEGYE from http://ecx.images-amazon.com/images/I/31jBkknzSnL._SY445_.jpg\n",
            "Downloading B005CJKODM from http://ecx.images-amazon.com/images/I/31WczgZw1tL._SX342_.jpg\n",
            "Downloading B004IGJ580 from http://ecx.images-amazon.com/images/I/41yg76aNYfL._SY445_.jpg\n",
            "Downloading B00EWLJGU8 from http://ecx.images-amazon.com/images/I/41LMTByCMkL._SX342_.jpg\n",
            "Downloading B00BT0P7LC from http://ecx.images-amazon.com/images/I/41Yyby48hOL._SX342_.jpg\n",
            "Downloading B004YHIB1U from http://ecx.images-amazon.com/images/I/41PiDJ-zYPL._SY445_.jpg\n",
            "Downloading B00A8E8W1M from http://ecx.images-amazon.com/images/I/31xwiz2u1xL._SY200_.jpg\n",
            "Downloading B005VF72DW from http://ecx.images-amazon.com/images/I/41c8njP9DPL._SY445_.jpg\n",
            "Downloading B00A1AJ9TW from http://ecx.images-amazon.com/images/I/31kfLxdwuPL._SY200_.jpg\n",
            "Downloading B00B3QB7GQ from http://ecx.images-amazon.com/images/I/41yaNsUNsIL._SX342_.jpg\n",
            "Downloading B003U6Y7NW from http://ecx.images-amazon.com/images/I/41Vr1hrsJeL._SX342_.jpg\n",
            "Downloading B00EDTN7FE from http://ecx.images-amazon.com/images/I/31fUMX58NyL._SX342_.jpg\n",
            "Downloading B005OOJXNM from http://ecx.images-amazon.com/images/I/41lppSVWBFL._SY445_.jpg\n",
            "Downloading B00AKTD6I4 from http://ecx.images-amazon.com/images/I/413%2B52u%2B6EL._SY200_.jpg\n",
            "Downloading B008LR38J4 from http://ecx.images-amazon.com/images/I/41c-0x-JV0L._SY200_.jpg\n",
            "Failed to download B008LR38J4: HTTP 404\n",
            "Downloading B00D3F802E from http://ecx.images-amazon.com/images/I/410%2BbPoFwuL._SY200_.jpg\n",
            "Failed to download B00D3F802E: HTTP 404\n",
            "Downloading B002Y2T6K2 from http://ecx.images-amazon.com/images/I/415haJ8apIL._SY445_.jpg\n",
            "Downloading B00385WOW6 from http://ecx.images-amazon.com/images/I/31CBaaYW24L._SX342_.jpg\n",
            "Downloading B00AIXNAHY from http://ecx.images-amazon.com/images/I/412IH0Tt%2BzL._SY445_.jpg\n",
            "Downloading B00FNZNSJM from http://ecx.images-amazon.com/images/I/41cq96s5WGL._SY445_.jpg\n",
            "Downloading B003Q6CGZM from http://ecx.images-amazon.com/images/I/31Rc7uX6oqL._SX342_.jpg\n",
            "Downloading B0088A1GCA from http://ecx.images-amazon.com/images/I/410KZu4CmTL._SX342_.jpg\n",
            "Downloading B004M8T7UA from http://ecx.images-amazon.com/images/I/31ndgiVhevL._SX342_.jpg\n",
            "Downloading B00E9SDZVU from http://ecx.images-amazon.com/images/I/41vQbD%2BiihL._SX342_.jpg\n",
            "Failed to download B00E9SDZVU: HTTP 404\n",
            "Downloading B005VG5K26 from http://ecx.images-amazon.com/images/I/41PY3eqh0KL._SX342_.jpg\n",
            "Downloading B00EE0Y4AY from http://ecx.images-amazon.com/images/I/4143GiDeEsL._SX342_.jpg\n",
            "Failed to download B00EE0Y4AY: HTTP 404\n",
            "Downloading B004SKFLY8 from http://ecx.images-amazon.com/images/I/41fRLVy970L._SX342_.jpg\n",
            "Downloading B0061FHVSM from http://ecx.images-amazon.com/images/I/51VIeFaf4qL._SX342_.jpg\n",
            "Downloading B006R3DW1I from http://ecx.images-amazon.com/images/I/41iFcXCApSL._SX342_.jpg\n",
            "Downloading B00D73QYSY from http://ecx.images-amazon.com/images/I/51iixht5M-L._SY445_.jpg\n",
            "Downloading B00APG4A32 from http://ecx.images-amazon.com/images/I/31BK2WFQdGL._SY200_.jpg\n",
            "Downloading B00G7H39YY from http://ecx.images-amazon.com/images/I/41ox20nTVZL._SY445_.jpg\n",
            "Downloading B00DFY1X7C from http://ecx.images-amazon.com/images/I/316YRmqmTaL._SY445_.jpg\n",
            "Downloading B00EV1B9C2 from http://ecx.images-amazon.com/images/I/31leF4zsNiL._SY445_.jpg\n",
            "Downloading B00767OUTM from http://ecx.images-amazon.com/images/I/31sVCVWF7GL._SX342_.jpg\n",
            "Downloading B00B5TTFDI from http://ecx.images-amazon.com/images/I/41eP7epvxuL._SY445_.jpg\n",
            "Downloading B00AKOUZPQ from http://ecx.images-amazon.com/images/I/4110CdQhwxL._SY200_.jpg\n",
            "Downloading B008ODSCVO from http://ecx.images-amazon.com/images/I/31PRXJn16VL._SX342_.jpg\n",
            "Downloading B009LIFFX4 from http://ecx.images-amazon.com/images/I/41vXRTLapzL._SX342_.jpg\n",
            "Downloading B00DDVV6HY from http://ecx.images-amazon.com/images/I/31uXYjEFJoL._SY445_.jpg\n",
            "Downloading B007PCR54A from http://ecx.images-amazon.com/images/I/31iy0YZUviL._SX342_.jpg\n",
            "Downloading B007GC3IUE from http://ecx.images-amazon.com/images/I/41LbSbnhCfL._SX342_.jpg\n",
            "Downloading B004M6J946 from http://ecx.images-amazon.com/images/I/51gDO4T3a7L._SX342_.jpg\n",
            "Downloading B008Q08JOA from http://ecx.images-amazon.com/images/I/41TiqZVx9LL._SY445_.jpg\n",
            "Downloading B007U4S7O0 from http://ecx.images-amazon.com/images/I/31TF%2BAm8ifL._SX342_.jpg\n",
            "Downloading B00BC9QW5K from http://ecx.images-amazon.com/images/I/41FaBn3f5nL._SY445_.jpg\n",
            "Downloading B00FWGU18S from http://ecx.images-amazon.com/images/I/41pWGKoIW3L._SY445_.jpg\n",
            "Downloading B006G2AJFM from http://ecx.images-amazon.com/images/I/41ppGoS1prL._SX342_.jpg\n",
            "Downloading B004T6S94K from http://ecx.images-amazon.com/images/I/41kCixfyGyL._SY445_.jpg\n",
            "Downloading B00CAUYBWQ from http://ecx.images-amazon.com/images/I/51mc5SZa44L._SX342_.jpg\n",
            "Downloading B008OVPXWM from http://ecx.images-amazon.com/images/I/41A0UDoxF3L._SX342_.jpg\n",
            "Downloading B00AA0CT7C from http://ecx.images-amazon.com/images/I/311prA0OFnL._SY445_.jpg\n",
            "Downloading B00A9SQCQ4 from http://ecx.images-amazon.com/images/I/31xYZK6Z3wL._SX342_.jpg\n",
            "Downloading B008BF7XOW from http://ecx.images-amazon.com/images/I/41J3SFNriOL._SY445_.jpg\n",
            "Downloading B006SEUACA from http://ecx.images-amazon.com/images/I/41SZwOJRGgL._SX342_.jpg\n",
            "Downloading B006DKKCEK from http://ecx.images-amazon.com/images/I/51AgZqkL6zL._SY445_.jpg\n",
            "Downloading B0060I60U0 from http://ecx.images-amazon.com/images/I/41s0OR7sfZL._SX342_.jpg\n",
            "Downloading B006ZQX8RK from http://ecx.images-amazon.com/images/I/41laqUmYExL._SY445_.jpg\n",
            "Downloading B002QUSLZI from http://ecx.images-amazon.com/images/I/419fsHl0JcL._SY445_.jpg\n",
            "Downloading B00CIQNITY from http://ecx.images-amazon.com/images/I/41toLEMugUL._SX342_.jpg\n",
            "Downloading B00A13ES0O from http://ecx.images-amazon.com/images/I/31ICeMzuwgL._SX342_.jpg\n",
            "Downloading B0036DE0KY from http://ecx.images-amazon.com/images/I/418ESyuMwmL._SX342_.jpg\n",
            "Downloading B00BJSFGFG from http://ecx.images-amazon.com/images/I/41DKs2MR7QL._SY445_.jpg\n",
            "Downloading B003AM7L0C from http://ecx.images-amazon.com/images/I/31tbBSolc7L._SX342_.jpg\n",
            "Downloading B004CR5N18 from http://ecx.images-amazon.com/images/I/41aPioBCLpL._SX342_.jpg\n",
            "Downloading B0091J14CE from http://ecx.images-amazon.com/images/I/41TblkIo8SL._SY445_.jpg\n",
            "Downloading B009CMY9UY from http://ecx.images-amazon.com/images/I/413fE8z9yuL._SX342_.jpg\n",
            "Downloading B00GT2KXEG from http://ecx.images-amazon.com/images/I/41HLts1QMHL._SX342_.jpg\n",
            "Downloading B00CGYK8OG from http://ecx.images-amazon.com/images/I/41IZyU83cwL._SX342_.jpg\n",
            "Downloading B0035EABHU from http://ecx.images-amazon.com/images/I/319zLgoZhfL._SX342_.jpg\n",
            "Downloading B009YK9DK0 from http://ecx.images-amazon.com/images/I/31-EPURLwOL._SY200_.jpg\n",
            "Failed to download B009YK9DK0: HTTP 404\n",
            "Downloading B008200YHE from http://ecx.images-amazon.com/images/I/317CMgOm8OL._SY445_.jpg\n",
            "Downloading B0051USZFQ from http://ecx.images-amazon.com/images/I/31RNAPZfEKL._SY445_.jpg\n",
            "Downloading B00CV18O18 from http://ecx.images-amazon.com/images/I/31tifvk0vTL._SY200_.jpg\n",
            "Failed to download B00CV18O18: HTTP 404\n",
            "Downloading B002OE8DAO from http://ecx.images-amazon.com/images/I/41H11dlWLLL._SX342_.jpg\n",
            "Downloading B004XYPNA6 from http://ecx.images-amazon.com/images/I/31gn4HLMGvL._SX342_.jpg\n",
            "Downloading B005GBWRWM from http://ecx.images-amazon.com/images/I/41rYeW53nyL._SX342_.jpg\n",
            "Downloading B004PE9DVE from http://ecx.images-amazon.com/images/I/31vraZmq7GL._SY445_.jpg\n",
            "Downloading B005EVNHD2 from http://ecx.images-amazon.com/images/I/41KvAsuSwJL._SX342_.jpg\n",
            "Downloading B00CRNKD6E from http://ecx.images-amazon.com/images/I/41iyELwHSxL._SY445_.jpg\n",
            "Downloading B008KKOVI4 from http://ecx.images-amazon.com/images/I/31altZfkELL._SY445_.jpg\n",
            "Downloading B003TLO3GO from http://ecx.images-amazon.com/images/I/41RcMXIh2eL._SX342_.jpg\n",
            "Downloading B004HIMKZY from http://ecx.images-amazon.com/images/I/41UtY7EEi3L._SX342_.jpg\n",
            "Downloading B008R4Y0D4 from http://ecx.images-amazon.com/images/I/41bkp%2BlQH8L._SX342_.jpg\n",
            "Downloading B009MB1LMO from http://ecx.images-amazon.com/images/I/41QTPtlRKtL._SX342_.jpg\n",
            "Failed to download B009MB1LMO: HTTP 404\n",
            "Downloading B00CFSX110 from http://ecx.images-amazon.com/images/I/41GeTJK3y1L._SY445_.jpg\n",
            "Downloading B008PIFRR0 from http://ecx.images-amazon.com/images/I/31m6AiVJLkL._SY445_.jpg\n",
            "Downloading B006Z3H180 from http://ecx.images-amazon.com/images/I/41aUw8qu00L._SY445_.jpg\n",
            "Downloading B009Y6PZ9M from http://ecx.images-amazon.com/images/I/41ODaDNC4qL._SY445_.jpg\n",
            "Downloading B00C6PPT6W from http://ecx.images-amazon.com/images/I/41sFVlnG01L._SY445_.jpg\n",
            "Downloading B00935MSN0 from http://ecx.images-amazon.com/images/I/51wRpbL17YL._SX342_.jpg\n",
            "Downloading B00DEECQW4 from http://ecx.images-amazon.com/images/I/41H8MrMoRqL._SY445_.jpg\n",
            "Downloading B00AIXPFR2 from http://ecx.images-amazon.com/images/I/41LpGiLcjaL._SY445_.jpg\n",
            "Downloading B00B78D542 from http://ecx.images-amazon.com/images/I/411XcLAj-GL._SX342_.jpg\n",
            "Downloading B00B89OLCK from http://ecx.images-amazon.com/images/I/31K05MRavrL._SX342_.jpg\n",
            "Downloading B00BRCG24I from http://ecx.images-amazon.com/images/I/41pq2NIeqGL._SY200_.jpg\n",
            "Downloading B00AAILTPW from http://ecx.images-amazon.com/images/I/31y3q-baQ5L._SY200_.jpg\n",
            "Failed to download B00AAILTPW: HTTP 404\n",
            "Downloading B008S4CQCU from http://ecx.images-amazon.com/images/I/31gesZMU77L._SX342_.jpg\n",
            "Downloading B0060ROCF6 from http://ecx.images-amazon.com/images/I/31ccHYx5bKL._SX342_.jpg\n",
            "Downloading B009OJ3WAS from http://ecx.images-amazon.com/images/I/31jPsYMzNkL._SX342_.jpg\n",
            "Downloading B008F4PN9Q from http://ecx.images-amazon.com/images/I/41rbMTVdGqL._SX342_.jpg\n",
            "Downloading B00C0WT88G from http://ecx.images-amazon.com/images/I/515x36GR9fL._SY445_.jpg\n",
            "Downloading B005OHAODC from http://ecx.images-amazon.com/images/I/31Ag5vzMWcL._SX342_.jpg\n",
            "Downloading B00CLV9I90 from http://ecx.images-amazon.com/images/I/41WU669Ui7L._SX342_.jpg\n",
            "Downloading B00CAA6XV8 from http://ecx.images-amazon.com/images/I/41gBcNaVMuL._SY445_.jpg\n",
            "Downloading B00867V5UI from http://ecx.images-amazon.com/images/I/31JaVAYkhhL._SY445_.jpg\n",
            "Downloading B00EZHWUZM from http://ecx.images-amazon.com/images/I/41fmh6JKEEL._SX342_.jpg\n",
            "Downloading B003YD6YUU from http://ecx.images-amazon.com/images/I/413z-L4TE5L._SX342_.jpg\n",
            "Downloading B0013R0I88 from http://ecx.images-amazon.com/images/I/41CnyMlG5GL._SX342_.jpg\n",
            "Downloading B005KOYR50 from http://ecx.images-amazon.com/images/I/41FFy4Q%2BKGL._SX342_.jpg\n",
            "Downloading B00APSRH44 from http://ecx.images-amazon.com/images/I/31ZvsrEL1JL._SY200_.jpg\n",
            "Failed to download B00APSRH44: HTTP 404\n",
            "Downloading B004ZWGH7E from http://ecx.images-amazon.com/images/I/41CzrY5eR5L._SY445_.jpg\n",
            "Downloading B00CHO0TQC from http://ecx.images-amazon.com/images/I/41%2BTKd9IX%2BL._SY445_.jpg\n",
            "Downloading B00DZ19W1O from http://ecx.images-amazon.com/images/I/4176vXhBfAL._SY445_.jpg\n",
            "Downloading B00B3EHXH0 from http://ecx.images-amazon.com/images/I/31wzJxJapOL._SY445_.jpg\n",
            "Downloading B004FPYVSI from http://ecx.images-amazon.com/images/I/418tzwcuVAL._SX342_.jpg\n",
            "Downloading B007FH44RG from http://ecx.images-amazon.com/images/I/41EMiNOMo3L._SX342_.jpg\n",
            "Downloading B008DVXFPA from http://ecx.images-amazon.com/images/I/41HOvbrC-4L._SX342_.jpg\n",
            "Downloading B00821TA0A from http://ecx.images-amazon.com/images/I/31l3mrI4zWL._SY445_.jpg\n",
            "Downloading B006TVWJ4O from http://ecx.images-amazon.com/images/I/31FHpwpZ%2BSL._SX342_.jpg\n",
            "Downloading B00AHGAONU from http://ecx.images-amazon.com/images/I/31sqX-T1fTL._SY300_.jpg\n",
            "Failed to download B00AHGAONU: HTTP 404\n",
            "Downloading B0093ZDQM2 from http://ecx.images-amazon.com/images/I/41fPCUHVk6L._SX342_.jpg\n",
            "Downloading B004I6M7HQ from http://ecx.images-amazon.com/images/I/41MiJ92dOkL._SY445_.jpg\n",
            "Downloading B009C36JR4 from http://ecx.images-amazon.com/images/I/41JEZWCuoML._SY445_.jpg\n",
            "Downloading B00DTJRTAS from http://ecx.images-amazon.com/images/I/31EapQgFcNL._SY445_.jpg\n",
            "Downloading B006J431UM from http://ecx.images-amazon.com/images/I/41mTj2OlbNL._SX342_.jpg\n",
            "Downloading B008Q03I2I from http://ecx.images-amazon.com/images/I/41oSn6kM%2BqL._SY445_.jpg\n",
            "Downloading B0056F24M6 from http://ecx.images-amazon.com/images/I/31BlH42thLL._SY445_.jpg\n",
            "Downloading B00B513P1E from http://ecx.images-amazon.com/images/I/51bow1gNdzL._SX342_.jpg\n",
            "Downloading B004FPIBPW from http://ecx.images-amazon.com/images/I/41uxo2cvdCL._SX342_.jpg\n",
            "Downloading B005KP5M26 from http://ecx.images-amazon.com/images/I/416%2BCbgD7AL._SX342_.jpg\n",
            "Downloading B00B1TODH0 from http://ecx.images-amazon.com/images/I/31iTrfaCKKL._SY200_.jpg\n",
            "Failed to download B00B1TODH0: HTTP 404\n",
            "Downloading B00CJADW5E from http://ecx.images-amazon.com/images/I/31qzwwi5tdL._SY445_.jpg\n",
            "Downloading B00AQ8ANLW from http://ecx.images-amazon.com/images/I/412foBIkQ7L._SY445_.jpg\n",
            "Downloading B006VZXL36 from http://ecx.images-amazon.com/images/I/31QLJ4e3RjL._SX342_.jpg\n",
            "Downloading B0039YOYNI from http://ecx.images-amazon.com/images/I/41tiah1Bo-L._SX342_.jpg\n",
            "Downloading B009311IJY from http://ecx.images-amazon.com/images/I/41bIJFiDc-L._SX342_.jpg\n",
            "Downloading B00943OWKS from http://ecx.images-amazon.com/images/I/31F9ETzelzL._SY445_.jpg\n",
            "Downloading B00CEKI6VE from http://ecx.images-amazon.com/images/I/41tQI0qbvRL._SY445_.jpg\n",
            "Downloading B00870NI2C from http://ecx.images-amazon.com/images/I/41TcumI3UqL._SX342_.jpg\n",
            "Downloading B008PAL0TC from http://ecx.images-amazon.com/images/I/31dpZ73xBIL._SX342_.jpg\n",
            "Downloading B004X9MAWA from http://ecx.images-amazon.com/images/I/41J4YgGifQL._SX342_.jpg\n",
            "Downloading B008R9FD2G from http://ecx.images-amazon.com/images/I/41F-rwHSGxL._SX342_.jpg\n",
            "Downloading B00F61CW2C from http://ecx.images-amazon.com/images/I/51P9R7b2DpL._SY445_.jpg\n",
            "Downloading B003BT5RQE from http://ecx.images-amazon.com/images/I/31RpIaiSslL._SX342_.jpg\n",
            "Downloading B006ZIN4ZY from http://ecx.images-amazon.com/images/I/31fxvU8ka8L._SX342_.jpg\n",
            "Downloading B00EQ849NG from http://ecx.images-amazon.com/images/I/51T55KowZlL._SY445_.jpg\n",
            "Downloading B00CWTXSQA from http://ecx.images-amazon.com/images/I/41yHfkTJ70L._SX342_.jpg\n",
            "Downloading B00FQW15ZG from http://ecx.images-amazon.com/images/I/51pfulBBfGL._SY445_.jpg\n",
            "Downloading B00A7PAUNA from http://ecx.images-amazon.com/images/I/31nZSMSFPQL._SY445_.jpg\n",
            "Downloading B006WQXY7M from http://ecx.images-amazon.com/images/I/51adIpIPMLL._SX342_.jpg\n",
            "Downloading B00BTN3VY4 from http://ecx.images-amazon.com/images/I/41FC1Z1u6yL._SY445_.jpg\n",
            "Downloading B00COJ5X5M from http://ecx.images-amazon.com/images/I/31l1kyb9y0L._SY200_.jpg\n",
            "Failed to download B00COJ5X5M: HTTP 404\n",
            "Downloading B00CJB273G from http://ecx.images-amazon.com/images/I/41mXmHGjpaL._SX342_.jpg\n",
            "Downloading B00COV8KL4 from http://ecx.images-amazon.com/images/I/41NwxMB0ffL._SY445_.jpg\n",
            "Downloading B00B44YPNO from http://ecx.images-amazon.com/images/I/41Dh8jHknUL._SY445_.jpg\n",
            "Downloading B004Y6R5EA from http://ecx.images-amazon.com/images/I/51qH9BloP5L._SY445_.jpg\n",
            "Downloading B00CF1X8C4 from http://ecx.images-amazon.com/images/I/41tjgJI4o1L._SY445_.jpg\n",
            "Downloading B00ATLEFAQ from http://ecx.images-amazon.com/images/I/411rCiB5mjL._SY445_.jpg\n",
            "Downloading B00794F28K from http://ecx.images-amazon.com/images/I/41RPGL6uYiL._SX342_.jpg\n",
            "Downloading B00C3UAXYI from http://ecx.images-amazon.com/images/I/41r4FXjvrxL._SY445_.jpg\n",
            "Downloading B009MJQSYC from http://ecx.images-amazon.com/images/I/31wrvYD4CDL._SY200_.jpg\n",
            "Downloading B0091QV846 from http://ecx.images-amazon.com/images/I/41fCtPSApmL._SY445_.jpg\n",
            "Downloading B0052RCZOK from http://ecx.images-amazon.com/images/I/31GerDgOggL._SX342_.jpg\n",
            "Downloading B00833CMSY from http://ecx.images-amazon.com/images/I/31bW-xLoBhL._SY445_.jpg\n",
            "Downloading B00A3UTGMK from http://ecx.images-amazon.com/images/I/51WDBr9NO2L._SX342_.jpg\n",
            "Downloading B004T6S92C from http://ecx.images-amazon.com/images/I/313R0a3ldbL._SY445_.jpg\n",
            "Downloading B0087YZUW4 from http://ecx.images-amazon.com/images/I/31DW2tA9tuL._SX342_.jpg\n",
            "Downloading B00791C4MK from http://ecx.images-amazon.com/images/I/419K7OdN4%2BL._SX342_.jpg\n",
            "Downloading B00A8LSKUS from http://ecx.images-amazon.com/images/I/318BMDovy5L._SX342_.jpg\n",
            "Downloading B00BJMIKPK from http://ecx.images-amazon.com/images/I/41vvh%2BXPq%2BL._SY445_.jpg\n",
            "Downloading B00FFXY3EQ from http://ecx.images-amazon.com/images/I/312QGfRbwpL._SX342_.jpg\n",
            "Downloading B002Z7ETPS from http://ecx.images-amazon.com/images/I/41brsnr9rZL._SX342_.jpg\n",
            "Downloading B00499DHQW from http://ecx.images-amazon.com/images/I/31YGH-Kh-xL._SX342_.jpg\n",
            "Downloading B006ZU1JYU from http://ecx.images-amazon.com/images/I/31bx4zwUvcL._SX342_.jpg\n",
            "Downloading B004U9YUG2 from http://ecx.images-amazon.com/images/I/41IqWjIrUDL._SX342_.jpg\n",
            "Downloading B00BBQCWY4 from http://ecx.images-amazon.com/images/I/31bR5WWDyCL._SY445_.jpg\n",
            "Downloading B008AJ092M from http://ecx.images-amazon.com/images/I/41MPMMr6NwL._SX342_.jpg\n",
            "Downloading B008BWODAM from http://ecx.images-amazon.com/images/I/41JAvt8nFVL._SY445_.jpg\n",
            "Downloading B009UWRVJC from http://ecx.images-amazon.com/images/I/51wsJP0p21L._SY445_.jpg\n",
            "Downloading B00E00ISEG from http://ecx.images-amazon.com/images/I/41HXliNZ0tL._SY445_.jpg\n",
            "Downloading B008FWZP5K from http://ecx.images-amazon.com/images/I/31ZCJKDJq1L._SY445_.jpg\n",
            "Downloading B0081FQTBK from http://ecx.images-amazon.com/images/I/319tyhLDEJL._SX342_.jpg\n",
            "Downloading B004TSC2DM from http://ecx.images-amazon.com/images/I/41tj3%2Bpa1RL._SX342_.jpg\n",
            "Downloading B009LIFH78 from http://ecx.images-amazon.com/images/I/31rv3a2jkxL._SX342_.jpg\n",
            "Downloading B008CO87H4 from http://ecx.images-amazon.com/images/I/41GdDaznatL._SX342_.jpg\n",
            "Downloading B00B7YC2EA from http://ecx.images-amazon.com/images/I/511KiLDIqrL._SX342_.jpg\n",
            "Downloading B0055OH23Y from http://ecx.images-amazon.com/images/I/413c-RAYh1L._SY445_.jpg\n",
            "Downloading B003AQB3X4 from http://ecx.images-amazon.com/images/I/41tgxD4B1UL._SX342_.jpg\n",
            "Downloading B00ANK7AVO from http://g-ecx.images-amazon.com/images/G/01/x-locale/brands/small-logo/3034386011._CB359598362_SX150_.jpg\n",
            "Downloading B00EIQLXTE from http://ecx.images-amazon.com/images/I/315Rj0GxjmL._SY445_.jpg\n",
            "Downloading B003UU1MMC from http://ecx.images-amazon.com/images/I/41GhHgs4nLL._SY445_.jpg\n",
            "Downloading B00CNXU246 from http://ecx.images-amazon.com/images/I/51eJumvM1hL._SX342_.jpg\n",
            "Downloading B0062ZT7QK from http://ecx.images-amazon.com/images/I/41Nw7F68JmL._SX342_.jpg\n",
            "Downloading B007FP6U84 from http://ecx.images-amazon.com/images/I/31Tq7zlwAoL._SX342_.jpg\n",
            "Downloading B00428BYUG from http://ecx.images-amazon.com/images/I/41L9YYQx5NL._SY445_.jpg\n",
            "Downloading B00AITNIAW from http://ecx.images-amazon.com/images/I/31sqhdfWCiL._SX342_.jpg\n",
            "Downloading B00B8GYG78 from http://ecx.images-amazon.com/images/I/41khW7VP6gL._SY445_.jpg\n",
            "Downloading B00B8B687Y from http://ecx.images-amazon.com/images/I/31yc4u9-YsL._SY445_.jpg\n",
            "Downloading B00DCTLPSM from http://g-ecx.images-amazon.com/images/G/01/x-locale/brands/small-logo/3034386011._CB359598362_SX150_.jpg\n",
            "Downloading B00CC6ZLZO from http://ecx.images-amazon.com/images/I/413jHyChbCL._SY445_.jpg\n",
            "Downloading B00BHG5NS0 from http://ecx.images-amazon.com/images/I/41-WrhcxVEL._SY445_.jpg\n",
            "Downloading B00EDMIIGE from http://ecx.images-amazon.com/images/I/41XNPGjmtnL._SX342_.jpg\n",
            "Downloading B00D5BNU80 from http://ecx.images-amazon.com/images/I/41HeQTvnECL._SX342_.jpg\n",
            "Downloading B00APFED80 from http://ecx.images-amazon.com/images/I/31YNmGRtCvL._SY445_.jpg\n",
            "Downloading B004EWG4JG from http://ecx.images-amazon.com/images/I/41Abj5%2B1r-L._SX342_.jpg\n",
            "Downloading B00E6H1ECG from http://ecx.images-amazon.com/images/I/41Kj37o0hmL._SX342_.jpg\n",
            "Downloading B0082ABDIS from http://ecx.images-amazon.com/images/I/31DxWX17FdL._SX342_.jpg\n",
            "Downloading B00DP4WHK4 from http://ecx.images-amazon.com/images/I/41NwfFxOsdL._SX342_.jpg\n",
            "Downloading B009B8RNB6 from http://ecx.images-amazon.com/images/I/31xxZhBhifL._SX342_.jpg\n",
            "Downloading B00E38FAGE from http://ecx.images-amazon.com/images/I/41myF29xyQL._SX342_.jpg\n",
            "Downloading B001N450MU from http://ecx.images-amazon.com/images/I/31UN-eMht5L._SY445_.jpg\n",
            "Downloading B0073ETM28 from http://ecx.images-amazon.com/images/I/41G3oueENnL._SX342_.jpg\n",
            "Downloading B00B7W7GN4 from http://ecx.images-amazon.com/images/I/41DZv1ZD-cL._SY445_.jpg\n",
            "Downloading B00DQZISFK from http://ecx.images-amazon.com/images/I/41LIKsFS0BL._SX342_.jpg\n",
            "Downloading B007ZYKK1I from http://ecx.images-amazon.com/images/I/41UKHzNOQeL._SY445_.jpg\n",
            "Downloading B00C1E84JW from http://ecx.images-amazon.com/images/I/31SsNXIOOCL._SY445_.jpg\n",
            "Downloading B0036DDYZQ from http://ecx.images-amazon.com/images/I/41O%2BsZrJgpL._SX342_.jpg\n",
            "Downloading B00ETITRJ4 from http://ecx.images-amazon.com/images/I/41iftupd0zL._SX342_.jpg\n",
            "Downloading B00A9SQBVU from http://ecx.images-amazon.com/images/I/31-q0GkUk1L._SX342_.jpg\n",
            "Downloading B00B4YZ1UK from http://ecx.images-amazon.com/images/I/41XDTRClnvL._SY445_.jpg\n",
            "Downloading B00880KBV2 from http://ecx.images-amazon.com/images/I/41qpzz3DDzL._SX342_.jpg\n",
            "Downloading B00E9OQU66 from http://ecx.images-amazon.com/images/I/31jrlE6cb5L._SX342_.jpg\n",
            "Failed to download B00E9OQU66: HTTP 404\n",
            "Downloading B00AYWXFLK from http://g-ecx.images-amazon.com/images/G/01/x-locale/brands/small-logo/3034061011._CB359598500_SX150_.jpg\n",
            "Downloading B00GOITRZG from http://ecx.images-amazon.com/images/I/413rG1brp4L._SY445_.jpg\n",
            "Downloading B00AYCLRCO from http://ecx.images-amazon.com/images/I/31PF-%2B6l6IL._SY200_.jpg\n",
            "Downloading B009LII1MQ from http://ecx.images-amazon.com/images/I/31OkA8ud1CL._SX342_.jpg\n",
            "Downloading B00F4MYSLG from http://ecx.images-amazon.com/images/I/41gI3WphzvL._SY445_.jpg\n",
            "Downloading B00297C9Y2 from http://ecx.images-amazon.com/images/I/31not%2BFYtvL.jpg\n",
            "Downloading B00C5TOGQS from http://ecx.images-amazon.com/images/I/41TpEqSw45L._SY445_.jpg\n",
            "Downloading B007RTTXIW from http://ecx.images-amazon.com/images/I/41w-JmAZYOL._SX342_.jpg\n",
            "Downloading B005XDGPC6 from http://ecx.images-amazon.com/images/I/41gGVFI2Q3L._SX342_.jpg\n",
            "Downloading B00CN4Y228 from http://ecx.images-amazon.com/images/I/314miyIFqyL._SX342_.jpg\n",
            "Failed to download B00CN4Y228: HTTP 404\n",
            "Downloading B007J2YA5I from http://ecx.images-amazon.com/images/I/41WlWhaex-L._SY445_.jpg\n",
            "Downloading B00AHOZRH0 from http://ecx.images-amazon.com/images/I/41Sil8FyzHL._SY445_.jpg\n",
            "Downloading B005ACHE0C from http://ecx.images-amazon.com/images/I/41O4kfo3ICL._SX342_.jpg\n",
            "Downloading B004XS7WLU from http://ecx.images-amazon.com/images/I/316NO4qcodL._SY445_.jpg\n",
            "Downloading B009KQV928 from http://ecx.images-amazon.com/images/I/31sgZrlWooL._SX342_.jpg\n",
            "Downloading B004NRO7N2 from http://ecx.images-amazon.com/images/I/41JVrZjQ-DL._SX342_.jpg\n",
            "Downloading B00BEJ36ZW from http://ecx.images-amazon.com/images/I/41c6Xb5dXIL._SY200_.jpg\n",
            "Failed to download B00BEJ36ZW: HTTP 404\n",
            "Downloading B009IO0L1W from http://ecx.images-amazon.com/images/I/31YtWTCbFwL._SY445_.jpg\n",
            "Downloading B005DC3A34 from http://ecx.images-amazon.com/images/I/51N1E9UtcxL._SY445_.jpg\n",
            "Downloading B004ZF6TS8 from http://ecx.images-amazon.com/images/I/411ZnBq15fL._SX342_.jpg\n",
            "Downloading B00EF5XTKY from http://ecx.images-amazon.com/images/I/41dzpLvB8XL._SY445_.jpg\n",
            "Downloading B00BT0N1BU from http://ecx.images-amazon.com/images/I/31eGLG6-lKL._SX342_.jpg\n",
            "Downloading B00BUKAT7I from http://ecx.images-amazon.com/images/I/313osirpbRL._SX342_.jpg\n",
            "Downloading B0091F3D2W from http://ecx.images-amazon.com/images/I/31YJkkV9zlL._SX342_.jpg\n",
            "Downloading B00DME430E from http://ecx.images-amazon.com/images/I/31Dtt%2BQWvmL._SY200_.jpg\n",
            "Failed to download B00DME430E: HTTP 404\n",
            "Downloading B006PDWT82 from http://ecx.images-amazon.com/images/I/31kcJ%2BrmwwL._SX342_.jpg\n",
            "Downloading B0028RYM7A from http://ecx.images-amazon.com/images/I/41OX9qjIqhL._SY445_.jpg\n",
            "Downloading B00BJFEH90 from http://ecx.images-amazon.com/images/I/312s6GQaAwL._SY200_.jpg\n",
            "Downloading B00FBSEYU8 from http://ecx.images-amazon.com/images/I/31zvtBD57yL._SY445_.jpg\n",
            "Downloading B004OYUJEU from http://ecx.images-amazon.com/images/I/41bJWe2MmVL._SX342_.jpg\n",
            "Downloading B0089I9NFS from http://ecx.images-amazon.com/images/I/41q5u4IimKL._SY445_.jpg\n",
            "Downloading B004TJLCSM from http://ecx.images-amazon.com/images/I/41N20ZlbjIL._SX342_.jpg\n",
            "Downloading B00GROXPFU from http://ecx.images-amazon.com/images/I/31rotsHA1XL._SY445_.jpg\n",
            "Downloading B00ANOMW2W from http://ecx.images-amazon.com/images/I/41HNA24QJhL._SY200_.jpg\n",
            "Downloading B00DM0RNMI from http://ecx.images-amazon.com/images/I/31ioyvN8W7L._SX342_.jpg\n",
            "Downloading B006ZJ76NE from http://ecx.images-amazon.com/images/I/31i3Kr9DVfL._SX342_.jpg\n",
            "Downloading B007ABAWJ6 from http://ecx.images-amazon.com/images/I/41Swsz%2Bv2RL._SX342_.jpg\n",
            "Downloading B00CY8XXSM from http://ecx.images-amazon.com/images/I/41IjRswWzCL._SY445_.jpg\n",
            "Downloading B00DGO6PYM from http://ecx.images-amazon.com/images/I/31BYE6HQ43L._SX342_.jpg\n",
            "Downloading B00ANVR0LI from http://ecx.images-amazon.com/images/I/41-SQy-G6RL._SY445_.jpg\n",
            "Downloading B00387EZGW from http://ecx.images-amazon.com/images/I/31%2B28nNIr7L._SX342_.jpg\n",
            "Downloading B007G10BPA from http://ecx.images-amazon.com/images/I/31nqeHRy0hL._SX342_.jpg\n",
            "Downloading B002G9UBB6 from http://ecx.images-amazon.com/images/I/41ttKICYd6L._SY445_.jpg\n",
            "Downloading B0076FD1CQ from http://ecx.images-amazon.com/images/I/51Gpf182txL._SX342_.jpg\n",
            "Downloading B009HG2R3G from http://ecx.images-amazon.com/images/I/41hbVp1RkqL._SX342_.jpg\n",
            "Downloading B006WSO41K from http://ecx.images-amazon.com/images/I/317-oOLA0CL._SX342_.jpg\n",
            "Downloading B00GCR6HZM from http://ecx.images-amazon.com/images/I/41QK7mT3taL._SX342_.jpg\n",
            "Downloading B004QVSUKG from http://ecx.images-amazon.com/images/I/4112ypzTqpL._SY445_.jpg\n",
            "Downloading B00BRBCKW2 from http://ecx.images-amazon.com/images/I/41%2BCBjxsXXL._SY200_.jpg\n",
            "Downloading B00CF204YS from http://ecx.images-amazon.com/images/I/41ppIqEJdgL._SY445_.jpg\n",
            "Downloading B00A560X38 from http://ecx.images-amazon.com/images/I/41iAe9Hs3sL._SY445_.jpg\n",
            "Downloading B008FEOZD6 from http://ecx.images-amazon.com/images/I/31KFbgTdOIL._SX342_.jpg\n",
            "Downloading B007G10FRE from http://ecx.images-amazon.com/images/I/31XcERV1urL._SX342_.jpg\n",
            "Downloading B007CVN2NM from http://ecx.images-amazon.com/images/I/31T2G3fag%2BL._SY445_.jpg\n",
            "Downloading B004ZSQUX4 from http://ecx.images-amazon.com/images/I/41qgoHP%2BbSL._SY445_.jpg\n",
            "Downloading B00BFAKKSQ from http://ecx.images-amazon.com/images/I/51wn1qKCUYL._SY445_.jpg\n",
            "Downloading B00943P1VW from http://ecx.images-amazon.com/images/I/41pafhf%2BkTL._SY445_.jpg\n",
            "Downloading B008KFIVIA from http://ecx.images-amazon.com/images/I/41L7FC5gr4L._SX342_.jpg\n",
            "Downloading B005LT0ZKA from http://ecx.images-amazon.com/images/I/31rpP3YBQjL._SX342_.jpg\n",
            "Downloading B00GYL56PS from http://ecx.images-amazon.com/images/I/41AgQwqBxHL._SY445_.jpg\n",
            "Downloading B00A13GNUM from http://ecx.images-amazon.com/images/I/41ag4%2BzAgtL._SX342_.jpg\n",
            "Downloading B0062ZTICI from http://ecx.images-amazon.com/images/I/41lo1zw1xSL._SX342_.jpg\n",
            "Downloading B006X53UZI from http://ecx.images-amazon.com/images/I/419coDkEhxL._SX342_.jpg\n",
            "Downloading B0081833NY from http://ecx.images-amazon.com/images/I/41JX%2B7ksmvL._SY445_.jpg\n",
            "Downloading B0088D23DS from http://ecx.images-amazon.com/images/I/4124z5NBFFL._SX342_.jpg\n",
            "Downloading B00A1HXBNK from http://ecx.images-amazon.com/images/I/31DFUEpVa5L._SY445_.jpg\n",
            "Downloading B0076R8H0U from http://ecx.images-amazon.com/images/I/51fnD0SAOVL._SX342_.jpg\n",
            "Downloading B005TD07GK from http://ecx.images-amazon.com/images/I/515d8S3ZZJL._SX342_.jpg\n",
            "Downloading B008AEE1AS from http://ecx.images-amazon.com/images/I/41Kt2U%2BV%2ByL._SX342_.jpg\n",
            "Downloading B00ATN3H1C from http://ecx.images-amazon.com/images/I/410wUz6kS8L._SY200_.jpg\n",
            "Downloading B005C8WCEM from http://ecx.images-amazon.com/images/I/41LpQqZY9jL._SY445_.jpg\n",
            "Downloading B00AIXFL58 from http://ecx.images-amazon.com/images/I/314Jxkk2IyL._SY445_.jpg\n",
            "Downloading B004MPQFC6 from http://ecx.images-amazon.com/images/I/316rmQPybhL._SX342_.jpg\n",
            "Downloading B00CR4L57Y from http://ecx.images-amazon.com/images/I/41o7AJvHryL._SY445_.jpg\n",
            "Downloading B00DH8YRWO from http://ecx.images-amazon.com/images/I/41FQ7W%2BwzpL._SY445_.jpg\n",
            "Downloading B00BEMEFU4 from http://ecx.images-amazon.com/images/I/413K3E-GA3L._SX342_.jpg\n",
            "Downloading B005FSOEJK from http://ecx.images-amazon.com/images/I/41sfcmKk3RL._SX342_.jpg\n",
            "Downloading B0050W76WS from http://ecx.images-amazon.com/images/I/41vNtyLtRkL._SY445_.jpg\n",
            "Downloading B008ZAYNUU from http://ecx.images-amazon.com/images/I/31%2B7nDN8-0L._SX342_.jpg\n",
            "Downloading B00D7EJJDU from http://ecx.images-amazon.com/images/I/41%2BEzQ7d6wL._SX342_.jpg\n",
            "Failed to download B00D7EJJDU: HTTP 404\n",
            "Downloading B00982TSFE from http://ecx.images-amazon.com/images/I/41iLNMRZ8bL._SX342_.jpg\n",
            "Downloading B007WACR0W from http://ecx.images-amazon.com/images/I/31GPWBoOwRL._SX342_.jpg\n",
            "Downloading B006ZHT4ZO from http://ecx.images-amazon.com/images/I/3154K%2BndX4L._SX342_.jpg\n",
            "Downloading B00BCMG914 from http://ecx.images-amazon.com/images/I/31jOOPtqbiL._SY200_.jpg\n",
            "Failed to download B00BCMG914: HTTP 404\n",
            "Downloading B00AUAPDHU from http://ecx.images-amazon.com/images/I/41gDH1BrohL._SX342_.jpg\n",
            "Downloading B004UWWJPI from http://ecx.images-amazon.com/images/I/41Vlk0kMerL._SY445_.jpg\n",
            "Downloading B00858C822 from http://ecx.images-amazon.com/images/I/41vBVZ2eUKL._SX342_.jpg\n",
            "Downloading B00CEHQW7M from http://ecx.images-amazon.com/images/I/419CjlY3kNL._SX342_.jpg\n",
            "Downloading B00EYPSU4A from http://ecx.images-amazon.com/images/I/41OAl36E9pL._SY445_.jpg\n",
            "Downloading B000PIGECA from http://ecx.images-amazon.com/images/I/31G8s2o1T6L._SY445_.jpg\n",
            "Downloading B00AHOYY56 from http://ecx.images-amazon.com/images/I/41Rwhc662IL._SY445_.jpg\n",
            "Downloading B00CIZVX2O from http://ecx.images-amazon.com/images/I/314wObL1prL._SY445_.jpg\n",
            "Downloading B006H2GU0E from http://ecx.images-amazon.com/images/I/31JZU1-dpLL._SY445_.jpg\n",
            "Downloading B008UAEELI from http://ecx.images-amazon.com/images/I/411zID3St7L._SX342_.jpg\n",
            "Downloading B00C17ZRLW from http://ecx.images-amazon.com/images/I/31QIz2erfaL._SX342_.jpg\n",
            "Downloading B009LIK4XA from http://ecx.images-amazon.com/images/I/31UyKYyn50L._SX342_.jpg\n",
            "Downloading B00CQA7GP4 from http://ecx.images-amazon.com/images/I/21TTRTzQzXL._SY445_.jpg\n",
            "Downloading B00EW41B30 from http://ecx.images-amazon.com/images/I/31XRphK%2B4DL._SY445_.jpg\n",
            "Downloading B005LJBWVG from http://ecx.images-amazon.com/images/I/51SsCtZPUCL._SY445_.jpg\n",
            "Downloading B00DH9Y2JQ from http://ecx.images-amazon.com/images/I/41PxfaVhkHL._SY200_.jpg\n",
            "Failed to download B00DH9Y2JQ: HTTP 404\n",
            "Downloading B0088AZ9AK from http://ecx.images-amazon.com/images/I/41gWOkI8KZL._SX342_.jpg\n",
            "Downloading B006PF0ZPO from http://ecx.images-amazon.com/images/I/51jtfZ7z4iL._SX342_.jpg\n",
            "Downloading B004J8I3H6 from http://ecx.images-amazon.com/images/I/41uiBhhPIoL._SX342_.jpg\n",
            "Downloading B005CN9A2Y from http://ecx.images-amazon.com/images/I/41BasFjV0XL._SY445_.jpg\n",
            "Downloading B009EMNO2G from http://ecx.images-amazon.com/images/I/31wB8x4bXcL._SX342_.jpg\n",
            "Downloading B008596N18 from http://ecx.images-amazon.com/images/I/41zO%2B457aNL._SX342_.jpg\n",
            "Downloading B008RMYIVK from http://ecx.images-amazon.com/images/I/41eV4foXzFL._SY445_.jpg\n",
            "Downloading B005R4QUF8 from http://ecx.images-amazon.com/images/I/41YrhtP2ZWL._SX342_.jpg\n",
            "Downloading B006W55MFU from http://ecx.images-amazon.com/images/I/41ujgfFFQDL._SY445_.jpg\n",
            "Downloading B00AZYXWA6 from http://ecx.images-amazon.com/images/I/315C6hqRgxL._SY200_.jpg\n",
            "Downloading B008VIU2V0 from http://ecx.images-amazon.com/images/I/31tCv4fAoeL._SX342_.jpg\n",
            "Downloading B00BC8PHRK from http://ecx.images-amazon.com/images/I/31ycFqFdzEL._SX342_.jpg\n",
            "Failed to download B00BC8PHRK: HTTP 404\n",
            "Downloading B000X1N2KQ from http://ecx.images-amazon.com/images/I/31TaOEvvgHL._SY445_.jpg\n",
            "Downloading B00CC62GH0 from http://ecx.images-amazon.com/images/I/41Qo9tsu-VL._SY445_.jpg\n",
            "Downloading B00CWAEZT8 from http://ecx.images-amazon.com/images/I/41XzvBIFgkL._SX342_.jpg\n",
            "Downloading B00DCCA21A from http://ecx.images-amazon.com/images/I/31vC5Hy21ZL._SY445_.jpg\n",
            "Downloading B005SN1FLM from http://ecx.images-amazon.com/images/I/41yvX72ic9L._SX342_.jpg\n",
            "Downloading B00ARHNXWI from http://ecx.images-amazon.com/images/I/41ZaeDX-N0L._SY445_.jpg\n",
            "Downloading B00DQ3DS2A from http://ecx.images-amazon.com/images/I/41YQTPLJlEL._SY445_.jpg\n",
            "Downloading B007PIX4XK from http://ecx.images-amazon.com/images/I/41CKQdrWg2L._SY445_.jpg\n",
            "Downloading B002PEZMSY from http://ecx.images-amazon.com/images/I/51m5RoUYyOL._SY445_.jpg\n",
            "Downloading B00CQSTQCW from http://ecx.images-amazon.com/images/I/41OPVWtEp0L._SY445_.jpg\n",
            "Downloading B003HE66SG from http://ecx.images-amazon.com/images/I/31WOUU6fo5L._SY445_.jpg\n",
            "Downloading B007WR2QDS from http://ecx.images-amazon.com/images/I/31cqojxP6lL._SX342_.jpg\n",
            "Downloading B00EAIWOPW from http://ecx.images-amazon.com/images/I/31ZBLLL5HsL._SX342_.jpg\n",
            "Failed to download B00EAIWOPW: HTTP 404\n",
            "Downloading B004UJIXV0 from http://ecx.images-amazon.com/images/I/41bYmloT21L._SY445_.jpg\n",
            "Downloading B008TS2HP6 from http://ecx.images-amazon.com/images/I/3199GVPb1sL._SX342_.jpg\n",
            "Downloading B00C59LZZS from http://ecx.images-amazon.com/images/I/51dUFCSgOKL._SX342_.jpg\n",
            "Downloading B00CWSL250 from http://ecx.images-amazon.com/images/I/41zFry5zy3L._SX342_.jpg\n",
            "Downloading B00AZFZ3QG from http://ecx.images-amazon.com/images/I/41bpttfXnNL._SX342_.jpg\n",
            "Downloading B0035JL5X4 from http://ecx.images-amazon.com/images/I/41njwcP98kL._SX342_.jpg\n",
            "Downloading B00AVVAW1K from http://ecx.images-amazon.com/images/I/31B1Uccq5TL._SY445_.jpg\n",
            "Downloading B005WKBXHW from http://ecx.images-amazon.com/images/I/41g4yXVlVHL._SY445_.jpg\n",
            "Downloading B005940YOO from http://ecx.images-amazon.com/images/I/41nROCh%2BtVL._SX342_.jpg\n",
            "Downloading B0073ESJ8Q from http://ecx.images-amazon.com/images/I/41yyiscyfUL._SX342_.jpg\n",
            "Downloading B007FG0QAG from http://ecx.images-amazon.com/images/I/41aSAMJJoQL._SY445_.jpg\n",
            "Downloading B004FEE206 from http://ecx.images-amazon.com/images/I/31JCsrAU06L._SX342_.jpg\n",
            "Downloading B009EMNJ5I from http://ecx.images-amazon.com/images/I/41bMeCf9SCL._SY200_.jpg\n",
            "Downloading B00DPE1DD6 from http://ecx.images-amazon.com/images/I/31p8hMNSIgL._SX342_.jpg\n",
            "Failed to download B00DPE1DD6: HTTP 404\n",
            "Downloading B008AHK6MM from http://ecx.images-amazon.com/images/I/310QqH6aQBL._SX342_.jpg\n",
            "Downloading B00AHOXFEC from http://ecx.images-amazon.com/images/I/31shGk6xUWL._SY445_.jpg\n",
            "Downloading B004SLT8MI from http://ecx.images-amazon.com/images/I/41xjl50xa%2BL._SX342_.jpg\n",
            "Downloading B0097IYBBA from http://ecx.images-amazon.com/images/I/41FJPIx6CHL._SX342_.jpg\n",
            "Downloading B00A7CB4WE from http://ecx.images-amazon.com/images/I/41znEYK0AQL._SX342_.jpg\n",
            "Downloading B00DWPDDE0 from http://ecx.images-amazon.com/images/I/41C2psbJ-WL._SY445_.jpg\n",
            "Downloading B00CMEN80Q from http://ecx.images-amazon.com/images/I/51j7GbeImlL._SX342_.jpg\n",
            "Downloading B0051N2N2E from http://ecx.images-amazon.com/images/I/41KjlI-eKnL._SY445_.jpg\n",
            "Downloading B00E0I8VHC from http://ecx.images-amazon.com/images/I/41FahraP2EL._SY445_.jpg\n",
            "Downloading B00CJ5O9GK from http://ecx.images-amazon.com/images/I/41ws4SCUnvL._SY200_.jpg\n",
            "Failed to download B00CJ5O9GK: HTTP 404\n",
            "Downloading B00FKV0F8Q from http://ecx.images-amazon.com/images/I/51TsjZlYYYL._SY445_.jpg\n",
            "Downloading B009VH61RO from http://ecx.images-amazon.com/images/I/51icHhYtzqL._SX342_.jpg\n",
            "Downloading B00AHMJ3ZY from http://ecx.images-amazon.com/images/I/41G0UBOJxOL._SX342_.jpg\n",
            "Downloading B003VX1HNW from http://ecx.images-amazon.com/images/I/41lJK1wyFaL._SY445_.jpg\n",
            "Downloading B00A4MTLRC from http://ecx.images-amazon.com/images/I/41SZLoPLEnL._SX342_.jpg\n",
            "Downloading B004LRBIJA from http://ecx.images-amazon.com/images/I/41mq7iba7jL._SX342_.jpg\n",
            "Downloading B0098172YE from http://ecx.images-amazon.com/images/I/41-yDLZCVNL._SY445_.jpg\n",
            "Downloading B00A2YVMRY from http://ecx.images-amazon.com/images/I/51UwpW2ubqL._SX342_.jpg\n",
            "Downloading B00D0AEFL2 from http://ecx.images-amazon.com/images/I/41Q1LqglDAL._SX342_.jpg\n",
            "Downloading B009JG2AR2 from http://ecx.images-amazon.com/images/I/41t8baqkXxL._SY445_.jpg\n",
            "Downloading B008P92PAG from http://ecx.images-amazon.com/images/I/41GUwe%2BE-bL._SY445_.jpg\n",
            "Downloading B007WC0S2O from http://ecx.images-amazon.com/images/I/41cvHhFM6FL._SY445_.jpg\n",
            "Downloading B009PYPP3E from http://ecx.images-amazon.com/images/I/31lcZ9i32cL._SX342_.jpg\n",
            "Downloading B0062FMSIY from http://ecx.images-amazon.com/images/I/31fLTbHcQhL._SY445_.jpg\n",
            "Downloading B006ATCK6C from http://ecx.images-amazon.com/images/I/41svJJ2yFxL._SX342_.jpg\n",
            "Downloading B008RJZTU2 from http://ecx.images-amazon.com/images/I/31k884p1giL._SX342_.jpg\n",
            "Downloading B008CQJ9D8 from http://ecx.images-amazon.com/images/I/41H2ov1VEOL._SX342_.jpg\n",
            "Downloading B004RM9OZE from http://ecx.images-amazon.com/images/I/41nSYwQPy8L._SX342_.jpg\n",
            "Downloading B00144G78A from http://ecx.images-amazon.com/images/I/31lE9fFzJ5L._SY445_.jpg\n",
            "Downloading B009KM5EQO from http://ecx.images-amazon.com/images/I/314DJQmAMVL._SY445_.jpg\n",
            "Downloading B003W3DRWK from http://ecx.images-amazon.com/images/I/31s2%2B-vV8tL._SX342_.jpg\n",
            "Downloading B007CVK41K from http://ecx.images-amazon.com/images/I/41lrCuI28SL._SY445_.jpg\n",
            "Downloading B00CHSEO7I from http://ecx.images-amazon.com/images/I/417JoimfvuL._SY445_.jpg\n",
            "Downloading B00AOZN7GA from http://ecx.images-amazon.com/images/I/41j14feJ3uL._SY445_.jpg\n",
            "Downloading B00B9D4FSA from http://ecx.images-amazon.com/images/I/41lap4D3aCL._SY445_.jpg\n",
            "Downloading B00BGZRO3O from http://ecx.images-amazon.com/images/I/41kxIPjJOcL._SY445_.jpg\n",
            "Downloading B00CFU74SY from http://ecx.images-amazon.com/images/I/41-1Hw9tyCL._SY445_.jpg\n",
            "Downloading B00AKB4EJC from http://ecx.images-amazon.com/images/I/31FAj9z9NgL._SX342_.jpg\n",
            "Downloading B0080BCXPQ from http://ecx.images-amazon.com/images/I/31yjfjup7wL._SX342_.jpg\n",
            "Downloading B005DN8RVI from http://ecx.images-amazon.com/images/I/41AzRr4uQXL._SX342_.jpg\n",
            "Downloading B003FMU2NA from http://ecx.images-amazon.com/images/I/31YcGXf3coL._SX342_.jpg\n",
            "Downloading B008771YU8 from http://ecx.images-amazon.com/images/I/51Wzqx9-5OL._SX342_.jpg\n",
            "Downloading B008ASQO4U from http://ecx.images-amazon.com/images/I/41CYJxW7M%2BL._SX342_.jpg\n",
            "Downloading B009LV1NAK from http://ecx.images-amazon.com/images/I/31qUiN-jDpL._SX342_.jpg\n",
            "Downloading B007WACY5K from http://ecx.images-amazon.com/images/I/31J18FmCTCL._SY445_.jpg\n",
            "Downloading B00E0GPEQU from http://ecx.images-amazon.com/images/I/41xGW370gSL._SY445_.jpg\n",
            "Downloading B00B76OUUC from http://ecx.images-amazon.com/images/I/413HF%2BF%2BFxL._SY445_.jpg\n",
            "Downloading B004U9YVZ2 from http://ecx.images-amazon.com/images/I/41I3jrOlMUL._SX342_.jpg\n",
            "Downloading B007CMDOXE from http://ecx.images-amazon.com/images/I/51N32TUq0lL._SX342_.jpg\n",
            "Downloading B005LC23ES from http://ecx.images-amazon.com/images/I/41MBq9nDbRL._SX342_.jpg\n",
            "Downloading B00B4GJ4BK from http://ecx.images-amazon.com/images/I/41M0PORihtL._SY445_.jpg\n",
            "Downloading B008CPODQM from http://ecx.images-amazon.com/images/I/41sCyVaFt8L._SY445_.jpg\n",
            "Downloading B00A16F97C from http://ecx.images-amazon.com/images/I/41BeroERSrL._SY200_.jpg\n",
            "Downloading B00DUH1ZA4 from http://ecx.images-amazon.com/images/I/41LGISzyr6L._SX342_.jpg\n",
            "Downloading B004LVNO8Y from http://ecx.images-amazon.com/images/I/31ugL0RTvQL._SX342_.jpg\n",
            "Downloading B008D9AOMY from http://ecx.images-amazon.com/images/I/517HHGclCtL._SY445_.jpg\n",
            "Downloading B0089K3E9C from http://ecx.images-amazon.com/images/I/41mBoJ8IEtL._SX342_.jpg\n",
            "Downloading B009VJ1TBA from http://ecx.images-amazon.com/images/I/41HUPMXxhxL._SX342_.jpg\n",
            "Downloading B008BWOZFU from http://ecx.images-amazon.com/images/I/41D331KGVIL._SY445_.jpg\n",
            "Downloading B00DJ7BI2A from http://ecx.images-amazon.com/images/I/41Otc3GowZL._SY445_.jpg\n",
            "Downloading B009VYW3Y2 from http://ecx.images-amazon.com/images/I/41-wwXdG1HL._SX342_.jpg\n",
            "Downloading B00B8QNPH0 from http://ecx.images-amazon.com/images/I/51l9znnzGHL._SY445_.jpg\n",
            "Downloading B006WXNZHO from http://ecx.images-amazon.com/images/I/41SQwILmr5L._SY445_.jpg\n",
            "Downloading B00CBP4H06 from http://ecx.images-amazon.com/images/I/41eFFR-5N%2BL._SY445_.jpg\n",
            "Downloading B00EVPPG5Y from http://ecx.images-amazon.com/images/I/31RConngvbL._SY445_.jpg\n",
            "Downloading B0028Y5EKW from http://ecx.images-amazon.com/images/I/41kfDu6OLWL._SY445_.jpg\n",
            "Downloading B004OVDN6E from http://ecx.images-amazon.com/images/I/41CJKFf6fQL._SX342_.jpg\n",
            "Downloading B0072IKU30 from http://ecx.images-amazon.com/images/I/41R7xgIEI7L._SY445_.jpg\n",
            "Downloading B00A1CI0HC from http://ecx.images-amazon.com/images/I/31YD24D0tUL._SX342_.jpg\n",
            "Downloading B00A76ZL3S from http://ecx.images-amazon.com/images/I/41pBuDWQrRL._SY445_.jpg\n",
            "Downloading B00BG0TKHW from http://ecx.images-amazon.com/images/I/31G7QPrn1oL._SY200_.jpg\n",
            "Failed to download B00BG0TKHW: HTTP 404\n",
            "Downloading B00B80TRKA from http://ecx.images-amazon.com/images/I/313wHcn0wUL._SY445_.jpg\n",
            "Downloading B00CBTTOX2 from http://ecx.images-amazon.com/images/I/41cixnwxUmL._SX342_.jpg\n",
            "Downloading B009NCHCBQ from http://ecx.images-amazon.com/images/I/311zAxD1ltL._SX342_.jpg\n",
            "Downloading B006J42TDC from http://ecx.images-amazon.com/images/I/41evytkjOgL._SX342_.jpg\n",
            "Downloading B00BIY352M from http://ecx.images-amazon.com/images/I/31UnnenVewL._SY200_.jpg\n",
            "Failed to download B00BIY352M: HTTP 404\n",
            "Downloading B0087BPK3G from http://ecx.images-amazon.com/images/I/41yCyjRudEL._SY445_.jpg\n",
            "Downloading B00BUJEBB4 from http://ecx.images-amazon.com/images/I/41VnsAMGXAL._SX342_.jpg\n",
            "Downloading B009NDC7V0 from http://ecx.images-amazon.com/images/I/415ErhsKuJL._SY200_.jpg\n",
            "Downloading B008DF2EOO from http://ecx.images-amazon.com/images/I/31felohY7BL._SY445_.jpg\n",
            "Downloading B00AWNZ4YW from http://ecx.images-amazon.com/images/I/41JRxUXuhUL._SX342_.jpg\n",
            "Downloading B00AM1U4R6 from http://ecx.images-amazon.com/images/I/41%2BqqRoz01L._SX342_.jpg\n",
            "Downloading B00BQWYOSK from http://ecx.images-amazon.com/images/I/412b5Q6ky5L._SY200_.jpg\n",
            "Failed to download B00BQWYOSK: HTTP 404\n",
            "Downloading B00CMBC04E from http://ecx.images-amazon.com/images/I/41X8dd6qIKL._SY445_.jpg\n",
            "Downloading B002SQJQ2C from http://ecx.images-amazon.com/images/I/412NViQz-jL._SY445_.jpg\n",
            "Downloading B0082BCRHI from http://ecx.images-amazon.com/images/I/311quEV5y6L._SY445_.jpg\n",
            "Downloading B004PFKIOE from http://ecx.images-amazon.com/images/I/41IfhaPWVKL._SY445_.jpg\n",
            "Downloading B0088B32OO from http://ecx.images-amazon.com/images/I/31Jznc2m%2BsL._SX342_.jpg\n",
            "Downloading B008G3LZIE from http://ecx.images-amazon.com/images/I/41iajbb33gL._SX342_.jpg\n",
            "Downloading B005EVNJ1C from http://ecx.images-amazon.com/images/I/515FVbuzupL._SX342_.jpg\n",
            "Downloading B0073ESK5I from http://ecx.images-amazon.com/images/I/41Om8%2B1VU8L._SX342_.jpg\n",
            "Downloading B007ZP0DFA from http://ecx.images-amazon.com/images/I/31dc0l9jMPL._SX342_.jpg\n",
            "Downloading B008DGXO7E from http://ecx.images-amazon.com/images/I/31ynPhm3FGL._SX342_.jpg\n",
            "Downloading B00C931A8C from http://ecx.images-amazon.com/images/I/41fDhFlL3DL._SY445_.jpg\n",
            "Downloading B0093PD0T6 from http://ecx.images-amazon.com/images/I/41wffO49ZaL._SX342_.jpg\n",
            "Downloading B008G3KN7S from http://ecx.images-amazon.com/images/I/41nRLJXTBRL._SX342_.jpg\n",
            "Downloading B00AIXHL4C from http://ecx.images-amazon.com/images/I/31rMfoB8A0L._SY445_.jpg\n",
            "Downloading B003BT5ZGQ from http://ecx.images-amazon.com/images/I/31I7ygBE5fL._SX342_.jpg\n",
            "Downloading B007C7J5KK from http://ecx.images-amazon.com/images/I/31REY-iaU5L._SX342_.jpg\n",
            "Downloading B00DS1BUFW from http://ecx.images-amazon.com/images/I/41bfN0rkKyL._SY200_.jpg\n",
            "Downloading B00EUGDXR2 from http://ecx.images-amazon.com/images/I/41xUeqA0qRL._SY445_.jpg\n",
            "Downloading B007TUQ0LC from http://ecx.images-amazon.com/images/I/41nuIrWBkfL._SX342_.jpg\n",
            "Downloading B002KCHHJS from http://ecx.images-amazon.com/images/I/41k7z6QZdGL._SY445_.jpg\n",
            "Downloading B008IGSOY2 from http://ecx.images-amazon.com/images/I/41J2Rdv0FCL._SY445_.jpg\n",
            "Downloading B0080QPYHU from http://ecx.images-amazon.com/images/I/41zDHUeAi4L._SY445_.jpg\n",
            "Downloading B005M2A9BG from http://ecx.images-amazon.com/images/I/41RYD5uExyL._SX342_.jpg\n",
            "Downloading B00AG42396 from http://ecx.images-amazon.com/images/I/316KQJyMGUL._SY445_.jpg\n",
            "Downloading B00AIBYOW6 from http://ecx.images-amazon.com/images/I/41mxzp5l7oL._SX342_.jpg\n",
            "Downloading B00A3UTOTU from http://ecx.images-amazon.com/images/I/51WDBr9NO2L._SX342_.jpg\n",
            "Downloading B0091QUGPS from http://ecx.images-amazon.com/images/I/31-hCcg4JIL._SY445_.jpg\n",
            "Downloading B00CII3TWS from http://ecx.images-amazon.com/images/I/41H%2BnlG6EDL._SX342_.jpg\n",
            "Downloading B0087YZEES from http://ecx.images-amazon.com/images/I/31faehDYndL._SX342_.jpg\n",
            "Downloading B00BQLD3UQ from http://ecx.images-amazon.com/images/I/41z2aOLbQYL._SY445_.jpg\n",
            "Downloading B00730QYY6 from http://ecx.images-amazon.com/images/I/31kNT0WIP1L._SY445_.jpg\n",
            "Downloading B00DJJIXDK from http://ecx.images-amazon.com/images/I/41TXhQYlI5L._SY200_.jpg\n",
            "Failed to download B00DJJIXDK: HTTP 404\n",
            "Downloading B008E663JY from http://ecx.images-amazon.com/images/I/31sr2FQChRL._SX342_.jpg\n",
            "Downloading B005IQ1ADM from http://ecx.images-amazon.com/images/I/41EztFw2UCL._SY445_.jpg\n",
            "Downloading B000WNPYQU from http://ecx.images-amazon.com/images/I/31prS0m0vaL._SY445_.jpg\n",
            "Downloading B0074PSMUY from http://ecx.images-amazon.com/images/I/514P-p--JwL._SX342_.jpg\n",
            "Downloading B0067LQGYK from http://ecx.images-amazon.com/images/I/41ag5KH12pL._SX342_.jpg\n",
            "Downloading B00D31C2JK from http://ecx.images-amazon.com/images/I/41PcXXxKr4L._SY445_.jpg\n",
            "Downloading B005F1OFXM from http://ecx.images-amazon.com/images/I/418Cqg2zI1L._SX342_.jpg\n",
            "Downloading B0090KHN7E from http://ecx.images-amazon.com/images/I/41y6igcEvTL._SX342_.jpg\n",
            "Downloading B007XD6TR0 from http://ecx.images-amazon.com/images/I/512RVu8DwnL._SX342_.jpg\n",
            "Downloading B005Z4JT26 from http://ecx.images-amazon.com/images/I/41vMvLW%2BaUL._SX342_.jpg\n",
            "Downloading B008CVS7EK from http://ecx.images-amazon.com/images/I/31Pci4oDptL._SY200_.jpg\n",
            "Failed to download B008CVS7EK: HTTP 404\n",
            "Downloading B008P5OFHQ from http://ecx.images-amazon.com/images/I/41vVCTEiQtL._SY445_.jpg\n",
            "Downloading B000NPQD4O from http://ecx.images-amazon.com/images/I/315caD6eGlL._SY445_.jpg\n",
            "Downloading B008KSA964 from http://ecx.images-amazon.com/images/I/41%2BKW1a6EkL._SX342_.jpg\n",
            "Downloading B004L2L9KS from http://ecx.images-amazon.com/images/I/31YBrPl3IZL._SX342_.jpg\n",
            "Downloading B003BH2QVK from http://ecx.images-amazon.com/images/I/313NFIBczBL._SX342_.jpg\n",
            "Downloading B00385WM5A from http://ecx.images-amazon.com/images/I/41y7veOPXmL._SX342_.jpg\n",
            "Downloading B00ARAZOHM from http://ecx.images-amazon.com/images/I/41ApfUf4Z6L._SX342_.jpg\n",
            "Downloading B00D8X9MQO from http://ecx.images-amazon.com/images/I/41A41adr9WL._SY200_.jpg\n",
            "Failed to download B00D8X9MQO: HTTP 404\n",
            "Downloading B00A3UT82I from http://ecx.images-amazon.com/images/I/314y0YtRNRL._SY445_.jpg\n",
            "Downloading B002X7A3D2 from http://ecx.images-amazon.com/images/I/31ehIpZZwLL._SX342_.jpg\n",
            "Downloading B004CFUPEU from http://ecx.images-amazon.com/images/I/31Te3xdrfOL._SY445_.jpg\n",
            "Downloading B00B9B8CJU from http://ecx.images-amazon.com/images/I/41bAEZlyqHL._SY445_.jpg\n",
            "Downloading B00DVUUJ70 from http://ecx.images-amazon.com/images/I/41GwMqgf9PL._SY445_.jpg\n",
            "Downloading B00FJJ2T9W from http://ecx.images-amazon.com/images/I/419HEkQF9ML._SY445_.jpg\n",
            "Downloading B005XEN2PS from http://ecx.images-amazon.com/images/I/41yc4Qr8DnL._SY445_.jpg\n",
            "Downloading B003E27SMO from http://ecx.images-amazon.com/images/I/41mLsN7F1wL._SY445_.jpg\n",
            "Downloading B008H6Q4EA from http://ecx.images-amazon.com/images/I/51Ud3fbufTL._SX342_.jpg\n",
            "Downloading B00858CCFA from http://ecx.images-amazon.com/images/I/31QOF6cDcHL._SX342_.jpg\n",
            "Downloading B008CFECDG from http://ecx.images-amazon.com/images/I/41Rkd6y6b0L._SX342_.jpg\n",
            "Downloading B008X8A5WE from http://ecx.images-amazon.com/images/I/41btAfizMDL._SX342_.jpg\n",
            "Downloading B00CGDKAZE from http://ecx.images-amazon.com/images/I/316QOG4ktNL._SY300_.jpg\n",
            "Downloading B00C4110EI from http://ecx.images-amazon.com/images/I/31nBJbh25UL._SX342_.jpg\n",
            "Failed to download B00C4110EI: HTTP 404\n",
            "Downloading B00F4IN2DU from http://ecx.images-amazon.com/images/I/41ne02PK9zL._SY445_.jpg\n",
            "Downloading B008FQ7VSA from http://ecx.images-amazon.com/images/I/31BpfGUvPOL._SY445_.jpg\n",
            "Downloading B005RYOEOM from http://ecx.images-amazon.com/images/I/41yHwfo2h-L._SX342_.jpg\n",
            "Downloading B007TUE70S from http://ecx.images-amazon.com/images/I/41OxUUBEvSL._SX342_.jpg\n",
            "Downloading B00BPDCX60 from http://ecx.images-amazon.com/images/I/41s%2BO12rVLL._SY445_.jpg\n",
            "Downloading B003ULNDK0 from http://ecx.images-amazon.com/images/I/31KEsjKP8DL._SX342_.jpg\n",
            "Downloading B008L0GSAW from http://ecx.images-amazon.com/images/I/41ba4Ia4GUL._SX342_.jpg\n",
            "Downloading B00EA8YB1W from http://ecx.images-amazon.com/images/I/31HPsYiRFBL._SY445_.jpg\n",
            "Downloading B00C12LB8U from http://ecx.images-amazon.com/images/I/316WuLOHwyL._SX342_.jpg\n",
            "Downloading B00COV7SP8 from http://ecx.images-amazon.com/images/I/41LvH2bWr7L._SY445_.jpg\n",
            "Downloading B000ITOOFU from http://ecx.images-amazon.com/images/I/41W2XT3X02L._SY445_.jpg\n",
            "Downloading B0073PZRD0 from http://ecx.images-amazon.com/images/I/41xvLbBVU9L._SX342_.jpg\n",
            "Downloading B001E5EAVU from http://ecx.images-amazon.com/images/I/41YSARX26dL._SY445_.jpg\n",
            "Downloading B002Y2PGDS from http://ecx.images-amazon.com/images/I/41QxVbxK3lL._SY445_.jpg\n",
            "Downloading B007WPHRVG from http://ecx.images-amazon.com/images/I/41PZ%2ByEpZDL._SY445_.jpg\n",
            "Downloading B007WFPSJE from http://ecx.images-amazon.com/images/I/51Gi0uAVexL._SY445_.jpg\n",
            "Downloading B007TYZUBY from http://ecx.images-amazon.com/images/I/41mSb3fa-dL._SX342_.jpg\n",
            "Downloading B003Z99POK from http://ecx.images-amazon.com/images/I/41YOwScFXDL._SY445_.jpg\n",
            "Downloading B0092PKY9G from http://ecx.images-amazon.com/images/I/31QWY2j0m2L._SX342_.jpg\n",
            "Downloading B00BBUR51A from http://ecx.images-amazon.com/images/I/41zsGqpuLuL._SX160_.jpg\n",
            "Downloading B00DFYTK3Q from http://ecx.images-amazon.com/images/I/41ECZCAd9KL._SX342_.jpg\n",
            "Downloading B00BC05Z0W from http://ecx.images-amazon.com/images/I/41EaDK12y%2BL._SX342_.jpg\n",
            "Downloading B00DJZPTO0 from http://ecx.images-amazon.com/images/I/31YeTCmtRzL._SY445_.jpg\n",
            "Downloading B00C4Q2RO0 from http://ecx.images-amazon.com/images/I/31j8dtCBXYL._SY200_.jpg\n",
            "Downloading B00AJJAMCI from http://ecx.images-amazon.com/images/I/41V9SxOgbJL._SY445_.jpg\n",
            "Downloading B00999CL2I from http://ecx.images-amazon.com/images/I/41z-ZMJ-1RL._SY445_.jpg\n",
            "Downloading B00CIAHMJW from http://ecx.images-amazon.com/images/I/41TDxs8JHuL._SX342_.jpg\n",
            "Downloading B0076ZHU2I from http://ecx.images-amazon.com/images/I/51uQIFqXcJL._SX342_.jpg\n",
            "Downloading B005BWQMHW from http://ecx.images-amazon.com/images/I/412TF6HPPhL._SY300_.jpg\n",
            "Downloading B009AO2FS2 from http://ecx.images-amazon.com/images/I/31QlTo4MUeL._SX342_.jpg\n",
            "Downloading B0092UBN0K from http://ecx.images-amazon.com/images/I/51m8sFIJWTL._SY445_.jpg\n",
            "Downloading B00GI1SEQM from http://ecx.images-amazon.com/images/I/41%2B3HttWafL._SY445_.jpg\n",
            "Downloading B00DG6SMMS from http://ecx.images-amazon.com/images/I/41WhgzBhNmL._SY445_.jpg\n",
            "Downloading B00E4GQPMI from http://ecx.images-amazon.com/images/I/41Ah7-6n2%2BL._SX342_.jpg\n",
            "Downloading B0077965KG from http://ecx.images-amazon.com/images/I/51WE4avhpML._SY445_.jpg\n",
            "Downloading B0073XZJOO from http://ecx.images-amazon.com/images/I/41inQByWXHL._SX342_.jpg\n",
            "Downloading B00DEPURG0 from http://ecx.images-amazon.com/images/I/41Rh-4Zw4FL._SX342_.jpg\n",
            "Downloading B008PRHLJS from http://ecx.images-amazon.com/images/I/31mwydfOMEL._SX342_.jpg\n",
            "Downloading B0070DXXJU from http://ecx.images-amazon.com/images/I/41CGHPSKrKL._SX342_.jpg\n",
            "Downloading B006WWUMDK from http://ecx.images-amazon.com/images/I/311HbaMT56L._SY445_.jpg\n",
            "Downloading B00BM9KPSK from http://ecx.images-amazon.com/images/I/41sgROjpnIL._SY445_.jpg\n",
            "Downloading B001STTPEE from http://ecx.images-amazon.com/images/I/41lf9w2M-ML._SY445_.jpg\n",
            "Downloading B00EIQK9WQ from http://ecx.images-amazon.com/images/I/31%2BKuCcZxmL._SY445_.jpg\n",
            "Downloading B002SQKN4M from http://ecx.images-amazon.com/images/I/41YHaoo6aPL._SY445_.jpg\n",
            "Downloading B00DHLUI2O from http://ecx.images-amazon.com/images/I/41Au5VL5GIL._SY445_.jpg\n",
            "Downloading B008P7ZH1M from http://ecx.images-amazon.com/images/I/41N2d5xcV3L._SY445_.jpg\n",
            "Downloading B0052IGSS8 from http://ecx.images-amazon.com/images/I/41JRPJ9wubL._SY445_.jpg\n",
            "Downloading B002S1653K from http://ecx.images-amazon.com/images/I/51M1Z9TkBVL._SX342_.jpg\n",
            "Downloading B0089K7U7Y from http://ecx.images-amazon.com/images/I/41JgpXi%2BFML._SY445_.jpg\n",
            "Downloading B004CMF336 from http://ecx.images-amazon.com/images/I/41zV7tr6XyL._SY445_.jpg\n",
            "Downloading B0037UZWUS from http://ecx.images-amazon.com/images/I/41cQ40KIVNL._SX342_.jpg\n",
            "Downloading B009AFBH1M from http://ecx.images-amazon.com/images/I/41x87mToRrL._SX342_.jpg\n",
            "Downloading B004AH2URU from http://ecx.images-amazon.com/images/I/41OMAoMLxFL._SY445_.jpg\n",
            "Downloading B00665Q19M from http://ecx.images-amazon.com/images/I/31YMiUyjbAL._SX342_.jpg\n",
            "Downloading B00CKZJHHA from http://ecx.images-amazon.com/images/I/41u-RICMB8L._SX342_.jpg\n",
            "Downloading B004DMXG4S from http://ecx.images-amazon.com/images/I/31BcIy4ZZ9L._SX342_.jpg\n",
            "Downloading B008OTL242 from http://ecx.images-amazon.com/images/I/41v3DQPMB%2BL._SX342_.jpg\n",
            "Downloading B00BR3C2J6 from http://ecx.images-amazon.com/images/I/31nqR803TDL._SX342_.jpg\n",
            "Downloading B00AYKTQ9C from http://ecx.images-amazon.com/images/I/511puQQ3YcL._SX342_.jpg\n",
            "Downloading B0058MIAS4 from http://ecx.images-amazon.com/images/I/41EgGh6nFDL._SX342_.jpg\n",
            "Downloading B0079EHQGG from http://ecx.images-amazon.com/images/I/41tE1y7RmvL._SY445_.jpg\n",
            "Downloading B00CM73M3Q from http://ecx.images-amazon.com/images/I/41G6lzEbXFL._SY200_.jpg\n",
            "Failed to download B00CM73M3Q: HTTP 404\n",
            "Downloading B0096NM8KC from http://ecx.images-amazon.com/images/I/41M%2BJHaUScL._SX342_.jpg\n",
            "Downloading B00BB7FJRU from http://ecx.images-amazon.com/images/I/31WD0n4H%2BuL._SY200_.jpg\n",
            "Downloading B007WASWOW from http://ecx.images-amazon.com/images/I/41U5PZCdkcL._SX342_.jpg\n",
            "Downloading B00C6TWPI8 from http://ecx.images-amazon.com/images/I/41AOirTviSL._SY445_.jpg\n",
            "Downloading B006VA9B08 from http://ecx.images-amazon.com/images/I/41D7diCsDYL._SX342_.jpg\n",
            "Downloading B009THYKIS from http://ecx.images-amazon.com/images/I/41mFf3ENpQL._SY445_.jpg\n",
            "Downloading B008UAEUYE from http://ecx.images-amazon.com/images/I/41f9CWlQ59L._SX342_.jpg\n",
            "Downloading B009NGCDYI from http://ecx.images-amazon.com/images/I/31HU-pGYRZL._SY445_.jpg\n",
            "Downloading B00D2CVCBY from http://ecx.images-amazon.com/images/I/41SlYuVHr1L._SX342_.jpg\n",
            "Downloading B00BGZRDSK from http://ecx.images-amazon.com/images/I/41IvQQ1j0dL._SY445_.jpg\n",
            "Downloading B00DSRC0SW from http://ecx.images-amazon.com/images/I/41hKJdcF7EL._SX342_.jpg\n",
            "Downloading B004ZIJB8A from http://ecx.images-amazon.com/images/I/41EtNNkKKBL._SX342_.jpg\n",
            "Downloading B009CC8WSO from http://ecx.images-amazon.com/images/I/41xb%2BY8ZOXL._SY445_.jpg\n",
            "Downloading B002NVGA30 from http://ecx.images-amazon.com/images/I/41rBq0Yu-yL._SY445_.jpg\n",
            "Downloading B00CTRSDJC from http://ecx.images-amazon.com/images/I/41G3RbUbPxL._SY445_.jpg\n",
            "Downloading B008CO888C from http://ecx.images-amazon.com/images/I/41JgsmC4GCL._SY445_.jpg\n",
            "Downloading B004BX98O6 from http://ecx.images-amazon.com/images/I/41zZxVbpzvL._SX342_.jpg\n",
            "Downloading B00EU9E3ZK from http://g-ecx.images-amazon.com/images/G/01/x-locale/brands/small-logo/3032414011._CB359598581_SX150_.jpg\n",
            "Downloading B00CDAKGRW from http://ecx.images-amazon.com/images/I/41Eb4%2BpsNuL._SX342_.jpg\n",
            "Downloading B008YQW44C from http://ecx.images-amazon.com/images/I/21RAcMcbLcL.jpg\n",
            "Downloading B008FXOLSG from http://ecx.images-amazon.com/images/I/31K45D8fYgL._SX342_.jpg\n",
            "Downloading B00EETDREE from http://ecx.images-amazon.com/images/I/31ijJ%2BxYL0L._SY200_.jpg\n",
            "Failed to download B00EETDREE: HTTP 404\n",
            "Downloading B007H12CWY from http://ecx.images-amazon.com/images/I/41Tz7ipTl2L._SX342_.jpg\n",
            "Downloading B00BQZAX2I from http://ecx.images-amazon.com/images/I/31zjlHKRy-L._SX342_.jpg\n",
            "Downloading B00C65Y800 from http://ecx.images-amazon.com/images/I/31Oy8tONtEL._SX342_.jpg\n",
            "Downloading B008CO7ODM from http://ecx.images-amazon.com/images/I/41DkRDiXvWL._SX342_.jpg\n",
            "Downloading B008JBP5KC from http://ecx.images-amazon.com/images/I/31H83DWh7fL._SX342_.jpg\n",
            "Downloading B007U2A4Y8 from http://ecx.images-amazon.com/images/I/31YdRFDY9iL._SX342_.jpg\n",
            "Downloading B00AZYXPDA from http://ecx.images-amazon.com/images/I/315C6hqRgxL._SY200_.jpg\n",
            "Downloading B006DYETUE from http://ecx.images-amazon.com/images/I/41qgZv2vvwL._SY445_.jpg\n",
            "Downloading B0083COA5S from http://ecx.images-amazon.com/images/I/41mm5CtEQAL._SY445_.jpg\n",
            "Downloading B00AOC82HW from http://ecx.images-amazon.com/images/I/31xtcJR3lDL._SX342_.jpg\n",
            "Downloading B00EDS5Q0O from http://ecx.images-amazon.com/images/I/41rSKvtJLFL._SY445_.jpg\n",
            "Downloading B008VUZF92 from http://ecx.images-amazon.com/images/I/41WG-v5qegL._SX342_.jpg\n",
            "Downloading B00D8DW8CY from http://ecx.images-amazon.com/images/I/51N%2B64YSj5L._SX342_.jpg\n",
            "Downloading B002PEH0RK from http://ecx.images-amazon.com/images/I/51bYNj%2BlsKL._SY445_.jpg\n",
            "Downloading B004RDQD0M from http://ecx.images-amazon.com/images/I/41OjqjAK7eL._SX342_.jpg\n",
            "Downloading B009WJ0LYA from http://ecx.images-amazon.com/images/I/31cJV3PGLOL._SX342_.jpg\n",
            "Downloading B00DURAMP8 from http://ecx.images-amazon.com/images/I/31QXPOhGfPL._SX160_.jpg\n",
            "Downloading B00D6AP2J0 from http://ecx.images-amazon.com/images/I/31cuX4oz7yL._SY445_.jpg\n",
            "Downloading B005AGP1EO from http://ecx.images-amazon.com/images/I/41Hnuh0oR%2BL._SX342_.jpg\n",
            "Downloading B002TK59CS from http://ecx.images-amazon.com/images/I/419Eau9L1gL._SY445_.jpg\n",
            "Downloading B008OC8IF0 from http://ecx.images-amazon.com/images/I/314ZZPeDkEL._SX342_.jpg\n",
            "Downloading B008YR3AAS from http://ecx.images-amazon.com/images/I/412Incypj4L._SY445_.jpg\n",
            "Downloading B007WAEKJI from http://ecx.images-amazon.com/images/I/31eBgJdTZIL._SX342_.jpg\n",
            "Downloading B00FJDYWAC from http://ecx.images-amazon.com/images/I/31v4C4Kc--L._SX342_.jpg\n",
            "Downloading B009LJ95N4 from http://ecx.images-amazon.com/images/I/41kgibNXxwL._SX342_.jpg\n",
            "Downloading B007H0R31U from http://ecx.images-amazon.com/images/I/31FMZBjZMfL._SX342_.jpg\n",
            "Downloading B008DSMW20 from http://ecx.images-amazon.com/images/I/3179X%2BdgHpL._SX342_.jpg\n",
            "Downloading B00GNLSVVK from http://ecx.images-amazon.com/images/I/413p4WFF-DL._SX342_.jpg\n",
            "Downloading B009NVZUK2 from http://ecx.images-amazon.com/images/I/41eavhmU2TL._SY445_.jpg\n",
            "Downloading B0089K2X58 from http://ecx.images-amazon.com/images/I/41EiJjIECCL._SX342_.jpg\n",
            "Downloading B0076OE6QC from http://ecx.images-amazon.com/images/I/41nbwh9mqZL._SX342_.jpg\n",
            "Downloading B009LM7BWS from http://ecx.images-amazon.com/images/I/31kDFt2XaqL._SY445_.jpg\n",
            "Downloading B005PHRIWQ from http://ecx.images-amazon.com/images/I/31bDeu-IODL._SY445_.jpg\n",
            "Downloading B008E6CMZI from http://ecx.images-amazon.com/images/I/31O0sO78KqL._SX342_.jpg\n",
            "Downloading B00BMAPBWO from http://ecx.images-amazon.com/images/I/41UEtWrZFgL._SY445_.jpg\n",
            "Downloading B001ELTWX0 from http://ecx.images-amazon.com/images/I/21oNHdEaRWL.jpg\n",
            "Downloading B002F0HXCG from http://ecx.images-amazon.com/images/I/4118ZysrUlL.jpg\n",
            "Downloading B00A6X2EXM from http://ecx.images-amazon.com/images/I/41SwJOwFlmL._SX342_.jpg\n",
            "Downloading B00E34SMMM from http://ecx.images-amazon.com/images/I/31lACDgDCCL._SY445_.jpg\n",
            "Downloading B00CA3JIBC from http://ecx.images-amazon.com/images/I/51fy-LKWO5L._SX342_.jpg\n",
            "Downloading B000E2F2VG from http://ecx.images-amazon.com/images/I/411FKFY1CQL._SY445_.jpg\n",
            "Downloading B0091S4O6I from http://ecx.images-amazon.com/images/I/41qOADemFSL._SX342_.jpg\n",
            "Downloading B008ZXZRRK from http://ecx.images-amazon.com/images/I/41p7qt9gKOL._SY445_.jpg\n",
            "Downloading B007E9XB88 from http://ecx.images-amazon.com/images/I/41BttaiQMfL._SX342_.jpg\n",
            "Downloading B005G16AIO from http://ecx.images-amazon.com/images/I/31Z94D2mJjL._SX342_.jpg\n",
            "Downloading B004XBC8FW from http://ecx.images-amazon.com/images/I/412BtqhXDaL._SX342_.jpg\n",
            "Downloading B003DO56SG from http://ecx.images-amazon.com/images/I/51j1fYf4%2BML._SY445_.jpg\n",
            "Downloading B005DTLG74 from http://ecx.images-amazon.com/images/I/41acNSwEtgL._SY445_.jpg\n",
            "Downloading B00EECCZ08 from http://ecx.images-amazon.com/images/I/51v48oFEbWL._SY445_.jpg\n",
            "Downloading B0052R8BM0 from http://ecx.images-amazon.com/images/I/41%2BmCwQuVvL._SX342_.jpg\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import requests   # To download images from URLs\n",
        "import json\n",
        "\n",
        "# Path to ASIN-to-URL mapping file\n",
        "ASIN2URL_FILE = \"fashion-iq-metadata/image_url/asin2url.dress.txt\"\n",
        "\n",
        "# Path to annotation JSON file for dresses\n",
        "ANNOTATION_FILE = \"fashion-iq/captions/cap.dress.train.json\"\n",
        "\n",
        "# Directory where downloaded images will be saved\n",
        "OUTPUT_DIR = \"fashion-iq-metadata/image_url/downloaded_images\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)  # Create directory if it doesn't exist\n",
        "\n",
        "# Maximum number of images to download\n",
        "NUM_IMAGES = 1000\n",
        "\n",
        "# Load annotations JSON\n",
        "with open(ANNOTATION_FILE, 'r') as f:\n",
        "    annotations = json.load(f)\n",
        "\n",
        "# Collect all valid ASINs from candidate and target fields in annotations\n",
        "valid_asins = {annotation['candidate'] for annotation in annotations} | {annotation['target'] for annotation in annotations}\n",
        "\n",
        "# Read the ASIN-to-URL file and keep only pairs where ASIN is in valid_asins\n",
        "with open(ASIN2URL_FILE, 'r') as f:\n",
        "    # Each line is split into [ASIN, URL]\n",
        "    asin_url_pairs = [line.strip().split(maxsplit=1) for line in f if line.split(maxsplit=1)[0] in valid_asins]\n",
        "\n",
        "# Randomly select a subset of ASIN-URL pairs to limit downloads\n",
        "random_subset = random.sample(asin_url_pairs, min(NUM_IMAGES, len(asin_url_pairs)))\n",
        "\n",
        "# Download images from the selected subset\n",
        "for asin, url in random_subset:\n",
        "    # Set output file path for the image\n",
        "    output_path = os.path.join(OUTPUT_DIR, f\"{asin}.jpg\")\n",
        "\n",
        "    # Only download if the image file doesn't already exist\n",
        "    if not os.path.exists(output_path):\n",
        "        try:\n",
        "            print(f\"Downloading {asin} from {url}\")\n",
        "\n",
        "            # Send HTTP GET request to download image\n",
        "            response = requests.get(url, stream=True)\n",
        "\n",
        "            # Check if the request was successful\n",
        "            if response.status_code == 200:\n",
        "                # Write image in chunks to avoid memory issues\n",
        "                with open(output_path, 'wb') as img_file:\n",
        "                    for chunk in response.iter_content(1024):  # 1 KB chunks\n",
        "                        img_file.write(chunk)\n",
        "            else:\n",
        "                print(f\"Failed to download {asin}: HTTP {response.status_code}\")\n",
        "        except Exception as e:\n",
        "            # Catch any errors (network, invalid URL, etc.)\n",
        "            print(f\"Error downloading {asin}: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "-GHeB7x6m3fB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c36f5122-3944-4cd3-a806-296d003c5a50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing category: dress\n",
            "CSV file saved for category 'dress' at: fashion-iq/annotations/dress_annotations.csv\n",
            "Processing category: shirt\n",
            "CSV file saved for category 'shirt' at: fashion-iq/annotations/shirt_annotations.csv\n",
            "Processing category: toptee\n",
            "CSV file saved for category 'toptee' at: fashion-iq/annotations/toptee_annotations.csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Base directory for the FashionIQ dataset\n",
        "BASE_DIR = \"fashion-iq\"\n",
        "\n",
        "# Directory containing caption JSON files\n",
        "CAPTIONS_DIR = os.path.join(BASE_DIR, \"captions\")\n",
        "\n",
        "# Directory containing train/val/test image split JSON files\n",
        "IMAGE_SPLITS_DIR = os.path.join(BASE_DIR, \"image_splits\")\n",
        "\n",
        "# Directory where processed annotation CSVs will be saved\n",
        "OUTPUT_DIR = os.path.join(BASE_DIR, \"annotations\")\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)  # Create directory if it doesn't exist\n",
        "\n",
        "def load_image_splits(category, split):\n",
        "    \"\"\"\n",
        "    Load image splits for a given category and split (train/val/test).\n",
        "\n",
        "    Args:\n",
        "        category (str): Category name (e.g., 'dress', 'shirt', 'toptee')\n",
        "        split (str): Split type ('train', 'val', 'test')\n",
        "\n",
        "    Returns:\n",
        "        set: Set of image IDs included in this split\n",
        "    \"\"\"\n",
        "    split_file = os.path.join(IMAGE_SPLITS_DIR, f\"split.{category}.{split}.json\")\n",
        "    with open(split_file, \"r\") as f:\n",
        "        return set(json.load(f))  # Return as a set for faster lookup\n",
        "\n",
        "def process_category(category):\n",
        "    \"\"\"\n",
        "    Process all splits for a given category and generate structured rows.\n",
        "\n",
        "    Args:\n",
        "        category (str): Category name\n",
        "\n",
        "    Returns:\n",
        "        list: List of dictionaries with split, category, reference/target images, and captions\n",
        "    \"\"\"\n",
        "    splits = [\"train\", \"val\", \"test\"]  # Define dataset splits\n",
        "    rows = []  # Initialize list to store processed rows\n",
        "\n",
        "    for split in splits:\n",
        "\n",
        "        # Load the valid image IDs for this split\n",
        "        valid_images = load_image_splits(category, split)\n",
        "\n",
        "        # Path to the caption JSON file for this category and split\n",
        "        captions_file = os.path.join(CAPTIONS_DIR, f\"cap.{category}.{split}.json\")\n",
        "        if not os.path.exists(captions_file):\n",
        "            print(f\"File not found: {captions_file}\")\n",
        "            continue  # Skip if file does not exist\n",
        "\n",
        "        # Load caption data from JSON\n",
        "        with open(captions_file, \"r\") as f:\n",
        "            captions_data = json.load(f)\n",
        "\n",
        "        # Iterate over each annotation in the JSON\n",
        "        for item in captions_data:\n",
        "            ref_image = item[\"candidate\"]  # Reference image ASIN\n",
        "            captions = item.get(\"captions\", [])  # List of captions describing differences\n",
        "            target_image = item.get(\"target\", \"unknown\" if split == \"test\" else None)  # Target image ASIN\n",
        "\n",
        "            # Skip if reference image is not part of the valid split\n",
        "            if ref_image not in valid_images:\n",
        "                continue\n",
        "\n",
        "            # For non-test splits, skip if target image is missing or invalid\n",
        "            if split != \"test\" and (not target_image or target_image not in valid_images):\n",
        "                continue\n",
        "\n",
        "            # Create a row for each caption\n",
        "            for caption in captions:\n",
        "                rows.append({\n",
        "                    \"split\": split,  # Dataset split\n",
        "                    \"category\": category,  # Category name\n",
        "                    \"reference_image\": ref_image,  # Reference image ASIN\n",
        "                    \"target_image\": target_image,  # Target image ASIN\n",
        "                    \"text_description\": caption  # Caption text\n",
        "                })\n",
        "\n",
        "    return rows  # Return all processed rows for this category\n",
        "\n",
        "# Process all categories and save as CSV\n",
        "categories = [\"dress\", \"shirt\", \"toptee\"]\n",
        "for category in categories:\n",
        "    print(f\"Processing category: {category}\")\n",
        "    rows = process_category(category)\n",
        "\n",
        "    # Convert list of dictionaries to pandas DataFrame\n",
        "    df_category = pd.DataFrame(rows)\n",
        "\n",
        "    # Save DataFrame as CSV for easy use in model training\n",
        "    category_csv_path = os.path.join(OUTPUT_DIR, f\"{category}_annotations.csv\")\n",
        "    df_category.to_csv(category_csv_path, index=False)\n",
        "    print(f\"CSV file saved for category '{category}' at: {category_csv_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "RF1l0THBm6ej",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee5b8eda-2ddc-4a44-d70a-816fbf88e0e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered annotations saved to /content/fashion-iq/common_ids.json\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "# File paths\n",
        "ANNOTATION_FILE = \"fashion-iq/captions/cap.dress.train.json\"  # JSON file with captions for dress images\n",
        "ASIN2URL_FILE = \"fashion-iq-metadata/image_url/asin2url.dress.txt\"  # Text file mapping ASINs to image URLs\n",
        "COMMON_IDS_FILE = \"/content/fashion-iq/common_ids.json\"  # Output JSON file to save filtered annotations\n",
        "\n",
        "# Load all ASINs that have image URLs\n",
        "asin_set = set()  # Use a set for fast lookup\n",
        "with open(ASIN2URL_FILE, 'r') as f:\n",
        "    for line in f:\n",
        "        asin, _ = line.strip().split(maxsplit=1)  # Split line into ASIN and URL\n",
        "        asin_set.add(asin)  # Add ASIN to the set\n",
        "\n",
        "# Load annotations from the JSON file\n",
        "with open(ANNOTATION_FILE, 'r') as f:\n",
        "    annotations = json.load(f)  # List of dictionaries, each with 'candidate', 'target', and 'captions'\n",
        "\n",
        "# Filter annotations to keep only those where both candidate and target images exist in ASIN set\n",
        "filtered_annotations = []  # Initialize list to store filtered annotations\n",
        "for annotation in annotations:\n",
        "    candidate_id = annotation['candidate']  # Candidate image ASIN\n",
        "    target_id = annotation['target']        # Target image ASIN\n",
        "\n",
        "    # Include only if both candidate and target ASINs have images available\n",
        "    if candidate_id in asin_set and target_id in asin_set:\n",
        "        filtered_annotations.append(annotation)\n",
        "\n",
        "# Save the filtered annotations to a new JSON file\n",
        "with open(COMMON_IDS_FILE, 'w') as f:\n",
        "    json.dump(filtered_annotations, f, indent=4)  # Pretty-print with indentation\n",
        "\n",
        "print(f\"Filtered annotations saved to {COMMON_IDS_FILE}\")  # Confirmation message\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DD2yMz01m8se"
      },
      "source": [
        "CLEANING DOWNLOADED IMAGES SUCH THAT ONLY FOUND ASINS ARE CONSIDERED FOR TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "AjBJDmUKm9Ml",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e171b8c-78aa-46e7-8ea4-9406347f797b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Searching in: /content/fashion-iq\n",
            "Searching in: /content/fashion-iq-metadata/image_url/downloaded_images\n",
            "Searching in: /content/fashion-iq-metadata/image_url/broken_links\n",
            "Total common ASINs: 7757\n",
            "Found ASINs: 952\n",
            "Missing ASINs: 6805\n",
            "Found ASINs saved to: /content/fashion-iq/images/dress/found_asins.json\n",
            "Missing ASINs saved to: /content/fashion-iq/images/dress/missing_asins.json\n",
            "ASIN-to-directory mapping saved to: /content/fashion-iq/images/dress/asin_to_directory.json\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "# Path to the filtered annotations JSON file\n",
        "common_ids_file = '/content/fashion-iq/common_ids.json'\n",
        "\n",
        "# Directories to search for downloaded images\n",
        "image_directories = [\n",
        "    '/content/fashion-iq',\n",
        "    '/content/fashion-iq-metadata/image_url/downloaded_images',\n",
        "    '/content/fashion-iq-metadata/image_url/broken_links'\n",
        "]\n",
        "\n",
        "# Directory to save cleaned image info\n",
        "output_dir = '/content/fashion-iq/images/dress'\n",
        "asin_to_directory_file = os.path.join(output_dir, 'asin_to_directory.json')\n",
        "\n",
        "# Create the output directory if it doesn't exist\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Load the filtered annotations\n",
        "with open(common_ids_file, 'r') as f:\n",
        "    common_ids_data = json.load(f)\n",
        "\n",
        "# Create a set of all candidate and target ASINs\n",
        "common_ids = {entry['candidate'] for entry in common_ids_data} | {entry['target'] for entry in common_ids_data}\n",
        "\n",
        "# Initialize sets to track found and missing ASINs\n",
        "found_asins = set()\n",
        "missing_asins = set(common_ids)\n",
        "asin_to_directory = {}\n",
        "\n",
        "# Iterate through each directory to find images corresponding to the ASINs\n",
        "for directory in image_directories:\n",
        "    print(f\"Searching in: {directory}\")\n",
        "    if os.path.exists(directory):\n",
        "        for file in os.listdir(directory):\n",
        "            file_name, file_ext = os.path.splitext(file)  # Split filename and extension\n",
        "            if file_name in common_ids:  # Check if file corresponds to a required ASIN\n",
        "                found_asins.add(file_name)\n",
        "                missing_asins.discard(file_name)  # Remove found ASIN from missing set\n",
        "                asin_to_directory[file_name] = directory  # Record the directory of the image\n",
        "\n",
        "# Print summary statistics\n",
        "print(f\"Total common ASINs: {len(common_ids)}\")\n",
        "print(f\"Found ASINs: {len(found_asins)}\")\n",
        "print(f\"Missing ASINs: {len(missing_asins)}\")\n",
        "\n",
        "# Paths to save JSON lists of found and missing ASINs\n",
        "found_asins_file = os.path.join(output_dir, 'found_asins.json')\n",
        "missing_asins_file = os.path.join(output_dir, 'missing_asins.json')\n",
        "\n",
        "# Save found ASINs to a JSON file\n",
        "with open(found_asins_file, 'w') as f:\n",
        "    json.dump(list(found_asins), f)\n",
        "\n",
        "# Save missing ASINs to a JSON file\n",
        "with open(missing_asins_file, 'w') as f:\n",
        "    json.dump(list(missing_asins), f)\n",
        "\n",
        "# Save mapping of ASINs to directories\n",
        "with open(asin_to_directory_file, 'w') as f:\n",
        "    json.dump(asin_to_directory, f)\n",
        "\n",
        "# Print confirmation messages\n",
        "print(f\"Found ASINs saved to: {found_asins_file}\")\n",
        "print(f\"Missing ASINs saved to: {missing_asins_file}\")\n",
        "print(f\"ASIN-to-directory mapping saved to: {asin_to_directory_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHAMVQ1enRSh"
      },
      "source": [
        "# **DATA** **CLEANING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "K0yI9VlrnR2F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0aee67b-2ffa-43c1-dbfc-121d62bdf12d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total files in directory: 949\n",
            "Files kept: 949\n",
            "Files deleted: 0\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "# Path to the JSON file containing ASINs of images that were successfully found\n",
        "found_asins_file = '/content/fashion-iq/images/dress/found_asins.json'\n",
        "\n",
        "# Directory containing all downloaded images\n",
        "images_directory = '/content/fashion-iq-metadata/image_url/downloaded_images'\n",
        "\n",
        "# Load the set of found ASINs from the JSON file\n",
        "with open(found_asins_file, 'r') as f:\n",
        "    found_asins = set(json.load(f))\n",
        "\n",
        "# List all files currently in the images directory\n",
        "all_files = os.listdir(images_directory)\n",
        "\n",
        "# Counters for tracking deleted and kept files\n",
        "deleted_files = 0\n",
        "kept_files = 0\n",
        "\n",
        "# Iterate over each file in the directory\n",
        "for file in all_files:\n",
        "    file_name, file_ext = os.path.splitext(file)  # Separate the filename from its extension\n",
        "    if file_name not in found_asins:  # Check if the file is not in the set of valid ASINs\n",
        "        os.remove(os.path.join(images_directory, file))  # Delete files that are not needed\n",
        "        print(f\"Deleted: {file}\")\n",
        "        deleted_files += 1\n",
        "    else:\n",
        "        kept_files += 1  # Count files that are kept\n",
        "\n",
        "# Print summary statistics after cleaning\n",
        "print(f\"Total files in directory: {len(all_files)}\")\n",
        "print(f\"Files kept: {kept_files}\")\n",
        "print(f\"Files deleted: {deleted_files}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JvwiVptnU0P"
      },
      "source": [
        "# **FINDING COMMON FROM CAP.DRESS.TRAIN AND DOWNLOADED IMAGES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "7zquvrqonVkl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a0b878d-455f-48d6-a3b7-b2a5df2913b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total entries in common set: 84\n",
            "Filtered entries saved to: /content/fashion-iq/captions/cap.train.json\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "# Directory where downloaded images are stored\n",
        "downloaded_images_dir = '/content/fashion-iq-metadata/image_url/downloaded_images'\n",
        "\n",
        "# List of JSON files containing caption data to filter\n",
        "json_files = [\n",
        "    '/content/fashion-iq/captions/cap.dress.train.json'\n",
        "]\n",
        "\n",
        "# Output JSON file to store filtered entries\n",
        "output_file = '/content/fashion-iq/captions/cap.train.json'\n",
        "\n",
        "# Create a set of ASINs corresponding to images that exist in the downloaded images directory\n",
        "downloaded_asins = {os.path.splitext(file)[0] for file in os.listdir(downloaded_images_dir)}\n",
        "\n",
        "# Initialize a list to store entries where both candidate and target images exist\n",
        "common_entries = []\n",
        "\n",
        "# Iterate through each JSON file to filter entries\n",
        "for json_file in json_files:\n",
        "    with open(json_file, 'r') as f:\n",
        "        data = json.load(f)\n",
        "        for entry in data:\n",
        "            target = entry['target']\n",
        "            candidate = entry['candidate']\n",
        "            # Only keep entries where both target and candidate images are present\n",
        "            if target in downloaded_asins and candidate in downloaded_asins:\n",
        "                common_entries.append(entry)\n",
        "\n",
        "# Save the filtered entries to the output JSON file\n",
        "with open(output_file, 'w') as f:\n",
        "    json.dump(common_entries, f, indent=4)\n",
        "\n",
        "# Print summary of filtering results\n",
        "print(f\"Total entries in common set: {len(common_entries)}\")\n",
        "print(f\"Filtered entries saved to: {output_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "8Z9wBDktnYT_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45af620b-acff-4b05-9515-e0c709010970"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copied: B00BJMIKPK.jpg\n",
            "Copied: B00FTGZDXY.jpg\n",
            "Copied: B0060OU0P0.jpg\n",
            "Copied: B00B7DZNG0.jpg\n",
            "Copied: B00C4OMRUQ.jpg\n",
            "Copied: B00A3USRAW.jpg\n",
            "Copied: B00E0CYF9G.jpg\n",
            "Copied: B004ZWGGDY.jpg\n",
            "Copied: B00AIBYOW6.jpg\n",
            "Copied: B00CFU74SY.jpg\n",
            "Copied: B008VIU2V0.jpg\n",
            "Copied: B00DP4WHK4.jpg\n",
            "Copied: B005TD07GK.jpg\n",
            "Copied: B004QTQ8FW.jpg\n",
            "Copied: B00DH8YRWO.jpg\n",
            "Copied: B009LJ95N4.jpg\n",
            "Copied: B008FWZP5K.jpg\n",
            "Copied: B003IF4HLC.jpg\n",
            "Copied: B008P5OFHQ.jpg\n",
            "Copied: B00BM1QDWU.jpg\n",
            "Copied: B00EW41B30.jpg\n",
            "Copied: B007G10BPA.jpg\n",
            "Copied: B005KQWIYK.jpg\n",
            "Copied: B006BBKVPG.jpg\n",
            "Copied: B0098MOTAI.jpg\n",
            "Copied: B005EVNJ1C.jpg\n",
            "Copied: B00BEMDD0C.jpg\n",
            "Copied: B00CQSTQCW.jpg\n",
            "Copied: B004LRBIJA.jpg\n",
            "Copied: B00CDAL83I.jpg\n",
            "Copied: B006VA9B08.jpg\n",
            "Copied: B0061K4F7M.jpg\n",
            "Copied: B006H2GU0E.jpg\n",
            "Copied: B00BBQCWY4.jpg\n",
            "Copied: B00ARHNXWI.jpg\n",
            "Copied: B004SLT8MI.jpg\n",
            "Copied: B00BM7318S.jpg\n",
            "Copied: B008UAEXZK.jpg\n",
            "Copied: B006WXNZHO.jpg\n",
            "Copied: B00DJZPTO0.jpg\n",
            "Copied: B0056F24M6.jpg\n",
            "Copied: B00G7H39YY.jpg\n",
            "Copied: B008BA7THS.jpg\n",
            "Copied: B007WC0S2O.jpg\n",
            "Copied: B00385WM5A.jpg\n",
            "Copied: B00B9B8CJU.jpg\n",
            "Copied: B00730QYY6.jpg\n",
            "Copied: B0039YOYNI.jpg\n",
            "Copied: B008CFECDG.jpg\n",
            "Copied: B004I6M7HQ.jpg\n",
            "Copied: B006J42TDC.jpg\n",
            "Copied: B00428BYUG.jpg\n",
            "Copied: B00BHSAXQU.jpg\n",
            "Copied: B009LM7BWS.jpg\n",
            "Copied: B009VYW3Y2.jpg\n",
            "Copied: B000PIGECA.jpg\n",
            "Copied: B00AIXPFR2.jpg\n",
            "Copied: B005SN1M74.jpg\n",
            "Copied: B004FPIBPW.jpg\n",
            "Copied: B009HG2R3G.jpg\n",
            "Copied: B004QVSUKG.jpg\n",
            "Copied: B0073114DG.jpg\n",
            "Copied: B00FNZN4BY.jpg\n",
            "Copied: B00CC6ZLZO.jpg\n",
            "Copied: B004M6J946.jpg\n",
            "Copied: B0075NK230.jpg\n",
            "Copied: B00GT2KXEG.jpg\n",
            "Copied: B004L2L9SK.jpg\n",
            "Copied: B00CMEN80Q.jpg\n",
            "Copied: B008S4CQCU.jpg\n",
            "Copied: B0088B32OO.jpg\n",
            "Copied: B008EQ06I8.jpg\n",
            "Copied: B0088A1GCA.jpg\n",
            "Copied: B006DKKCEK.jpg\n",
            "Copied: B004BX98O6.jpg\n",
            "Copied: B009JG2AR2.jpg\n",
            "Copied: B00B4GJ4BK.jpg\n",
            "Copied: B006O3EH7Y.jpg\n",
            "Copied: B005ACHE0C.jpg\n",
            "Copied: B008AJ092M.jpg\n",
            "Copied: B00DEECQW4.jpg\n",
            "Copied: B00G0SF448.jpg\n",
            "Copied: B00CC6ZJPQ.jpg\n",
            "Copied: B008LRMY3A.jpg\n",
            "Copied: B00BFFAA1S.jpg\n",
            "Copied: B009WJ0LYA.jpg\n",
            "Copied: B00BU01DQO.jpg\n",
            "Copied: B004ZWCHBE.jpg\n",
            "Copied: B00CLPGF38.jpg\n",
            "Copied: B007WASWOW.jpg\n",
            "Copied: B0058MIAS4.jpg\n",
            "Copied: B008I2VXU8.jpg\n",
            "Copied: B005940YOO.jpg\n",
            "Copied: B008200YHE.jpg\n",
            "Copied: B004U9YUG2.jpg\n",
            "Copied: B006X53UZI.jpg\n",
            "Copied: B004I6PIPY.jpg\n",
            "Copied: B00CHSEO7I.jpg\n",
            "Copied: B00FJDYWAC.jpg\n",
            "Copied: B008FYW62I.jpg\n",
            "Copied: B00A3UTGMK.jpg\n",
            "Copied: B005DTLG74.jpg\n",
            "Copied: B0085IY406.jpg\n",
            "Copied: B00EIQK9WQ.jpg\n",
            "Copied: B00AM1U4R6.jpg\n",
            "Copied: B008X8A5WE.jpg\n",
            "Copied: B00ECF1I28.jpg\n",
            "Copied: B0089I9NFS.jpg\n",
            "Copied: B005N7GPO0.jpg\n",
            "Copied: B0093PD0T6.jpg\n",
            "Copied: B00GYL56PS.jpg\n",
            "Copied: B007WACXG0.jpg\n",
            "Copied: B008E6CMZI.jpg\n",
            "Copied: B005EVNHD2.jpg\n",
            "Copied: B005DC3A34.jpg\n",
            "Copied: B00A1CI0HC.jpg\n",
            "Copied: B00E0GPEQU.jpg\n",
            "Copied: B00DQZISFK.jpg\n",
            "Copied: B003VWOX6Q.jpg\n",
            "Copied: B007CVN2NM.jpg\n",
            "Copied: B007CDY5R2.jpg\n",
            "Copied: B008KSA964.jpg\n",
            "Copied: B00297C9Y2.jpg\n",
            "Copied: B00EIQLXTE.jpg\n",
            "Copied: B00BT0P7LC.jpg\n",
            "Copied: B004UJIXV0.jpg\n",
            "Copied: B008ZXTX84.jpg\n",
            "Copied: B009THYKIS.jpg\n",
            "Copied: B005RYO48S.jpg\n",
            "Copied: B009NDC7V0.jpg\n",
            "Copied: B008PHPQ5O.jpg\n",
            "Copied: B009S5Y2FM.jpg\n",
            "Copied: B00AIXFL58.jpg\n",
            "Copied: B00935MSN0.jpg\n",
            "Copied: B004NRO7N2.jpg\n",
            "Copied: B004FEE206.jpg\n",
            "Copied: B004I5A7SS.jpg\n",
            "Copied: B005R4QUF8.jpg\n",
            "Copied: B0084OF18A.jpg\n",
            "Copied: B007SRJ3CE.jpg\n",
            "Copied: B00B4IBVQY.jpg\n",
            "Copied: B00CC6ZKPA.jpg\n",
            "Copied: B003Z99POK.jpg\n",
            "Copied: B004ZF6TS8.jpg\n",
            "Copied: B00791C4MK.jpg\n",
            "Copied: B007CPTQ1A.jpg\n",
            "Copied: B008596N18.jpg\n",
            "Copied: B008SMUFO8.jpg\n",
            "Copied: B008VPNVDO.jpg\n",
            "Copied: B006GK20BK.jpg\n",
            "Copied: B005WKBXHW.jpg\n",
            "Copied: B003HE66SG.jpg\n",
            "Copied: B004J254Z6.jpg\n",
            "Copied: B00BI4KLU6.jpg\n",
            "Copied: B0052R8BM0.jpg\n",
            "Copied: B00740VYFE.jpg\n",
            "Copied: B000OVR5E4.jpg\n",
            "Copied: B0091GQSAK.jpg\n",
            "Total images in cap.train.json: 158\n",
            "Total images copied: 158\n",
            "Images saved to: /content/fashion-iq-metadata/image_url/downloaded_images_short\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import shutil\n",
        "\n",
        "# Path to the filtered captions JSON file\n",
        "cap_train_json = '/content/fashion-iq/captions/cap.train.json'\n",
        "\n",
        "# Directory where all downloaded images are stored\n",
        "downloaded_images_dir = '/content/fashion-iq-metadata/image_url/downloaded_images'\n",
        "\n",
        "# Directory where only images referenced in cap.train.json will be copied\n",
        "output_dir = '/content/fashion-iq-metadata/image_url/downloaded_images_short'\n",
        "\n",
        "# Create the output directory if it does not exist\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Load the filtered caption data\n",
        "with open(cap_train_json, 'r') as f:\n",
        "    cap_train_data = json.load(f)\n",
        "\n",
        "# Collect all ASINs (both candidate and target) from the filtered captions\n",
        "cap_train_asins = set()\n",
        "for entry in cap_train_data:\n",
        "    cap_train_asins.add(entry['candidate'])\n",
        "    cap_train_asins.add(entry['target'])\n",
        "\n",
        "# Copy images corresponding to the collected ASINs to the output directory\n",
        "copied_count = 0\n",
        "for asin in cap_train_asins:\n",
        "    image_file = f\"{asin}.jpg\"  # Construct the image file name\n",
        "    src_path = os.path.join(downloaded_images_dir, image_file)  # Source path\n",
        "    dst_path = os.path.join(output_dir, image_file)  # Destination path\n",
        "    if os.path.exists(src_path):\n",
        "        shutil.copy(src_path, dst_path)\n",
        "        copied_count += 1\n",
        "        print(f\"Copied: {image_file}\")\n",
        "    else:\n",
        "        print(f\"Image not found: {image_file}\")\n",
        "\n",
        "# Print summary of the copying process\n",
        "print(f\"Total images in cap.train.json: {len(cap_train_asins)}\")\n",
        "print(f\"Total images copied: {copied_count}\")\n",
        "print(f\"Images saved to: {output_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "JNYaNpypneKk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c7d9bac-1964-4618-a490-270a8bb58869"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data saved to: /content/fashion-iq/captions/cap.train_split.json\n",
            "Testing data saved to: /content/fashion-iq/captions/cap.test_split.json\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import random\n",
        "import os\n",
        "\n",
        "# Path to the filtered captions JSON file\n",
        "input_json = '/content/fashion-iq/captions/cap.train.json'\n",
        "\n",
        "# Paths where the train and test split JSON files will be saved\n",
        "train_output_json = '/content/fashion-iq/captions/cap.train_split.json'\n",
        "test_output_json = '/content/fashion-iq/captions/cap.test_split.json'\n",
        "\n",
        "# Load the filtered caption data\n",
        "with open(input_json, 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Shuffle the data randomly to ensure a random distribution in train/test sets\n",
        "random.shuffle(data)\n",
        "\n",
        "# Compute index for 80-20 split\n",
        "split_idx = int(len(data) * 0.8)\n",
        "\n",
        "# Split data into training (80%) and testing (20%) sets\n",
        "train_data = data[:split_idx]\n",
        "test_data = data[split_idx:]\n",
        "\n",
        "# Save training data to JSON file with pretty formatting\n",
        "with open(train_output_json, 'w') as f:\n",
        "    json.dump(train_data, f, indent=4)\n",
        "\n",
        "# Save testing data to JSON file with pretty formatting\n",
        "with open(test_output_json, 'w') as f:\n",
        "    json.dump(test_data, f, indent=4)\n",
        "\n",
        "# Print summary of saved files\n",
        "print(f\"Training data saved to: {train_output_json}\")\n",
        "print(f\"Testing data saved to: {test_output_json}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qahyS3fdn7sF"
      },
      "source": [
        "# **OPENCLIP MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "TS6vA33eoAKg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbc5a78e-3201-4c09-a9df-1f708ee4357a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: open_clip_torch in /usr/local/lib/python3.12/dist-packages (3.2.0)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (0.24.0+cu126)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (2025.11.3)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (6.3.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (4.67.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (0.36.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (0.7.0)\n",
            "Requirement already satisfied: timm>=1.0.17 in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (1.0.22)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from timm>=1.0.17->open_clip_torch) (6.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (3.5.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy->open_clip_torch) (0.2.14)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->open_clip_torch) (25.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->open_clip_torch) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->open_clip_torch) (1.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->open_clip_torch) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->open_clip_torch) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0->open_clip_torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0->open_clip_torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->open_clip_torch) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->open_clip_torch) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->open_clip_torch) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->open_clip_torch) (2025.11.12)\n"
          ]
        }
      ],
      "source": [
        "!pip install open_clip_torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install open_clip_torch"
      ],
      "metadata": {
        "id": "R0QKIH8aLSxa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b72fe06-b397-4f24-fcd9-22638a85338c"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: open_clip_torch in /usr/local/lib/python3.12/dist-packages (3.2.0)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (0.24.0+cu126)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (2025.11.3)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (6.3.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (4.67.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (0.36.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (0.7.0)\n",
            "Requirement already satisfied: timm>=1.0.17 in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (1.0.22)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from timm>=1.0.17->open_clip_torch) (6.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (3.5.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy->open_clip_torch) (0.2.14)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->open_clip_torch) (25.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->open_clip_torch) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->open_clip_torch) (1.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->open_clip_torch) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->open_clip_torch) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0->open_clip_torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0->open_clip_torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->open_clip_torch) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->open_clip_torch) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->open_clip_torch) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->open_clip_torch) (2025.11.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "-lFJLRRcoEsW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ab0cfd0-89a2-46b4-ca5f-15d218df9c41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/open_clip/factory.py:450: UserWarning: QuickGELU mismatch between final model config (quick_gelu=False) and pretrained tag 'openai' (quick_gelu=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10: 100%|| 3/3 [00:01<00:00,  1.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 4.065611998240153\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/10: 100%|| 3/3 [00:01<00:00,  2.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/10, Loss: 2.752739747365316\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/10: 100%|| 3/3 [00:01<00:00,  2.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/10, Loss: 2.696298678716024\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/10: 100%|| 3/3 [00:01<00:00,  2.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/10, Loss: 2.6826771100362143\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/10: 100%|| 3/3 [00:01<00:00,  2.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/10, Loss: 2.6839863459269204\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/10: 100%|| 3/3 [00:01<00:00,  2.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/10, Loss: 2.6788615385691323\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/10: 100%|| 3/3 [00:01<00:00,  2.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/10, Loss: 2.677632729212443\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/10: 100%|| 3/3 [00:01<00:00,  1.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/10, Loss: 2.679178555806478\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/10: 100%|| 3/3 [00:02<00:00,  1.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/10, Loss: 2.6772395769755044\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/10: 100%|| 3/3 [00:01<00:00,  2.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/10, Loss: 2.6758139928181968\n",
            "Model saved to /content/fashion-iq/OUTPUT/openclip_model.pt\n",
            "Testing embedding for image: /content/fashion-iq-metadata/image_url/downloaded_images_short/B008EQ06I8.jpg\n",
            "Predicted embedding shape: torch.Size([1, 512])\n"
          ]
        }
      ],
      "source": [
        "import os  # Import os module for file and directory operations\n",
        "import json  # Import json module to read/write JSON files\n",
        "import torch  # Import PyTorch for deep learning\n",
        "from torch.utils.data import Dataset, DataLoader  # Import Dataset and DataLoader for handling data\n",
        "from PIL import Image  # Import PIL for image loading\n",
        "from torchvision import transforms  # Import torchvision transforms for image preprocessing\n",
        "from tqdm import tqdm  # Import tqdm for progress bars\n",
        "import open_clip  # Import OpenCLIP library\n",
        "import random  # Import random module for random selection\n",
        "\n",
        "train_json = '/content/fashion-iq/captions/cap.train_split.json'  # Path to training JSON\n",
        "images_dir = '/content/fashion-iq-metadata/image_url/downloaded_images_short'  # Directory with images\n",
        "output_model_path = '/content/fashion-iq/OUTPUT/openclip_model.pt'  # Path to save trained model\n",
        "\n",
        "batch_size = 32  # Number of samples per batch\n",
        "num_epochs = 10  # Number of training epochs\n",
        "learning_rate = 1e-4  # Learning rate for optimizer\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # Use GPU if available, else CPU\n",
        "\n",
        "class CaptionDataset(Dataset):\n",
        "    def __init__(self, json_path, images_dir, transform=None):\n",
        "        self.data = json.load(open(json_path, 'r'))  # Load JSON annotations\n",
        "        self.images_dir = images_dir  # Store images directory path\n",
        "        self.transform = transform  # Store image transform function\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)  # Return number of samples in dataset\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        entry = self.data[idx]  # Get data entry at index\n",
        "        candidate_image_path = os.path.join(self.images_dir, f\"{entry['candidate']}.jpg\")  # Path to candidate image\n",
        "        text = entry['captions'][0]  # Get first caption text\n",
        "\n",
        "        if not os.path.exists(candidate_image_path):  # Check if image file exists\n",
        "            raise FileNotFoundError(f\"Image not found: {candidate_image_path}\")  # Raise error if missing\n",
        "\n",
        "        candidate_image = Image.open(candidate_image_path).convert(\"RGB\")  # Open image and convert to RGB\n",
        "        if self.transform:  # Apply transforms if provided\n",
        "            candidate_image = self.transform(candidate_image)\n",
        "\n",
        "        return candidate_image, text  # Return image and text\n",
        "\n",
        "transform = transforms.Compose([  # Compose multiple image transformations\n",
        "    transforms.Resize((224, 224)),  # Resize image to 224x224\n",
        "    transforms.ToTensor(),  # Convert image to PyTorch tensor\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),  # Normalize image\n",
        "])\n",
        "\n",
        "train_dataset = CaptionDataset(train_json, images_dir, transform=transform)  # Create dataset object\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)  # Create DataLoader for batching\n",
        "\n",
        "model = open_clip.create_model(\"ViT-B-32\", pretrained=\"openai\")  # Load OpenCLIP ViT-B-32 model\n",
        "model = model.to(device)  # Move model to GPU or CPU\n",
        "tokenizer = open_clip.get_tokenizer(\"ViT-B-32\")  # Load tokenizer for text captions\n",
        "\n",
        "from open_clip import image_transform  # Import image_transform utility\n",
        "preprocess = image_transform(model.visual.image_size, is_train=False)  # Create preprocessing function for single images\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()  # Define cross-entropy loss\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)  # Define AdamW optimizer\n",
        "\n",
        "print(\"Starting Training...\")  # Print message indicating training start\n",
        "for epoch in range(num_epochs):  # Loop over each epoch\n",
        "    model.train()  # Set model to training mode\n",
        "    total_loss = 0.0  # Initialize loss for the epoch\n",
        "    for images, texts in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):  # Loop over batches\n",
        "        images = images.to(device)  # Move images to GPU/CPU\n",
        "        tokenized_texts = tokenizer(texts).to(device)  # Tokenize captions and move to GPU/CPU\n",
        "\n",
        "        image_features = model.encode_image(images)  # Encode images into feature vectors\n",
        "        text_features = model.encode_text(tokenized_texts)  # Encode texts into feature vectors\n",
        "\n",
        "        logits_per_image = image_features @ text_features.T  # Compute similarity logits (image->text)\n",
        "        logits_per_text = text_features @ image_features.T  # Compute similarity logits (text->image)\n",
        "        labels = torch.arange(len(images)).to(device)  # Create target labels for loss\n",
        "\n",
        "        loss = (criterion(logits_per_image, labels) + criterion(logits_per_text, labels)) / 2  # Compute average loss\n",
        "        total_loss += loss.item()  # Add batch loss to total\n",
        "\n",
        "        optimizer.zero_grad()  # Reset gradients\n",
        "        loss.backward()  # Backpropagate\n",
        "        optimizer.step()  # Update model parameters\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(train_loader)}\")  # Print epoch loss\n",
        "\n",
        "os.makedirs(os.path.dirname(output_model_path), exist_ok=True)  # Make directory if it doesn't exist\n",
        "torch.save(model, output_model_path)  # Save trained model\n",
        "print(f\"Model saved to {output_model_path}\")  # Print confirmation\n",
        "\n",
        "def predict_description(candidate_image_path, model, tokenizer, transform):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    candidate_image = Image.open(candidate_image_path).convert(\"RGB\")  # Load image\n",
        "    candidate_image = transform(candidate_image).unsqueeze(0).to(device)  # Transform and add batch dimension\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient computation\n",
        "        image_features = model.encode_image(candidate_image)  # Encode image\n",
        "\n",
        "    return image_features  # Return embedding\n",
        "\n",
        "existing_images = os.listdir(images_dir)  # List all images in directory\n",
        "if len(existing_images) == 0:  # Check if directory is empty\n",
        "    raise FileNotFoundError(f\"No images found in {images_dir}\")  # Raise error if empty\n",
        "\n",
        "candidate_image = os.path.join(images_dir, random.choice(existing_images))  # Pick a random image\n",
        "print(\"Testing embedding for image:\", candidate_image)  # Print image path\n",
        "\n",
        "embedding = predict_description(candidate_image, model, tokenizer, preprocess)  # Get embedding\n",
        "print(\"Predicted embedding shape:\", embedding.shape)  # Print embedding shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "jdPF50AwoJzj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84e57be7-a60c-4b09-9199-96537252dae7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing images...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 158/158 [00:01<00:00, 136.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All images processed and saved to: /content/fashion-iq-metadata/image_url/processed_images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import os  # Import os module for file and directory operations\n",
        "from PIL import Image  # Import PIL for opening and saving images\n",
        "from torchvision import transforms  # Import torchvision transforms for preprocessing\n",
        "from tqdm import tqdm  # Import tqdm for progress bars\n",
        "\n",
        "input_dir = '/content/fashion-iq-metadata/image_url/downloaded_images_short'  # Directory with raw images\n",
        "output_dir = '/content/fashion-iq-metadata/image_url/processed_images'  # Directory to save processed images\n",
        "\n",
        "os.makedirs(output_dir, exist_ok=True)  # Create output directory if it doesn't exist\n",
        "\n",
        "# Define image transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize images to 224x224 pixels\n",
        "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),  # Normalize pixel values\n",
        "])\n",
        "\n",
        "print(\"Processing images...\")  # Inform user that processing has started\n",
        "for filename in tqdm(os.listdir(input_dir)):  # Loop over all files in input directory with progress bar\n",
        "    input_path = os.path.join(input_dir, filename)  # Full path to the input image\n",
        "    output_path = os.path.join(output_dir, filename)  # Full path for saving processed image\n",
        "\n",
        "    try:\n",
        "        image = Image.open(input_path).convert(\"RGB\")  # Open image and convert to RGB\n",
        "        processed_image = transform(image)  # Apply transformations to the image\n",
        "\n",
        "        processed_image_pil = transforms.ToPILImage()(processed_image)  # Convert tensor back to PIL image\n",
        "        processed_image_pil.save(output_path, format=\"JPEG\")  # Save processed image in JPEG format\n",
        "    except Exception as e:  # Handle errors if image cannot be processed\n",
        "        print(f\"Error processing image {filename}: {e}\")  # Print error message\n",
        "\n",
        "print(\"All images processed and saved to:\", output_dir)  # Inform user that processing is complete\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Full working script: safely create/load an OpenCLIP model, ensure a saved checkpoint exists,\n",
        "# then run predict_description on one image. This fixes the FileNotFoundError by creating\n",
        "# and saving a checkpoint if it doesn't already exist.\n",
        "\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "from PIL import Image\n",
        "import open_clip\n",
        "\n",
        "# ---------- Settings ----------\n",
        "MODEL_PATH = \"model_complete.pth\"   # file that your original code expected\n",
        "CLIP_ARCH = \"ViT-B-32\"              # architecture used in your notebook\n",
        "DEVICE = \"cpu\"                      # keep CPU-compatible; change to \"cuda\" if you have GPU\n",
        "# ------------------------------\n",
        "\n",
        "device = torch.device(DEVICE)\n",
        "\n",
        "# Create model and preprocessing transforms (this returns the architecture and the matching preprocess)\n",
        "# If pretrained='openai' this will load the official pretrained weights.\n",
        "print(\"Creating model architecture and transforms...\")\n",
        "model, _, preprocess = open_clip.create_model_and_transforms(CLIP_ARCH, pretrained=\"openai\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# If the expected checkpoint exists, try to load it as a state_dict.\n",
        "# If it does not exist, save the current (pretrained) model's state_dict to MODEL_PATH so subsequent loads succeed.\n",
        "if os.path.exists(MODEL_PATH):\n",
        "    try:\n",
        "        print(f\"Found checkpoint at '{MODEL_PATH}'. Loading state_dict...\")\n",
        "        state = torch.load(MODEL_PATH, map_location=device)\n",
        "        # If the saved object is a full model, it may be necessary to detect that.\n",
        "        # We'll try to load as state_dict first; if that fails, attempt direct torch.load model.\n",
        "        if isinstance(state, dict):\n",
        "            # Common case: saved state_dict\n",
        "            model.load_state_dict(state)\n",
        "            print(\"Loaded state_dict into model.\")\n",
        "        else:\n",
        "            # If someone saved the full model object (less common/less recommended),\n",
        "            # replace the model variable with the loaded object.\n",
        "            model = state.to(device)\n",
        "            model.eval()\n",
        "            print(\"Loaded full model object from checkpoint.\")\n",
        "    except Exception as e:\n",
        "        # Fall back: warn and continue with the pretrained model in memory\n",
        "        print(\"Warning: failed to load checkpoint as state_dict or model object.\")\n",
        "        print(\"Error:\", e)\n",
        "        print(\"Continuing with the freshly created pretrained model (not loaded from checkpoint).\")\n",
        "else:\n",
        "    # No checkpoint file found  create one by saving the current pretrained model's state_dict.\n",
        "    try:\n",
        "        print(f\"No checkpoint found at '{MODEL_PATH}'. Saving current pretrained model.state_dict() to this path...\")\n",
        "        torch.save(model.state_dict(), MODEL_PATH)\n",
        "        print(f\"Saved pretrained model.state_dict() -> '{MODEL_PATH}'.\")\n",
        "    except Exception as e:\n",
        "        print(\"Error when attempting to save the model state_dict:\", e)\n",
        "        print(\"You may need write permissions for the current working directory.\")\n",
        "\n",
        "# Get tokenizer (OpenCLIP's tokenizer)\n",
        "tokenizer = open_clip.get_tokenizer(CLIP_ARCH)\n",
        "\n",
        "# Prediction helper\n",
        "def predict_description(image_path, model, tokenizer, all_descriptions, preprocess, device=\"cpu\"):\n",
        "    \"\"\"\n",
        "    Given an image_path and a list of candidate descriptions (strings),\n",
        "    return the description that has the highest cosine similarity with the image embedding.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(image_path):\n",
        "        raise FileNotFoundError(f\"Image not found: {image_path}\")\n",
        "\n",
        "    # Open and preprocess image\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    image_input = preprocess(image).unsqueeze(0).to(device)  # shape: (1, C, H, W)\n",
        "\n",
        "    # Tokenize all descriptions (this returns a tensor of token ids ready for model.encode_text)\n",
        "    text_tokens = tokenizer(all_descriptions).to(device)\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # encode_image returns (batch, dim)\n",
        "        image_features = model.encode_image(image_input)         # (1, D)\n",
        "        image_features = torch.nn.functional.normalize(image_features, p=2, dim=-1)\n",
        "\n",
        "        # encode_text accepts tokenized descriptions (N, token_len)\n",
        "        text_features = model.encode_text(text_tokens)          # (N, D)\n",
        "        text_features = torch.nn.functional.normalize(text_features, p=2, dim=-1)\n",
        "\n",
        "        # similarity: (1, N)\n",
        "        similarities = (image_features @ text_features.T)\n",
        "\n",
        "    best_idx = torch.argmax(similarities, dim=1).item()\n",
        "    return all_descriptions[best_idx]\n",
        "\n",
        "# -------------------------\n",
        "# Load candidate descriptions (test captions)\n",
        "# -------------------------\n",
        "descriptions_file = \"/content/fashion-iq/captions/cap.test_split.json\"\n",
        "\n",
        "if not os.path.exists(descriptions_file):\n",
        "    # Try alternative paths if the dataset was cloned elsewhere (give a helpful message)\n",
        "    print(\"Warning: descriptions file not found at the expected path:\", descriptions_file)\n",
        "    print(\"Make sure you have cloned the 'fashion-iq' dataset and that the path is correct.\")\n",
        "    # As a fallback, create a tiny dummy list so the demo still runs (but results won't be meaningful).\n",
        "    all_descriptions = [\n",
        "        \"sleeveless red dress with flared skirt\",\n",
        "        \"short-sleeve floral summer dress\",\n",
        "        \"long sleeve black top with buttons\",\n",
        "        \"blue denim jacket with pockets\",\n",
        "    ]\n",
        "    print(\"Using a small fallback list of descriptions (demo only).\")\n",
        "else:\n",
        "    with open(descriptions_file, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "    # Each entry in FashionIQ captions usually has a 'captions' list\n",
        "    all_descriptions = [entry[\"captions\"][0] if isinstance(entry.get(\"captions\"), list) and entry[\"captions\"] else \"\" for entry in data]\n",
        "    # Filter out any empty description entries\n",
        "    all_descriptions = [d for d in all_descriptions if d]\n",
        "\n",
        "# -------------------------\n",
        "# Run prediction on a sample image\n",
        "# -------------------------\n",
        "sample_image = \"/content/fashion-iq-metadata/image_url/downloaded_images/B00CL0KUU2.jpg\"\n",
        "\n",
        "if not os.path.exists(sample_image):\n",
        "    print(\"Warning: sample image not found at the expected path:\", sample_image)\n",
        "    print(\"Make sure you have downloaded images to '/content/fashion-iq-metadata/...'.\")\n",
        "    # Optionally you can set sample_image to a different local path here if you have one.\n",
        "else:\n",
        "    try:\n",
        "        predicted_description = predict_description(sample_image, model, tokenizer, all_descriptions, preprocess, device=DEVICE)\n",
        "        print(f\"Input Image: {sample_image}\")\n",
        "        print(f\"Predicted Description: {predicted_description}\")\n",
        "    except Exception as e:\n",
        "        print(\"Error during prediction:\", e)\n"
      ],
      "metadata": {
        "id": "T3EB22dj8t7G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53ee7044-4d02-48b8-b725-7a8ef693c752"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating model architecture and transforms...\n",
            "Found checkpoint at 'model_complete.pth'. Loading state_dict...\n",
            "Loaded state_dict into model.\n",
            "Warning: sample image not found at the expected path: /content/fashion-iq-metadata/image_url/downloaded_images/B00CL0KUU2.jpg\n",
            "Make sure you have downloaded images to '/content/fashion-iq-metadata/...'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install open_clip_torch"
      ],
      "metadata": {
        "id": "cChsGhCVJhZe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "632de701-1d9c-4d29-c233-48346772812a"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: open_clip_torch in /usr/local/lib/python3.12/dist-packages (3.2.0)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (0.24.0+cu126)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (2025.11.3)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (6.3.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (4.67.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (0.36.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (0.7.0)\n",
            "Requirement already satisfied: timm>=1.0.17 in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (1.0.22)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from timm>=1.0.17->open_clip_torch) (6.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (3.5.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy->open_clip_torch) (0.2.14)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->open_clip_torch) (25.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->open_clip_torch) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->open_clip_torch) (1.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->open_clip_torch) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->open_clip_torch) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0->open_clip_torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0->open_clip_torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->open_clip_torch) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->open_clip_torch) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->open_clip_torch) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->open_clip_torch) (2025.11.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TRAINING\n"
      ],
      "metadata": {
        "id": "9WRzk-PfP2zU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm\n",
        "import open_clip\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# ============================================\n",
        "# IMPROVED TRAINING CONFIGURATION\n",
        "# ============================================\n",
        "\n",
        "train_json = '/content/fashion-iq/captions/cap.train_split.json'\n",
        "images_dir = '/content/fashion-iq-metadata/image_url/downloaded_images_short'\n",
        "output_model_path = '/content/fashion-iq/OUTPUT/openclip_model.pt'\n",
        "\n",
        "os.makedirs('/content/fashion-iq/OUTPUT', exist_ok=True)\n",
        "\n",
        "# IMPROVED HYPERPARAMETERS\n",
        "batch_size = 8  # Smaller batch for better gradient updates with small dataset\n",
        "num_epochs = 50  # More epochs\n",
        "learning_rate = 5e-6  # Fine-tuned learning rate\n",
        "warmup_epochs = 5\n",
        "weight_decay = 0.1\n",
        "temperature = 0.07  # Temperature for contrastive loss\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"IMPROVED TRAINING CONFIGURATION\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"Batch size: {batch_size}\")\n",
        "print(f\"Epochs: {num_epochs}\")\n",
        "print(f\"Learning rate: {learning_rate}\")\n",
        "print(f\"Warmup epochs: {warmup_epochs}\")\n",
        "print(f\"Temperature: {temperature}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ============================================\n",
        "# IMPROVED DATASET WITH DATA AUGMENTATION\n",
        "# ============================================\n",
        "\n",
        "class ImprovedFashionDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Improved dataset that:\n",
        "    1. Uses BOTH captions per sample (doubles effective data)\n",
        "    2. Applies data augmentation\n",
        "    3. Includes target images for triplet-style learning\n",
        "    \"\"\"\n",
        "    def __init__(self, json_path, images_dir, preprocess, augment=True):\n",
        "        with open(json_path, 'r') as f:\n",
        "            self.raw_data = json.load(f)\n",
        "\n",
        "        self.images_dir = images_dir\n",
        "        self.preprocess = preprocess\n",
        "        self.augment = augment\n",
        "\n",
        "        # Expand data to use both captions\n",
        "        self.data = []\n",
        "        for entry in self.raw_data:\n",
        "            candidate_path = os.path.join(images_dir, f\"{entry['candidate']}.jpg\")\n",
        "            target_path = os.path.join(images_dir, f\"{entry['target']}.jpg\")\n",
        "\n",
        "            # Check if both images exist\n",
        "            if os.path.exists(candidate_path) and os.path.exists(target_path):\n",
        "                # Add entry for each caption (doubles the data)\n",
        "                for caption in entry.get('captions', []):\n",
        "                    if caption:\n",
        "                        self.data.append({\n",
        "                            'candidate_path': candidate_path,\n",
        "                            'target_path': target_path,\n",
        "                            'caption': caption,\n",
        "                            'candidate_id': entry['candidate'],\n",
        "                            'target_id': entry['target']\n",
        "                        })\n",
        "\n",
        "        print(f\"Loaded {len(self.data)} training samples (expanded from {len(self.raw_data)} entries)\")\n",
        "\n",
        "        # Augmentation transforms\n",
        "        self.augment_transform = transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(p=0.5),\n",
        "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "            transforms.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        entry = self.data[idx]\n",
        "\n",
        "        # Load candidate (reference) image\n",
        "        candidate_image = Image.open(entry['candidate_path']).convert(\"RGB\")\n",
        "\n",
        "        # Load target image\n",
        "        target_image = Image.open(entry['target_path']).convert(\"RGB\")\n",
        "\n",
        "        # Apply augmentation during training\n",
        "        if self.augment and random.random() > 0.5:\n",
        "            candidate_image = self.augment_transform(candidate_image)\n",
        "            target_image = self.augment_transform(target_image)\n",
        "\n",
        "        # Apply CLIP preprocessing\n",
        "        candidate_tensor = self.preprocess(candidate_image)\n",
        "        target_tensor = self.preprocess(target_image)\n",
        "\n",
        "        return {\n",
        "            'candidate': candidate_tensor,\n",
        "            'target': target_tensor,\n",
        "            'caption': entry['caption']\n",
        "        }\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# IMPROVED LOSS FUNCTION\n",
        "# ============================================\n",
        "\n",
        "class ImprovedContrastiveLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Improved loss that considers:\n",
        "    1. Reference image + text should match target image\n",
        "    2. Standard CLIP contrastive loss\n",
        "    3. Temperature scaling for better gradients\n",
        "    \"\"\"\n",
        "    def __init__(self, temperature=0.07):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        self.cross_entropy = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, image_features, text_features, target_features=None):\n",
        "        # Normalize features\n",
        "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        # Standard CLIP loss (image-text alignment)\n",
        "        logits_per_image = (image_features @ text_features.T) / self.temperature\n",
        "        logits_per_text = (text_features @ image_features.T) / self.temperature\n",
        "\n",
        "        batch_size = image_features.shape[0]\n",
        "        labels = torch.arange(batch_size, device=image_features.device)\n",
        "\n",
        "        loss_i2t = self.cross_entropy(logits_per_image, labels)\n",
        "        loss_t2i = self.cross_entropy(logits_per_text, labels)\n",
        "\n",
        "        clip_loss = (loss_i2t + loss_t2i) / 2\n",
        "\n",
        "        # If target features provided, add composed retrieval loss\n",
        "        if target_features is not None:\n",
        "            target_features = target_features / target_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "            # Combined query (reference image + text) should match target\n",
        "            combined_query = image_features + text_features\n",
        "            combined_query = combined_query / combined_query.norm(dim=-1, keepdim=True)\n",
        "\n",
        "            # Similarity between combined query and target\n",
        "            logits_composed = (combined_query @ target_features.T) / self.temperature\n",
        "\n",
        "            composed_loss = self.cross_entropy(logits_composed, labels)\n",
        "\n",
        "            total_loss = clip_loss + composed_loss\n",
        "        else:\n",
        "            total_loss = clip_loss\n",
        "\n",
        "        return total_loss\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# MODEL SETUP\n",
        "# ============================================\n",
        "\n",
        "print(\"\\nLoading OpenCLIP model...\")\n",
        "model, _, preprocess = open_clip.create_model_and_transforms(\"ViT-B-32\", pretrained=\"openai\")\n",
        "model = model.to(device)\n",
        "tokenizer = open_clip.get_tokenizer(\"ViT-B-32\")\n",
        "print(\" Model loaded successfully\")\n",
        "\n",
        "# ============================================\n",
        "# DATA PREPARATION\n",
        "# ============================================\n",
        "\n",
        "train_dataset = ImprovedFashionDataset(\n",
        "    train_json,\n",
        "    images_dir,\n",
        "    preprocess,\n",
        "    augment=True\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True if device == \"cuda\" else False,\n",
        "    drop_last=True  # Important for consistent batch sizes\n",
        ")\n",
        "\n",
        "print(f\"\\nDataset size: {len(train_dataset)} samples\")\n",
        "print(f\"Number of batches: {len(train_loader)}\")\n",
        "\n",
        "# ============================================\n",
        "# OPTIMIZER WITH LAYER-WISE LEARNING RATES\n",
        "# ============================================\n",
        "\n",
        "# Different learning rates for different parts of the model\n",
        "# Lower LR for pretrained layers, higher for later layers\n",
        "def get_parameter_groups(model, base_lr):\n",
        "    \"\"\"Create parameter groups with layer-wise learning rate decay.\"\"\"\n",
        "\n",
        "    # Visual encoder parameters\n",
        "    visual_params = []\n",
        "    # Text encoder parameters\n",
        "    text_params = []\n",
        "    # Other parameters\n",
        "    other_params = []\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        if 'visual' in name:\n",
        "            visual_params.append(param)\n",
        "        elif 'transformer' in name or 'token_embedding' in name or 'positional_embedding' in name:\n",
        "            text_params.append(param)\n",
        "        else:\n",
        "            other_params.append(param)\n",
        "\n",
        "    return [\n",
        "        {'params': visual_params, 'lr': base_lr * 0.1},  # Lower LR for visual\n",
        "        {'params': text_params, 'lr': base_lr * 0.1},    # Lower LR for text\n",
        "        {'params': other_params, 'lr': base_lr}          # Normal LR for projection heads\n",
        "    ]\n",
        "\n",
        "param_groups = get_parameter_groups(model, learning_rate)\n",
        "optimizer = torch.optim.AdamW(param_groups, lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "# Learning rate scheduler with warmup\n",
        "def get_lr_scheduler(optimizer, num_epochs, warmup_epochs, num_batches):\n",
        "    total_steps = num_epochs * num_batches\n",
        "    warmup_steps = warmup_epochs * num_batches\n",
        "\n",
        "    def lr_lambda(step):\n",
        "        if step < warmup_steps:\n",
        "            return step / warmup_steps\n",
        "        else:\n",
        "            progress = (step - warmup_steps) / (total_steps - warmup_steps)\n",
        "            return 0.5 * (1 + np.cos(np.pi * progress))\n",
        "\n",
        "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "scheduler = get_lr_scheduler(optimizer, num_epochs, warmup_epochs, len(train_loader))\n",
        "\n",
        "# Loss function\n",
        "criterion = ImprovedContrastiveLoss(temperature=temperature)\n",
        "\n",
        "# ============================================\n",
        "# TRAINING LOOP\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"STARTING IMPROVED TRAINING\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "best_loss = float('inf')\n",
        "training_history = []\n",
        "\n",
        "# Enable gradient checkpointing to save memory\n",
        "if hasattr(model.visual, 'set_grad_checkpointing'):\n",
        "    model.visual.set_grad_checkpointing(True)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    num_batches = 0\n",
        "\n",
        "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "    for batch in pbar:\n",
        "        candidate_images = batch['candidate'].to(device)\n",
        "        target_images = batch['target'].to(device)\n",
        "        captions = batch['caption']\n",
        "\n",
        "        # Tokenize captions\n",
        "        text_tokens = tokenizer(captions).to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        candidate_features = model.encode_image(candidate_images)\n",
        "        target_features = model.encode_image(target_images)\n",
        "        text_features = model.encode_text(text_tokens)\n",
        "\n",
        "        # Calculate loss with target features\n",
        "        loss = criterion(candidate_features, text_features, target_features)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping to prevent exploding gradients\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        num_batches += 1\n",
        "\n",
        "        # Update progress bar\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        pbar.set_postfix({\n",
        "            'loss': f'{loss.item():.4f}',\n",
        "            'lr': f'{current_lr:.2e}'\n",
        "        })\n",
        "\n",
        "    # Calculate epoch metrics\n",
        "    avg_loss = total_loss / num_batches\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "    training_history.append({\n",
        "        'epoch': epoch + 1,\n",
        "        'loss': avg_loss,\n",
        "        'learning_rate': current_lr\n",
        "    })\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs} | Loss: {avg_loss:.4f} | LR: {current_lr:.2e}\")\n",
        "\n",
        "    # Save best model\n",
        "    if avg_loss < best_loss:\n",
        "        best_loss = avg_loss\n",
        "        torch.save(model.state_dict(), output_model_path)\n",
        "        print(f\"   New best model saved (loss: {avg_loss:.4f})\")\n",
        "\n",
        "    # Early stopping check (optional)\n",
        "    if avg_loss < 0.1:\n",
        "        print(\"   Loss threshold reached, stopping early\")\n",
        "        break\n",
        "\n",
        "# ============================================\n",
        "# SAVE FINAL RESULTS\n",
        "# ============================================\n",
        "\n",
        "history_path = '/content/fashion-iq/OUTPUT/training_history.json'\n",
        "with open(history_path, 'w') as f:\n",
        "    json.dump(training_history, f, indent=2)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"TRAINING COMPLETED!\")\n",
        "print(\"=\" * 60)\n",
        "print(f\" Best loss: {best_loss:.4f}\")\n",
        "print(f\" Model saved to: {output_model_path}\")\n",
        "print(f\" History saved to: {history_path}\")\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "id": "6rgFVnwNP4aF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb572af6-ae25-4c2e-b448-2251eebd6454"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "IMPROVED TRAINING CONFIGURATION\n",
            "============================================================\n",
            "Device: cuda\n",
            "Batch size: 8\n",
            "Epochs: 50\n",
            "Learning rate: 5e-06\n",
            "Warmup epochs: 5\n",
            "Temperature: 0.07\n",
            "============================================================\n",
            "\n",
            "Loading OpenCLIP model...\n",
            " Model loaded successfully\n",
            "Loaded 134 training samples (expanded from 67 entries)\n",
            "\n",
            "Dataset size: 134 samples\n",
            "Number of batches: 16\n",
            "\n",
            "============================================================\n",
            "STARTING IMPROVED TRAINING\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/50: 100%|| 16/16 [00:10<00:00,  1.53it/s, loss=3.5919, lr=1.00e-07]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50 | Loss: 3.7490 | LR: 1.00e-07\n",
            "   New best model saved (loss: 3.7490)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/50: 100%|| 16/16 [00:07<00:00,  2.08it/s, loss=3.5407, lr=2.00e-07]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/50 | Loss: 3.6770 | LR: 2.00e-07\n",
            "   New best model saved (loss: 3.6770)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/50: 100%|| 16/16 [00:10<00:00,  1.56it/s, loss=3.5349, lr=3.00e-07]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/50 | Loss: 3.5685 | LR: 3.00e-07\n",
            "   New best model saved (loss: 3.5685)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/50: 100%|| 16/16 [00:05<00:00,  2.98it/s, loss=3.4243, lr=4.00e-07]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/50 | Loss: 3.4188 | LR: 4.00e-07\n",
            "   New best model saved (loss: 3.4188)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/50: 100%|| 16/16 [00:05<00:00,  2.96it/s, loss=3.4020, lr=5.00e-07]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/50 | Loss: 3.3313 | LR: 5.00e-07\n",
            "   New best model saved (loss: 3.3313)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/50: 100%|| 16/16 [00:06<00:00,  2.57it/s, loss=3.1565, lr=4.99e-07]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/50 | Loss: 3.1092 | LR: 4.99e-07\n",
            "   New best model saved (loss: 3.1092)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/50: 100%|| 16/16 [00:05<00:00,  2.90it/s, loss=3.1327, lr=4.98e-07]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/50 | Loss: 3.0521 | LR: 4.98e-07\n",
            "   New best model saved (loss: 3.0521)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/50: 100%|| 16/16 [00:06<00:00,  2.64it/s, loss=2.5206, lr=4.95e-07]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/50 | Loss: 2.9322 | LR: 4.95e-07\n",
            "   New best model saved (loss: 2.9322)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/50: 100%|| 16/16 [00:10<00:00,  1.53it/s, loss=2.4144, lr=4.90e-07]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/50 | Loss: 2.8078 | LR: 4.90e-07\n",
            "   New best model saved (loss: 2.8078)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/50: 100%|| 16/16 [00:05<00:00,  2.76it/s, loss=2.5032, lr=4.85e-07]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/50 | Loss: 2.6751 | LR: 4.85e-07\n",
            "   New best model saved (loss: 2.6751)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/50: 100%|| 16/16 [00:06<00:00,  2.37it/s, loss=2.4637, lr=4.78e-07]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/50 | Loss: 2.6066 | LR: 4.78e-07\n",
            "   New best model saved (loss: 2.6066)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/50: 100%|| 16/16 [00:05<00:00,  2.80it/s, loss=2.4464, lr=4.71e-07]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/50 | Loss: 2.4910 | LR: 4.71e-07\n",
            "   New best model saved (loss: 2.4910)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/50: 100%|| 16/16 [00:05<00:00,  2.81it/s, loss=2.2088, lr=4.62e-07]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/50 | Loss: 2.2374 | LR: 4.62e-07\n",
            "   New best model saved (loss: 2.2374)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/50: 100%|| 16/16 [00:05<00:00,  2.94it/s, loss=2.9954, lr=4.52e-07]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/50 | Loss: 2.3125 | LR: 4.52e-07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/50: 100%|| 16/16 [00:06<00:00,  2.58it/s, loss=2.0157, lr=4.42e-07]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/50 | Loss: 2.1601 | LR: 4.42e-07\n",
            "   New best model saved (loss: 2.1601)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/50: 100%|| 16/16 [00:05<00:00,  2.96it/s, loss=2.2297, lr=4.30e-07]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/50 | Loss: 1.9543 | LR: 4.30e-07\n",
            "   New best model saved (loss: 1.9543)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/50: 100%|| 16/16 [00:05<00:00,  2.88it/s, loss=2.3046, lr=4.17e-07]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/50 | Loss: 1.9874 | LR: 4.17e-07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/50: 100%|| 16/16 [00:05<00:00,  2.70it/s, loss=1.6265, lr=4.04e-07]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/50 | Loss: 1.8089 | LR: 4.04e-07\n",
            "   New best model saved (loss: 1.8089)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/50: 100%|| 16/16 [00:05<00:00,  2.96it/s, loss=1.1579, lr=3.90e-07]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/50 | Loss: 1.6610 | LR: 3.90e-07\n",
            "   New best model saved (loss: 1.6610)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/50: 100%|| 16/16 [00:06<00:00,  2.41it/s, loss=1.7267, lr=3.75e-07]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/50 | Loss: 1.6522 | LR: 3.75e-07\n",
            "   New best model saved (loss: 1.6522)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21/50: 100%|| 16/16 [00:05<00:00,  2.87it/s, loss=1.3916, lr=3.60e-07]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21/50 | Loss: 1.5729 | LR: 3.60e-07\n",
            "   New best model saved (loss: 1.5729)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22/50: 100%|| 16/16 [00:05<00:00,  3.03it/s, loss=1.2019, lr=3.44e-07]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22/50 | Loss: 1.4619 | LR: 3.44e-07\n",
            "   New best model saved (loss: 1.4619)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23/50: 100%|| 16/16 [00:05<00:00,  2.70it/s, loss=1.9085, lr=3.27e-07]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23/50 | Loss: 1.3381 | LR: 3.27e-07\n",
            "   New best model saved (loss: 1.3381)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24/50: 100%|| 16/16 [00:05<00:00,  2.98it/s, loss=1.4797, lr=3.10e-07]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24/50 | Loss: 1.2034 | LR: 3.10e-07\n",
            "   New best model saved (loss: 1.2034)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25/50: 100%|| 16/16 [00:06<00:00,  2.47it/s, loss=1.1668, lr=2.93e-07]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25/50 | Loss: 1.1763 | LR: 2.93e-07\n",
            "   New best model saved (loss: 1.1763)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 26/50: 100%|| 16/16 [00:06<00:00,  2.54it/s, loss=0.7678, lr=2.76e-07]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26/50 | Loss: 1.2185 | LR: 2.76e-07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 27/50: 100%|| 16/16 [00:05<00:00,  3.01it/s, loss=0.8183, lr=2.59e-07]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27/50 | Loss: 1.1105 | LR: 2.59e-07\n",
            "   New best model saved (loss: 1.1105)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 28/50: 100%|| 16/16 [00:06<00:00,  2.61it/s, loss=1.1469, lr=2.41e-07]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28/50 | Loss: 1.0475 | LR: 2.41e-07\n",
            "   New best model saved (loss: 1.0475)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 29/50: 100%|| 16/16 [00:05<00:00,  2.77it/s, loss=0.6845, lr=2.24e-07]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29/50 | Loss: 1.0411 | LR: 2.24e-07\n",
            "   New best model saved (loss: 1.0411)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 30/50: 100%|| 16/16 [00:05<00:00,  2.99it/s, loss=0.8919, lr=2.07e-07]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30/50 | Loss: 0.8993 | LR: 2.07e-07\n",
            "   New best model saved (loss: 0.8993)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 31/50: 100%|| 16/16 [00:05<00:00,  2.96it/s, loss=0.8030, lr=1.90e-07]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 31/50 | Loss: 0.8983 | LR: 1.90e-07\n",
            "   New best model saved (loss: 0.8983)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 32/50: 100%|| 16/16 [00:05<00:00,  2.93it/s, loss=0.9031, lr=1.73e-07]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 32/50 | Loss: 0.8513 | LR: 1.73e-07\n",
            "   New best model saved (loss: 0.8513)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 33/50: 100%|| 16/16 [00:05<00:00,  2.90it/s, loss=0.6189, lr=1.56e-07]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 33/50 | Loss: 0.8826 | LR: 1.56e-07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 34/50: 100%|| 16/16 [00:05<00:00,  2.90it/s, loss=0.8169, lr=1.40e-07]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 34/50 | Loss: 0.9321 | LR: 1.40e-07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 35/50: 100%|| 16/16 [00:05<00:00,  2.72it/s, loss=0.9235, lr=1.25e-07]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 35/50 | Loss: 0.8256 | LR: 1.25e-07\n",
            "   New best model saved (loss: 0.8256)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 36/50: 100%|| 16/16 [00:06<00:00,  2.46it/s, loss=0.7081, lr=1.10e-07]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 36/50 | Loss: 0.8653 | LR: 1.10e-07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 37/50: 100%|| 16/16 [00:05<00:00,  2.75it/s, loss=0.6851, lr=9.61e-08]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 37/50 | Loss: 0.8700 | LR: 9.61e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 38/50: 100%|| 16/16 [00:06<00:00,  2.56it/s, loss=1.3431, lr=8.27e-08]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 38/50 | Loss: 0.8833 | LR: 8.27e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 39/50: 100%|| 16/16 [00:05<00:00,  2.93it/s, loss=0.6938, lr=7.02e-08]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 39/50 | Loss: 0.9250 | LR: 7.02e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 40/50: 100%|| 16/16 [00:07<00:00,  2.26it/s, loss=0.6075, lr=5.85e-08]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 40/50 | Loss: 0.7699 | LR: 5.85e-08\n",
            "   New best model saved (loss: 0.7699)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 41/50: 100%|| 16/16 [00:05<00:00,  2.83it/s, loss=0.7418, lr=4.77e-08]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 41/50 | Loss: 0.8198 | LR: 4.77e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 42/50: 100%|| 16/16 [00:05<00:00,  2.75it/s, loss=0.8137, lr=3.80e-08]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 42/50 | Loss: 0.8521 | LR: 3.80e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 43/50: 100%|| 16/16 [00:05<00:00,  2.97it/s, loss=0.9364, lr=2.93e-08]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 43/50 | Loss: 0.9075 | LR: 2.93e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 44/50: 100%|| 16/16 [00:06<00:00,  2.54it/s, loss=0.6007, lr=2.16e-08]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 44/50 | Loss: 0.8120 | LR: 2.16e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 45/50: 100%|| 16/16 [00:05<00:00,  2.90it/s, loss=1.1817, lr=1.51e-08]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 45/50 | Loss: 0.8475 | LR: 1.51e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 46/50: 100%|| 16/16 [00:06<00:00,  2.35it/s, loss=0.4019, lr=9.68e-09]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 46/50 | Loss: 0.7281 | LR: 9.68e-09\n",
            "   New best model saved (loss: 0.7281)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 47/50: 100%|| 16/16 [00:06<00:00,  2.63it/s, loss=0.5579, lr=5.46e-09]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 47/50 | Loss: 0.6920 | LR: 5.46e-09\n",
            "   New best model saved (loss: 0.6920)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 48/50: 100%|| 16/16 [00:06<00:00,  2.58it/s, loss=0.9850, lr=2.43e-09]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 48/50 | Loss: 0.7281 | LR: 2.43e-09\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 49/50: 100%|| 16/16 [00:05<00:00,  2.93it/s, loss=0.5446, lr=6.09e-10]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 49/50 | Loss: 0.7638 | LR: 6.09e-10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 50/50: 100%|| 16/16 [00:05<00:00,  2.68it/s, loss=0.4681, lr=0.00e+00]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 50/50 | Loss: 0.7125 | LR: 0.00e+00\n",
            "\n",
            "============================================================\n",
            "TRAINING COMPLETED!\n",
            "============================================================\n",
            " Best loss: 0.6920\n",
            " Model saved to: /content/fashion-iq/OUTPUT/openclip_model.pt\n",
            " History saved to: /content/fashion-iq/OUTPUT/training_history.json\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generate Embeddings\n"
      ],
      "metadata": {
        "id": "vLEhroVAQyql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "import open_clip\n",
        "\n",
        "# ============================================\n",
        "# REGENERATE EMBEDDINGS WITH IMPROVED MODEL\n",
        "# ============================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"REGENERATING EMBEDDINGS WITH IMPROVED MODEL\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\\n\")\n",
        "\n",
        "# Paths\n",
        "IMAGES_DIR = '/content/fashion-iq-metadata/image_url/downloaded_images_short'\n",
        "MODEL_PATH = '/content/fashion-iq/OUTPUT/openclip_model.pt'\n",
        "OUTPUT_EMBEDDINGS_FILE = '/content/fashion-iq/OUTPUT/image_embeddings.pkl'\n",
        "OUTPUT_METADATA_FILE = '/content/fashion-iq/OUTPUT/image_metadata.json'\n",
        "\n",
        "# Verify model exists\n",
        "if not os.path.exists(MODEL_PATH):\n",
        "    raise FileNotFoundError(f\"Model not found at {MODEL_PATH}\")\n",
        "\n",
        "os.makedirs('/content/fashion-iq/OUTPUT', exist_ok=True)\n",
        "\n",
        "# ============================================\n",
        "# LOAD IMPROVED MODEL\n",
        "# ============================================\n",
        "\n",
        "print(\"Loading improved model...\")\n",
        "model, _, preprocess = open_clip.create_model_and_transforms(\"ViT-B-32\", pretrained=\"openai\")\n",
        "model.to(device)\n",
        "\n",
        "# Load fine-tuned weights\n",
        "state_dict = torch.load(MODEL_PATH, map_location=device)\n",
        "model.load_state_dict(state_dict, strict=True)\n",
        "model.eval()\n",
        "print(\" Improved model loaded successfully\\n\")\n",
        "\n",
        "# ============================================\n",
        "# GENERATE EMBEDDINGS\n",
        "# ============================================\n",
        "\n",
        "all_embeddings = []\n",
        "all_image_ids = []\n",
        "all_image_paths = []\n",
        "\n",
        "image_files = [f for f in os.listdir(IMAGES_DIR) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "print(f\"Processing {len(image_files)} images...\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    for filename in tqdm(image_files, desc=\"Generating embeddings\"):\n",
        "        image_path = os.path.join(IMAGES_DIR, filename)\n",
        "        image_id = os.path.splitext(filename)[0]\n",
        "\n",
        "        try:\n",
        "            image = Image.open(image_path).convert(\"RGB\")\n",
        "            image_tensor = preprocess(image).unsqueeze(0).to(device)\n",
        "\n",
        "            embedding = model.encode_image(image_tensor)\n",
        "            embedding = embedding / embedding.norm(dim=-1, keepdim=True)\n",
        "\n",
        "            all_embeddings.append(embedding.cpu().numpy())\n",
        "            all_image_ids.append(image_id)\n",
        "            all_image_paths.append(image_path)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n Error processing {filename}: {e}\")\n",
        "            continue\n",
        "\n",
        "# Convert to array\n",
        "all_embeddings = np.vstack(all_embeddings)\n",
        "\n",
        "print(f\"\\n Generated {len(all_embeddings)} embeddings\")\n",
        "print(f\" Embedding shape: {all_embeddings.shape}\")\n",
        "\n",
        "# Save embeddings\n",
        "print(f\"\\nSaving embeddings...\")\n",
        "with open(OUTPUT_EMBEDDINGS_FILE, 'wb') as f:\n",
        "    pickle.dump({\n",
        "        'embeddings': all_embeddings,\n",
        "        'image_ids': all_image_ids,\n",
        "        'image_paths': all_image_paths\n",
        "    }, f)\n",
        "\n",
        "# Save metadata\n",
        "metadata = {\n",
        "    'total_images': len(all_image_ids),\n",
        "    'embedding_dimension': all_embeddings.shape[1],\n",
        "    'image_ids': all_image_ids,\n",
        "    'image_paths': all_image_paths\n",
        "}\n",
        "\n",
        "with open(OUTPUT_METADATA_FILE, 'w') as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "\n",
        "print(f\" Embeddings saved to: {OUTPUT_EMBEDDINGS_FILE}\")\n",
        "print(f\" Metadata saved to: {OUTPUT_METADATA_FILE}\")\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WeRzi5nZQ21y",
        "outputId": "78e7fd72-2e1c-467f-ac76-3d0598a06cab"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "REGENERATING EMBEDDINGS WITH IMPROVED MODEL\n",
            "============================================================\n",
            "Using device: cuda\n",
            "\n",
            "Loading improved model...\n",
            " Improved model loaded successfully\n",
            "\n",
            "Processing 158 images...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating embeddings: 100%|| 158/158 [00:03<00:00, 41.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Generated 158 embeddings\n",
            " Embedding shape: (158, 512)\n",
            "\n",
            "Saving embeddings...\n",
            " Embeddings saved to: /content/fashion-iq/OUTPUT/image_embeddings.pkl\n",
            " Metadata saved to: /content/fashion-iq/OUTPUT/image_metadata.json\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Image Retrieval Function\n"
      ],
      "metadata": {
        "id": "oNs7gWkVXDHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import pickle\n",
        "import open_clip\n",
        "\n",
        "# ============================================\n",
        "# IMPROVED IMAGE + TEXT RETRIEVAL\n",
        "# ============================================\n",
        "\n",
        "def retrieve_similar_images_improved(\n",
        "    query_image_path,\n",
        "    text_query,\n",
        "    model,\n",
        "    preprocess,\n",
        "    tokenizer,\n",
        "    device,\n",
        "    top_k=5,\n",
        "    text_weight=0.5\n",
        "):\n",
        "    \"\"\"\n",
        "    Improved retrieval with better similarity computation.\n",
        "    Uses additive composition as trained.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load embeddings database\n",
        "    with open('/content/fashion-iq/OUTPUT/image_embeddings.pkl', 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "        all_embeddings = data['embeddings']\n",
        "        all_image_ids = data['image_ids']\n",
        "        all_image_paths = data['image_paths']\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Encode reference image\n",
        "        query_image = Image.open(query_image_path).convert(\"RGB\")\n",
        "        query_tensor = preprocess(query_image).unsqueeze(0).to(device)\n",
        "        image_embedding = model.encode_image(query_tensor)\n",
        "        image_embedding = image_embedding / image_embedding.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        # Encode text modification\n",
        "        if text_query and text_query.strip():\n",
        "            text_tokens = tokenizer([text_query]).to(device)\n",
        "            text_embedding = model.encode_text(text_tokens)\n",
        "            text_embedding = text_embedding / text_embedding.norm(dim=-1, keepdim=True)\n",
        "\n",
        "            # ADDITIVE COMPOSITION (matching training)\n",
        "            # This is how the model was trained - add image and text features\n",
        "            combined_embedding = image_embedding + text_embedding\n",
        "            combined_embedding = combined_embedding / combined_embedding.norm(dim=-1, keepdim=True)\n",
        "            combined_embedding = combined_embedding.cpu().numpy()\n",
        "        else:\n",
        "            combined_embedding = image_embedding.cpu().numpy()\n",
        "\n",
        "    # Calculate similarities\n",
        "    similarities = np.dot(all_embeddings, combined_embedding.T).flatten()\n",
        "\n",
        "    # Get top-k indices\n",
        "    top_k_indices = np.argsort(similarities)[::-1][:top_k]\n",
        "\n",
        "    results = []\n",
        "    for idx in top_k_indices:\n",
        "        results.append({\n",
        "            'image_id': all_image_ids[idx],\n",
        "            'image_path': all_image_paths[idx],\n",
        "            'similarity_score': float(similarities[idx])\n",
        "        })\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# TEST THE IMPROVED RETRIEVAL\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"TESTING IMPROVED RETRIEVAL\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Reload model with improved weights\n",
        "model, _, preprocess = open_clip.create_model_and_transforms(\"ViT-B-32\", pretrained=\"openai\")\n",
        "model.to(device)\n",
        "state_dict = torch.load('/content/fashion-iq/OUTPUT/openclip_model.pt', map_location=device)\n",
        "model.load_state_dict(state_dict)\n",
        "model.eval()\n",
        "\n",
        "tokenizer = open_clip.get_tokenizer(\"ViT-B-32\")\n",
        "\n",
        "IMAGES_DIR = '/content/fashion-iq-metadata/image_url/downloaded_images_short'\n",
        "\n",
        "# Test image\n",
        "test_image_path = os.path.join(IMAGES_DIR, os.listdir(IMAGES_DIR)[0])\n",
        "print(f\"Reference image: {test_image_path}\")\n",
        "\n",
        "# Test 1: Image + Text\n",
        "text_modification = \"is shorter and sleeveless\"\n",
        "print(f\"\\nText query: '{text_modification}'\")\n",
        "\n",
        "results = retrieve_similar_images_improved(\n",
        "    query_image_path=test_image_path,\n",
        "    text_query=text_modification,\n",
        "    model=model,\n",
        "    preprocess=preprocess,\n",
        "    tokenizer=tokenizer,\n",
        "    device=device,\n",
        "    top_k=5,\n",
        "    text_weight=0.5\n",
        ")\n",
        "\n",
        "print(f\"\\nTop 5 Results (Image + Text Combined):\")\n",
        "for i, result in enumerate(results, 1):\n",
        "    score = result['similarity_score']\n",
        "    status = \" Good\" if score > 0.6 else \" OK\" if score > 0.4 else \" Low\"\n",
        "    print(f\"{i}. ID: {result['image_id']} | Similarity: {score:.4f} {status}\")\n",
        "\n",
        "# Test 2: Image only\n",
        "print(\"\\n\" + \"-\" * 40)\n",
        "print(\"Image Only Search:\")\n",
        "\n",
        "results_image_only = retrieve_similar_images_improved(\n",
        "    query_image_path=test_image_path,\n",
        "    text_query=\"\",\n",
        "    model=model,\n",
        "    preprocess=preprocess,\n",
        "    tokenizer=tokenizer,\n",
        "    device=device,\n",
        "    top_k=5\n",
        ")\n",
        "\n",
        "print(f\"\\nTop 5 Results (Image Only):\")\n",
        "for i, result in enumerate(results_image_only, 1):\n",
        "    score = result['similarity_score']\n",
        "    status = \" Good\" if score > 0.6 else \" OK\" if score > 0.4 else \" Low\"\n",
        "    print(f\"{i}. ID: {result['image_id']} | Similarity: {score:.4f} {status}\")\n",
        "\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9nuQmb8XM_A",
        "outputId": "c9f5243d-3ec4-4bd0-c715-bbd2675b2e45"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "TESTING IMPROVED RETRIEVAL\n",
            "============================================================\n",
            "Reference image: /content/fashion-iq-metadata/image_url/downloaded_images_short/B0089I9NFS.jpg\n",
            "\n",
            "Text query: 'is shorter and sleeveless'\n",
            "\n",
            "Top 5 Results (Image + Text Combined):\n",
            "1. ID: B0089I9NFS | Similarity: 0.7611  Good\n",
            "2. ID: B004BX98O6 | Similarity: 0.5914  OK\n",
            "3. ID: B00385WM5A | Similarity: 0.5842  OK\n",
            "4. ID: B008I2VXU8 | Similarity: 0.5228  OK\n",
            "5. ID: B0075NK230 | Similarity: 0.5137  OK\n",
            "\n",
            "----------------------------------------\n",
            "Image Only Search:\n",
            "\n",
            "Top 5 Results (Image Only):\n",
            "1. ID: B0089I9NFS | Similarity: 1.0000  Good\n",
            "2. ID: B0075NK230 | Similarity: 0.7035  Good\n",
            "3. ID: B00385WM5A | Similarity: 0.7012  Good\n",
            "4. ID: B004BX98O6 | Similarity: 0.6784  Good\n",
            "5. ID: B008I2VXU8 | Similarity: 0.6421  Good\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Streamlit"
      ],
      "metadata": {
        "id": "-uZMDntuYkPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q streamlit pyngrok open_clip_torch"
      ],
      "metadata": {
        "id": "eqtLMy9aYoiX"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "import torch\n",
        "import pickle\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import open_clip\n",
        "import os\n",
        "import json\n",
        "\n",
        "# ============================================\n",
        "# PAGE CONFIGURATION\n",
        "# ============================================\n",
        "\n",
        "st.set_page_config(\n",
        "    page_title=\"Fashion Image Search with Text Modifications\",\n",
        "    page_icon=\"\",\n",
        "    layout=\"wide\"\n",
        ")\n",
        "\n",
        "st.title(\" Fashion Image Search with Picture Descriptions\")\n",
        "st.markdown(\"\"\"\n",
        "**Upload a reference image and describe the changes you want!**\n",
        "\n",
        "For example: Upload a dress and type *\"make it sleeveless and blue\"* to find similar items with those modifications.\n",
        "\"\"\")\n",
        "\n",
        "# ============================================\n",
        "# PATHS\n",
        "# ============================================\n",
        "MODEL_PATH = '/content/fashion-iq/OUTPUT/openclip_model.pt'\n",
        "EMBEDDINGS_PATH = '/content/fashion-iq/OUTPUT/image_embeddings.pkl'\n",
        "CAPTIONS_PATH = '/content/fashion-iq/captions/cap.test_split.json'\n",
        "\n",
        "# ============================================\n",
        "# LOAD MODEL AND EMBEDDINGS (CACHED)\n",
        "# ============================================\n",
        "\n",
        "@st.cache_resource\n",
        "def load_model_and_data():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # Create model architecture\n",
        "    model = open_clip.create_model(\"ViT-B-32\", pretrained=\"openai\")\n",
        "\n",
        "    # Load trained weights\n",
        "    if os.path.exists(MODEL_PATH):\n",
        "        state_dict = torch.load(MODEL_PATH, map_location=device)\n",
        "        model.load_state_dict(state_dict)\n",
        "        model_status = \" Loaded fine-tuned model weights\"\n",
        "    else:\n",
        "        model_status = \" Using pretrained weights (fine-tuned model not found)\"\n",
        "\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Get preprocessing and tokenizer\n",
        "    preprocess = open_clip.image_transform(model.visual.image_size, is_train=False)\n",
        "    tokenizer = open_clip.get_tokenizer(\"ViT-B-32\")\n",
        "\n",
        "    # Load embeddings database\n",
        "    embeddings_data = None\n",
        "    embeddings_status = \"\"\n",
        "    if os.path.exists(EMBEDDINGS_PATH):\n",
        "        with open(EMBEDDINGS_PATH, 'rb') as f:\n",
        "            embeddings_data = pickle.load(f)\n",
        "        embeddings_status = f\" Loaded {len(embeddings_data['image_ids'])} image embeddings\"\n",
        "    else:\n",
        "        embeddings_status = \" Embeddings file not found!\"\n",
        "\n",
        "    # Load sample captions for suggestions\n",
        "    sample_captions = []\n",
        "    if os.path.exists(CAPTIONS_PATH):\n",
        "        with open(CAPTIONS_PATH, 'r') as f:\n",
        "            test_data = json.load(f)\n",
        "        sample_captions = list(set([\n",
        "            entry['captions'][0] for entry in test_data\n",
        "            if 'captions' in entry and entry['captions']\n",
        "        ]))[:20]  # Get 20 unique captions\n",
        "\n",
        "    return model, preprocess, tokenizer, embeddings_data, sample_captions, device, model_status, embeddings_status\n",
        "\n",
        "# Load everything\n",
        "model, preprocess, tokenizer, embeddings_data, sample_captions, device, model_status, embeddings_status = load_model_and_data()\n",
        "\n",
        "# ============================================\n",
        "# RETRIEVAL FUNCTIONS\n",
        "# ============================================\n",
        "\n",
        "def retrieve_with_image_and_text(uploaded_image, text_query, text_weight=0.5, top_k=6):\n",
        "    \"\"\"\n",
        "    Core retrieval function combining reference image with text modification.\n",
        "    Implements the project's main feature.\n",
        "    \"\"\"\n",
        "    if embeddings_data is None:\n",
        "        return []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Encode reference image\n",
        "        image_tensor = preprocess(uploaded_image).unsqueeze(0).to(device)\n",
        "        image_embedding = model.encode_image(image_tensor)\n",
        "        image_embedding = image_embedding / image_embedding.norm(dim=-1, keepdim=True)\n",
        "        image_embedding = image_embedding.cpu().numpy()\n",
        "\n",
        "        # Encode text modification if provided\n",
        "        if text_query and text_query.strip():\n",
        "            text_tokens = tokenizer([text_query]).to(device)\n",
        "            text_embedding = model.encode_text(text_tokens)\n",
        "            text_embedding = text_embedding / text_embedding.norm(dim=-1, keepdim=True)\n",
        "            text_embedding = text_embedding.cpu().numpy()\n",
        "\n",
        "            # Combine embeddings with weighted average\n",
        "            combined_embedding = (1 - text_weight) * image_embedding + text_weight * text_embedding\n",
        "            combined_embedding = combined_embedding / np.linalg.norm(combined_embedding, axis=-1, keepdims=True)\n",
        "        else:\n",
        "            combined_embedding = image_embedding\n",
        "\n",
        "    # Calculate similarities\n",
        "    all_embeddings = embeddings_data['embeddings']\n",
        "    similarities = np.dot(all_embeddings, combined_embedding.T).flatten()\n",
        "\n",
        "    # Get top-k results\n",
        "    top_k_indices = np.argsort(similarities)[::-1][:top_k]\n",
        "\n",
        "    results = []\n",
        "    for idx in top_k_indices:\n",
        "        results.append({\n",
        "            'image_id': embeddings_data['image_ids'][idx],\n",
        "            'image_path': embeddings_data['image_paths'][idx],\n",
        "            'similarity': float(similarities[idx])\n",
        "        })\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def retrieve_text_only(text_query, top_k=6):\n",
        "    \"\"\"Retrieve images using only text description.\"\"\"\n",
        "    if embeddings_data is None or not text_query:\n",
        "        return []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        text_tokens = tokenizer([text_query]).to(device)\n",
        "        text_embedding = model.encode_text(text_tokens)\n",
        "        text_embedding = text_embedding / text_embedding.norm(dim=-1, keepdim=True)\n",
        "        text_embedding = text_embedding.cpu().numpy()\n",
        "\n",
        "    all_embeddings = embeddings_data['embeddings']\n",
        "    similarities = np.dot(all_embeddings, text_embedding.T).flatten()\n",
        "    top_k_indices = np.argsort(similarities)[::-1][:top_k]\n",
        "\n",
        "    results = []\n",
        "    for idx in top_k_indices:\n",
        "        results.append({\n",
        "            'image_id': embeddings_data['image_ids'][idx],\n",
        "            'image_path': embeddings_data['image_paths'][idx],\n",
        "            'similarity': float(similarities[idx])\n",
        "        })\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def generate_description(uploaded_image, all_descriptions):\n",
        "    \"\"\"Generate description for an image using CLIP similarity.\"\"\"\n",
        "    if not all_descriptions:\n",
        "        return \"No descriptions available\", 0.0\n",
        "\n",
        "    text_tokens = tokenizer(all_descriptions).to(device)\n",
        "    image_tensor = preprocess(uploaded_image).unsqueeze(0).to(device)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        image_features = model.encode_image(image_tensor)\n",
        "        text_features = model.encode_text(text_tokens)\n",
        "\n",
        "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        similarities = (image_features @ text_features.T).cpu().numpy().flatten()\n",
        "\n",
        "    best_idx = np.argmax(similarities)\n",
        "    return all_descriptions[best_idx], float(similarities[best_idx])\n",
        "\n",
        "# ============================================\n",
        "# SIDEBAR\n",
        "# ============================================\n",
        "\n",
        "with st.sidebar:\n",
        "    st.header(\" Settings\")\n",
        "\n",
        "    # Search mode selection\n",
        "    search_mode = st.radio(\n",
        "        \"Search Mode\",\n",
        "        [\" +  Image + Text (Recommended)\", \" Text Only\", \" Image Only\"],\n",
        "        help=\"Choose how you want to search for fashion items\"\n",
        "    )\n",
        "\n",
        "    st.markdown(\"---\")\n",
        "\n",
        "    # Advanced settings\n",
        "    st.subheader(\"Advanced Settings\")\n",
        "    top_k = st.slider(\"Number of results\", min_value=3, max_value=12, value=6)\n",
        "\n",
        "    if \"Image + Text\" in search_mode:\n",
        "        text_weight = st.slider(\n",
        "            \"Text Influence\",\n",
        "            min_value=0.0,\n",
        "            max_value=1.0,\n",
        "            value=0.5,\n",
        "            help=\"0 = Image only, 1 = Text only, 0.5 = Equal balance\"\n",
        "        )\n",
        "    else:\n",
        "        text_weight = 0.5\n",
        "\n",
        "    show_scores = st.checkbox(\"Show similarity scores\", value=True)\n",
        "\n",
        "    st.markdown(\"---\")\n",
        "\n",
        "    # Status indicators\n",
        "    st.subheader(\" System Status\")\n",
        "    st.write(model_status)\n",
        "    st.write(embeddings_status)\n",
        "\n",
        "    st.markdown(\"---\")\n",
        "\n",
        "    # Example queries\n",
        "    st.subheader(\" Example Text Queries\")\n",
        "    st.markdown(\"\"\"\n",
        "    - *\"is shorter and has no sleeves\"*\n",
        "    - *\"is black instead of white\"*\n",
        "    - *\"has longer sleeves\"*\n",
        "    - *\"is more casual\"*\n",
        "    - *\"has a floral pattern\"*\n",
        "    \"\"\")\n",
        "\n",
        "# ============================================\n",
        "# MAIN CONTENT\n",
        "# ============================================\n",
        "\n",
        "# Create two columns for input\n",
        "col_upload, col_text = st.columns([1, 1])\n",
        "\n",
        "with col_upload:\n",
        "    st.subheader(\" Reference Image\")\n",
        "    uploaded_file = st.file_uploader(\n",
        "        \"Upload a fashion image\",\n",
        "        type=['jpg', 'jpeg', 'png'],\n",
        "        help=\"Upload a reference image to find similar items\"\n",
        "    )\n",
        "\n",
        "with col_text:\n",
        "    st.subheader(\" Text Modification\")\n",
        "\n",
        "    # Text input for modifications\n",
        "    text_query = st.text_input(\n",
        "        \"Describe the changes you want\",\n",
        "        placeholder=\"e.g., 'make it sleeveless and blue'\",\n",
        "        help=\"Describe how you want the results to differ from the reference image\"\n",
        "    )\n",
        "\n",
        "    # Quick suggestion buttons\n",
        "    if sample_captions:\n",
        "        st.caption(\"Quick suggestions (click to use):\")\n",
        "        suggestion_cols = st.columns(2)\n",
        "        for i, caption in enumerate(sample_captions[:6]):\n",
        "            with suggestion_cols[i % 2]:\n",
        "                if st.button(caption[:40] + \"...\" if len(caption) > 40 else caption, key=f\"sug_{i}\"):\n",
        "                    text_query = caption\n",
        "\n",
        "# ============================================\n",
        "# SEARCH EXECUTION\n",
        "# ============================================\n",
        "\n",
        "st.markdown(\"---\")\n",
        "\n",
        "# Determine what to search\n",
        "has_image = uploaded_file is not None\n",
        "has_text = text_query and text_query.strip()\n",
        "\n",
        "if has_image or has_text:\n",
        "    # Display inputs\n",
        "    if has_image:\n",
        "        uploaded_image = Image.open(uploaded_file).convert(\"RGB\")\n",
        "\n",
        "        input_col1, input_col2 = st.columns([1, 2])\n",
        "\n",
        "        with input_col1:\n",
        "            st.subheader(\" Your Reference Image\")\n",
        "            st.image(uploaded_image, use_container_width=True)\n",
        "\n",
        "            # Show detected description\n",
        "            if sample_captions:\n",
        "                with st.spinner(\"Analyzing image...\"):\n",
        "                    desc, conf = generate_description(uploaded_image, sample_captions)\n",
        "                    st.info(f\"**Detected style:** {desc}\")\n",
        "                    if show_scores:\n",
        "                        st.caption(f\"Confidence: {conf:.3f}\")\n",
        "\n",
        "    # Execute search based on mode\n",
        "    with st.spinner(\" Searching for matching items...\"):\n",
        "        if \"Image + Text\" in search_mode and has_image:\n",
        "            results = retrieve_with_image_and_text(\n",
        "                uploaded_image,\n",
        "                text_query if has_text else \"\",\n",
        "                text_weight=text_weight,\n",
        "                top_k=top_k\n",
        "            )\n",
        "            search_description = f\"Image + Text (weight: {text_weight:.1f})\" if has_text else \"Image Only\"\n",
        "        elif \"Text Only\" in search_mode and has_text:\n",
        "            results = retrieve_text_only(text_query, top_k=top_k)\n",
        "            search_description = \"Text Only\"\n",
        "        elif \"Image Only\" in search_mode and has_image:\n",
        "            results = retrieve_with_image_and_text(\n",
        "                uploaded_image,\n",
        "                \"\",\n",
        "                text_weight=0,\n",
        "                top_k=top_k\n",
        "            )\n",
        "            search_description = \"Image Only\"\n",
        "        else:\n",
        "            results = []\n",
        "            search_description = \"No valid input\"\n",
        "\n",
        "    # Display results\n",
        "    if has_image:\n",
        "        result_container = input_col2\n",
        "    else:\n",
        "        result_container = st\n",
        "\n",
        "    with result_container if has_image else st.container():\n",
        "        st.subheader(f\" Search Results ({search_description})\")\n",
        "\n",
        "        if has_text:\n",
        "            st.caption(f\"Text query: *\\\"{text_query}\\\"*\")\n",
        "\n",
        "        if results:\n",
        "            # Display in grid\n",
        "            result_cols = st.columns(3)\n",
        "            for i, result in enumerate(results):\n",
        "                with result_cols[i % 3]:\n",
        "                    if os.path.exists(result['image_path']):\n",
        "                        result_image = Image.open(result['image_path'])\n",
        "                        st.image(result_image, use_container_width=True)\n",
        "                        st.caption(f\"ID: {result['image_id']}\")\n",
        "                        if show_scores:\n",
        "                            # Color-code similarity\n",
        "                            sim = result['similarity']\n",
        "                            if sim > 0.7:\n",
        "                                st.success(f\"Similarity: {sim:.3f}\")\n",
        "                            elif sim > 0.5:\n",
        "                                st.warning(f\"Similarity: {sim:.3f}\")\n",
        "                            else:\n",
        "                                st.caption(f\"Similarity: {sim:.3f}\")\n",
        "                    else:\n",
        "                        st.warning(f\"Image not found: {result['image_id']}\")\n",
        "        else:\n",
        "            st.warning(\"No results found. Try a different query or image.\")\n",
        "\n",
        "else:\n",
        "    # No input provided - show instructions\n",
        "    st.info(\" **Get started:** Upload an image and/or enter a text description above!\")\n",
        "\n",
        "    # Show sample images from database\n",
        "    if embeddings_data is not None:\n",
        "        st.markdown(\"###  Sample Images from Database\")\n",
        "        st.caption(\"Click on any image category to see what's available:\")\n",
        "\n",
        "        sample_cols = st.columns(6)\n",
        "        for i, path in enumerate(embeddings_data['image_paths'][:6]):\n",
        "            with sample_cols[i]:\n",
        "                if os.path.exists(path):\n",
        "                    st.image(Image.open(path), use_container_width=True)\n",
        "                    st.caption(f\"ID: {embeddings_data['image_ids'][i][:10]}...\")\n",
        "\n",
        "# ============================================\n",
        "# FOOTER\n",
        "# ============================================\n",
        "\n",
        "st.markdown(\"---\")\n",
        "st.markdown(\"\"\"\n",
        "<div style='text-align: center; color: gray;'>\n",
        "    <p>Built with Streamlit + OpenCLIP | Fashion Image Search with Picture Descriptions</p>\n",
        "    <p><small>Based on CLIP (Contrastive Language-Image Pre-training) fine-tuned on Fashion-IQ dataset</small></p>\n",
        "</div>\n",
        "\"\"\", unsafe_allow_html=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jH6Vo_AWYro5",
        "outputId": "26a9ea74-ff5a-42d7-f03c-f8efc596a15d"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Kill any existing streamlit processes\n",
        "!pkill -f streamlit\n",
        "\n",
        "import subprocess\n",
        "import time\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Set your ngrok auth token\n",
        "# Get a FREE token from: https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "NGROK_TOKEN = \"35hxGOhLKxuICq9PZpBRJ6okyaX_3ySW5H35kHnWum5cqTV5J\"  # <-- REPLACE THIS!\n",
        "\n",
        "# Authenticate ngrok\n",
        "ngrok.set_auth_token(NGROK_TOKEN)\n",
        "\n",
        "# Kill any existing ngrok tunnels\n",
        "ngrok.kill()\n",
        "\n",
        "# Start streamlit in background\n",
        "process = subprocess.Popen(\n",
        "    [\"streamlit\", \"run\", \"app.py\", \"--server.port\", \"8501\", \"--server.headless\", \"true\"],\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.PIPE\n",
        ")\n",
        "\n",
        "# Wait for streamlit to start\n",
        "print(\"Starting Streamlit server...\")\n",
        "time.sleep(8)\n",
        "\n",
        "# Create public URL\n",
        "try:\n",
        "    public_url = ngrok.connect(8501)\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\" STREAMLIT APP IS RUNNING!\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"\\n Access your app at: {public_url}\")\n",
        "    print(\"\\n  Keep this cell running to maintain the connection\")\n",
        "    print(\"=\" * 60)\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    print(\"\\nTrying alternative method...\")"
      ],
      "metadata": {
        "id": "VycigTcpUNSu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb78786a-240e-4af6-ada9-d292e5bea1fb"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Streamlit server...\n",
            "\n",
            "============================================================\n",
            " STREAMLIT APP IS RUNNING!\n",
            "============================================================\n",
            "\n",
            " Access your app at: NgrokTunnel: \"https://dorothy-unclever-transversally.ngrok-free.dev\" -> \"http://localhost:8501\"\n",
            "\n",
            "  Keep this cell running to maintain the connection\n",
            "============================================================\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}